[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building reproducible analytical pipelines with R",
    "section": "",
    "text": "Preface\nIn the summer of 2022, a former colleague from my first job asked me if I wanted to help him teach a class at the University of Luxembourg. It was a class for the Master’s of Data Science, and the class was supposed to be taught by non-academics like us. The idea was to teach the students some “real-world” skills from the industry. It was a 40 hours class, and naturally we split them equally between us; my colleague focused on time series statistics but I really didn’t know what I should do. I knew I wanted to teach, I always liked teaching, but I am a public servant in the ministry of higher education and research in Luxembourg. I still code a lot, but I don’t do exciting machine learning anymore, or advanced econometrics like my colleague. Before (re)joining the public service I was a senior data scientist and then manager in one of the big four accounting firms. Before that, and this is where my colleague and I met, I was a research assistant in the research department of the national statistical institute of statistics in Luxembourg, and my colleague is still an applied researcher there.\nWhat could I teach these students? What “skills from the industry” could I possibly share with them? I am an expert in nothing in particular. Actually, I don’t really know anything very deeply, but know at least a little about many different things. There are many self-help books out there that state that it’s better to know a lot about only a few, maybe even only one, topic, than know a lot about many topics. I tend to disagree with this; at least in my experience, knowing enough about many different topics always allowed me to communicate effectively with many different people, from researchers focusing on very specific topics that needed my help to assist them in their research, to clients from a wide range of industries that were sharing their problems with me in my consulting years. If I needed to deepen my knowledge on a particular topic before I could intervene, I had the necessary theoretical background to grab a few books and learn the material. Also, I was never afraid of asking questions.\nThis is reflected in my blogging. As I’m writing these lines (beginning of 2023), I have been blogging for about ten years. Most of my blog posts are me trying to lay out a problem I had at work and how I solved it. Sometimes I do some things for pleasure or curiosity, like the two posts on the video game nethack, or the ones on 19th century newspapers where I learned a lot about NLP. Because I was lucky enough to work with different people from many backgrounds, I always had to solve a very wide range of problems.\nBut that still didn’t really help me to find a topic to teach… but then it dawned on me. Even though in my career I had to help many different people with many different backgrounds and needs, there were two things that everyone always required: traceability and reliability.\nEveryone wanted to know how I came to the conclusions that I came to, and most of them even wanted to be able to reproduce my steps as a form of double checking what I did (consultants are expensive, so you better make sure that they’re worth their hourly rate!). When I was a manager, I applied the same logic to my teammates. I wanted to be able to understand what they were doing, or at least know that if I needed to review their work deeply, the possibility was there.\nSo what I had to teach these students of data science was some best practices in software engineering. Most people working with data don’t get taught software engineering skills. Courses focus on probability theory, linear algebra, algorithms, and programming but not software engineering. That’s because software engineering skills get taught to software engineers. But while statisticians, data scientists, (or whatever we get called these days), are not software engineers, they do write a lot of code. And code that is quite important at that. And yet, most of us do work like pigs (no disrespect to pigs).\nFor example, how much of the code you write that produces very sensitive and important results, be it in science or in industry, is thoroughly tested? How much of the code you use relies on a single person showing up for work and using some secret knowledge that is undocumented? What if that person ends up under a bus? How much code do you run that no one dares touch anymore because that one person from before did end up under a bus?\nHow many people do you have to ping when you need to get an update to a quarterly report? How many people do you have to ping to know how Table 3 from that report from 2020 that was quickly put together during the Covid-19 lockdowns was computed? Are all the people involved even working in your company still?\nWhen collaborating with teammates to write a report or scientific paper, do you consider potential risks? (If you’re wondering What risks? then you’re definitely not considering them.)\nAre you able to tell anyone, exactly, how that number that gets used by the CEO in that one report was made? What if there’s an investigation, or some external audit? Would the auditors be able to run the code and understand what is going on with as little intervention as possible (ideally none) from you? But I don’t work in an industry that gets audited, you may think. Well, maybe not, or maybe one day your work will get audited anyways. Maybe it’ll get audited internally for whatever reason. Maybe there’s a new law that went into force that requires your work, or parts of your work, to be easily traceable.\nAnd if you’re a scientist, your work does get audited, or at least it should be in theory. I don’t know any scientist (and I know more scientists than the average person, thanks to my background and current job) that is against the idea of open science, open data, reproducibility, and so on. Not one. But in practice, how many papers are truly reproducible? How many scientific results are auditable and traceable?\nLack of traceability and reproducibility can sometimes lead to serious consequences. If you’re in the social sciences, you likely know about the Reinhart and Rogoff paper. Reinhard and Rogoff are two American economists that published a paper in 2010 that showed that when countries are too much in debt (over 60% of GDP according to the authors) then annual growth decreases by two percent. These papers provided an empirical justification for austerity measures in the aftermath of the 2009 European debt crisis. But there was a serious problem with the Reinhard and Rogoff paper. It’s not that they somehow didn’t use the correct theoretical framework or modelling procedure in their paper. It’s not that their assumptions were disputable or too unrealistic. It’s that they performed their calculations inside an Excel spreadsheet and did not, and this is not a joke, they did not select every country’s real GDP growth to compute the average real GDP growth for high-debt countries:\n\n\n\nYou can see that not all countries are selected…\n\n\nAnd this is not the only problem with this paper.\nThe problem is not that this mistake was made. Reinhard and Rogoff are only human and mistakes can happen. What’s problematic is that this was picked up and corrected too late. In an ideal world, Reinhard and Rogoff would not have used tools that make mistakes like this almost impossible to find once they’re made. Instead, they would have used tools that would have made such a thing not happen in the first place, or, as a second best, making it easier and faster for someone else to find this mistake. And this is not something that is only useful in research, but also in any industry. Being able to trust results, tracing back calculations and auditing are not only concerns of researchers.\nSo this is what I decided to teach the students: how they could structure their projects in such a way that they could spot problems like that during development, but also make it easy to reproduce and retrace who did what and when. I wrote my course notes into a freely available bookdown that I used for teaching. When I started compiling my notes, I discovered the concept Reproducible Analytical Pipelines as developed by the Office for National Statistics. I found the name “Reproducible Analytical Pipeline” really perfect for what I was aiming at. The ONS team for evangelising RAPs also published a free ebook in 2019 already. Another big source of inspiration is Software Carpentry to which I was exposed during my PhD years, around 2014-ish if memory serves. While working on a project with some German colleagues from the University of Bonn, the PI made us work using these concepts to manage the project. I was really impressed by it, and these ideas and techniques stayed with me since then.\nThe bottom line is: the ideas I’m presenting here are nothing new. It’s just that I took some time to compile them and make them accessible and interesting (at least I hope so) for users of the R programming language.\nAt least my students found the course interesting. But not just students. I tweeted about this course and shared the notes with a wider audience, and this is when I got very positive feedback from people that were not my students. People wanted to buy this as a book and go deeper into the topics laid out. This is when I realised that, as far as I know, there is not a practical book available discussing these topics. So I decided to write one, but I took my time getting started. What finally, really, got me working on it was when Dmytro Perepolkin reached out to me and suggested I contact several persons to get their inputs and ideas and get started. I followed his advice, and this led to very fruitful discussions with Sébastien Rochette, Miles McBain and Dmytro. Their ideas and inputs definitely improved the quality of this book, so many thanks to them.\nThis book is divided into two parts. The first part teaches you what I believe is essential knowledge you should possess in order to write truly reproducible pipelines. This essential knowledge is constituted of:\n\nVersion control with Git and how to manage projects with Github;\nFunctional programming;\nLiterate programming.\n\nThe main idea from part 1 is “don’t repeat yourself”. Git and Github will help us avoid losing code, and losing track of who should do what in a project (even if you’re working alone on a project, you will see that using Git and Github will save you many hours and headaches). Getting familiar with functional and literate programming should improve the quality of our code by avoiding two common sources of mistakes: computing results that rely on the state of our program (and later, the state of the whole hardware we are using) and copy and paste mistakes.\nThe second part of the book will then build upon this knowledge to introduce several tools that will help us go beyond the benefits of version control and functional and literate programming:\n\nDependency management with {renv};\nBuild automation with {targets};\nReproducible environments with Docker;\nContinuous integration and delivery.\n\nWhile this is not a book for beginners (you really should be familiar with R before reading this), I will not assume that you have any knowledge of the tools presented in part 2. In fact, even if you’re already familiar with Git, Github, functional programming and literate programming, I think that you will still learn something useful from reading part 1.\nI hope that you will enjoy reading this book and applying the ideas in your day-to-day, ideas which hopefully should improve the reliability, traceability and reproducibility of your code. You can read this book for free on https://raps-with-r.dev/ and will also be able to buy a physical copy, soon.\nIf you have feedback, drop me an email at bruno [at] brodrigues [dot] co.\nEnjoy!"
  },
  {
    "objectID": "intro.html#who-is-this-book-for",
    "href": "intro.html#who-is-this-book-for",
    "title": "1  Introduction",
    "section": "1.1 Who is this book for?",
    "text": "1.1 Who is this book for?\nThis book is for anyone that uses raw data to build any type of output based on that raw data. This can be a simple quarterly report for example, in which the data is used for tables and graphs, or a scientific article for a peer reviewed journal or even an interactive web application. It doesn’t matter, because the process is, at its core, always very similar:\n\nGet the data;\nClean the data;\nWrite code to analyse the data;\nPut the results into the final product.\n\nThis book will already assume some familiarity with programming, and in particular the R programming language. However, if you’re comfortable with another programming language like Python, you could still learn a lot from reading this book. Some tools presented in this book are specific to R, but there will always be an alternative for the language you prefer using, meaning that you could apply the advice from this book to your needs and preferences."
  },
  {
    "objectID": "intro.html#what-is-the-aim-of-this-book",
    "href": "intro.html#what-is-the-aim-of-this-book",
    "title": "1  Introduction",
    "section": "1.2 What is the aim of this book?",
    "text": "1.2 What is the aim of this book?\nThe aim of this book is to make the process of analysing data as reliable, retraceable, and reproducible as possible, and do this by design. This means that once we’re done with the analysis, we’re done. We don’t want to spend time, nor have the time, to rewrite or refactor an analysis and make it reproducible after the fact. We all know that this is not going to happen. Once an analysis is done, it’s time to go to the next analysis. And if we need to rerun an older analysis (for example, because the data got updated), then we’ll simply figure it out at that point. Hopefully, we will remember every quirk of our code and know which script to run at which point in the process, which comments are outdated and can be safely ignored, what features of the data need to be checked (and when they need to be checked), and so on…\nGoing forward, we’re going to refer to the process above as a “reproducible analytical pipeline”, or RAP for short. There are only two ways to make such a RAP reproducible; either we are lucky enough to have someone on the team whose job is to do this, or we do it ourselves. And this second option is very likely the most common. The issue is, as stated above, that we simply don’t do it. We are always in the rush to get to the results, and don’t think about making the process reproducible. This is because we always think that making the process reproducible takes time and this time is better employed to perform the analysis itself. But this is a misconception, for two reasons.\nThe first reason is that employing the techniques that we are going to discuss in this book, won’t actually take much time. As you will see, they’re not really things that you “add on top of the analysis” that take time, but will be part of the analysis and project management themselves. And some of these techniques will even save you time (especially testing) and headaches.\nThe second reason is that an analysis is never, ever, a one-shot. Only the most simple things, like pulling out a number from some data base may be a one-shot. And even then, chances are that once you provide that number, you’ll be asked to pull out a variation of that number (for example, by disaggregating by one or several variables). Or maybe you’ll get asked for an update to that number in six months. So you will learn very quickly to keep that SQL query in a script somewhere to make sure that you provide a number that is consistent. But what about more complex analyses? Is keeping the script enough? Well that’s already a good start. The problem is that very often, there is no script, or not a script for each step of the analysis.\nI’ve seen this play out many times in many different organisations. It’s that time of the year again, we have to write a report. 10 people are involved, and just gathering the data is already complicated. Some get their data from Word documents attached to emails, some from a website, some from a report from another department that is a PDF… I remember a story that a senior manager at my previous job used to tell us: once, a client put out a call for a project that involved helping them setting up a PDF scraper. They periodically needed data from another department that came in PDFs. The manager asked what was, at least from our perspective, an obvious question: why can’t they send you the underlying data from that PDF in a machine readable format? They had never thought to ask. So my manager went to that department, and talked to the people putting that PDF together. Their answer? “Well, we could send them the data in any format they want, but they’ve asked us to send the tables in a PDF format”.\nSo the first, and probably most important lesson here is: when starting to build a RAP, make sure that you talk with all the people involved."
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "1  Introduction",
    "section": "1.3 Prerequisites",
    "text": "1.3 Prerequisites\nYou should be comfortable with the R programming language. This book will assume that you have been using R for some projects already, and want to improve not only your knowledge of the language itself, but also how to successfully manage complex projects. Ideally, you should know about packages, how to install them, you should have written some functions already, know about loops and have some basic knowledge of data structures like lists. While this is not a book on visualisation, we will be making some graphs using the {ggplot2} package, so if you’re familiar with that, that’s good. If not, no worries, visualisation, data munging or data analysis is not the point of this book.\nIdeally, you should also not be afraid of not using Graphical User Interfaces (GUI). While you can follow along using an IDE like RStudio, we will not be teaching any features from any program with a GUI. This is not to gatekeep, but because interacting graphically with a program is simply not reproducible. This is the second lesson of building RAPs: there should be no human intervention needed to get the outputs once the RAP is started. So our target is to write code that can be executed non-interactively by a machine. This is because one necessary condition for a workflow to be reproducible and get referred to as a RAP, is for the workflow to be able to be executed by a machine, automatically, without any human intervention. If this is the case, then your workflow is likely reproducible, or can at least be made reproducible much more easily than if it requires some special manipulation by a human somewhere in the loop."
  },
  {
    "objectID": "intro.html#what-is-reproducibility",
    "href": "intro.html#what-is-reproducibility",
    "title": "1  Introduction",
    "section": "1.4 What is reproducibility?",
    "text": "1.4 What is reproducibility?\nA reproducible project means that this project can be rerun by anyone at 0 (or very minimal) cost. But there are different levels of reproducibility, and we will discuss this in the next section. Let’s discuss some requirements that a project must have to be considered a RAP.\n\n1.4.1 Using open-source tools to build a RAP is a hard requirement\nOpen source is a hard requirement for reproducibility.\nNo ifs nor buts. And I’m not only talking about the code you typed for your research paper/report/analysis. I’m talking about the whole ecosystem that you used to type your code and build the workflow.\nIs your code open? That’s good. Or is it at least available to other people from your organisation, in a way that they could re-execute it if needed? Good.\nBut is it code written in a proprietary program, like STATA, SAS or MATLAB? Then your project is not reproducible. It doesn’t matter if this code is well documented and written and available on a version control system (internally to your company or open to the public). This project is not reproducible. Why?\nBecause there is no way to re-execute your code with the exact same version of this proprietary program down the line. As I’m writing these lines, MATLAB, for example, is at version R2022b. And buying an older version is not guaranteed. I’m sure if you contact their sales department they might be able to sell you an older version. Maybe you can even simply redownload older versions that you’ve already bought. But maybe it’s not that simple. Or maybe they won’t offer this option anymore in the future, who knows? In any case, if you google “purchase old version of Matlab” you will see that many researchers and engineers have this need.\n\n\n\nWanting to run older versions of analytics software is a recurrent need.\n\n\nAnd if you’re running old code written for version, say, R2008a, there’s no guarantee that it will produce the exact same results on version 2022b. And let’s not even mention the toolboxes (if you’re not familiar with MATLAB’s toolboxes, they’re the equivalent of packages or libraries in other programming languages). These evolve as well, and there’s no guarantee that you can purchase older versions of said toolboxes. And it’s likely that newer versions of toolboxes cannot even run on older versions of Matlab.\nAnd let me be clear, what I’m describing here with MATLAB could also be said for any other proprietary programs still commonly (unfortunately) used in research and in statistics (like STATA or SAS). And even if some, or even all, of the editors of these proprietary tools provide ways to buy and run older versions of their software, my point is that the fact that you have to rely on them for this is a barrier to reproducibility, and there is no guarantee they will provide the option to purchase older versions forever. Also, who guarantees that they will be around forever? Or, and that’s more likely, that they will keep offering a program that you install on your machine instead of shifting to a subscription based model?\nFor just $199 a month, you can execute your SAS/STATA/MATLAB scripts on the cloud! Worry about data confidentiality? No worries, data gets encrypted and stored safely on our secure servers! Run your analysis from anywhere and don’t worry about losing your work if your cat knocks over your coffee on your laptop! And if you purchase the pro licence, for an additional $100 a month, you can even execute your code in parallel!\nThink this is science fiction? Google “SAS cloud” to see SAS’s cloud based offering.\n\n\n1.4.2 There are hidden dependencies that can hinder the reproducibility of a project\nThen there’s another problem: let’s suppose you’ve written a nice, thoroughly tested and documented workflow, and made it available on Github (and let’s even assume that the data is available for people to freely download, and that the paper is open access). Or, if you’re working in the private sector, you did everything above as well, the only difference being that the workflow is only available to people inside the company instead of being available freely and publicly online.\nLet’s further assume that you’ve used R or Python, or any other open source programming language. Could this study/analysis be said to be reproducible? Well, if the analysis ran on a proprietary operating system, then the conclusion is: your project is not reproducible.\nThis is because the operating system the code runs on can also influence the reproducibility of the project. There are some particularities in operating systems that may make certain things work differently. Admittedly, this is in practice rarely a problem, but it does happen, especially if you’re working with very high precision floating point arithmetic like you would do in the financial sector.\nThankfully, there is no need to change operating systems to deal with this issue, and we will learn how to use Docker to safeguard against this problem.\n\n\n1.4.3 The requirements of a RAP\nSo where does that leave us? Basically, for something to be truly reproducible, it has to respect the following bullet points:\n\nSource code must obviously be available and thoroughly tested and documented (which is why we will be using Git and Github);\nAll the dependencies must be easy to find and install (we are going to deal with this using dependency management tools);\nTo be written with an open source programming language (nocode tools like Excel are by default non-reproducible because they can’t be used non-interactively, and which is why we are going to use the R programming language);\nThe project needs to be run on an open source operating system (thankfully, we can deal with this without having to install and learn to use a new operating system, thanks to Docker);\nData and the paper/report need obviously to be accessible as well, if not publicly as is the case for research, then within your company. This means that the concept of “scripts and/or data available upon request” belongs in the trash.\n\n\n\n\nA real sentence from a real paper published in THE LANCET Regional Health. How about make the data available and I won’t scratch your car, how’s that for a reasonable request?"
  },
  {
    "objectID": "intro.html#are-there-different-types-of-reproducibility",
    "href": "intro.html#are-there-different-types-of-reproducibility",
    "title": "1  Introduction",
    "section": "1.5 Are there different types of reproducibility?",
    "text": "1.5 Are there different types of reproducibility?\nLet’s take one step back: we live in the real world, and in the real world, there are some constraints that are outside of our control. These constraints can make it impossible to build a true RAP, so sometimes we need to settle for something that might not be a true RAP, but a second or even third best thing.\nIn what follows, let’s assume this: in the discussion below, code is tested and documented, so let’s only discuss the code running the pipeline itself.\nThe worst reproducible pipeline would be something that works, but only on your machine. This can be simply due to the fact that you hardcoded paths that only exist on your laptop. Anyone wanting to rerun the pipeline would need to change the paths. This is something that needs to be documented in a README which we assumed was the case, so there’s that. But maybe this pipeline only runs on your laptop because the computational environment that you’re using is hard to reproduce. Maybe you use software, even if it’s open source software, that is not easy to install (anyone that tried to install R packages on Linux that depend on the {rJava} package know what I’m talking about).\nSo a least worse pipeline would be one that could be run more easily on any similar machine as yours. This could be achieved by not using hardcoded absolute paths, and by providing instructions to set up the environment. For example, in the case of R, this could be as simple as providing a script called something like install_deps.R that would be a call to install.packages(). It could look like this:\n\ninstall.packages(c(\"package1\",\n                   \"package2\",\n                   etc))\n\nThe issue here is that you need to make sure that the right versions of the packages get installed. If your script uses {ggplot2} version 2.2.1, then users should install this version as well, and by running the script above, the latest version of {ggplot2} (as of writing, version 3.4.0) will get installed. Maybe that’s not a problem, but it can be if your script uses a function from version 2.2.1 that is not available anymore in the latest version (or maybe its name got changed, or maybe it was modified somehow and doesn’t provide the exact same result). And the more packages the script uses (and the older it is), the higher the likelihood that some package version will not be compatible. There is also the issue of the R version itself. Generally speaking, recent versions of R seem to not be too bad when it comes to running older code written in R. I know this because in 2022 I’ve ran every example that comes bundled with R since version 0.6.0 on the then current version of R, version 4.2.2. Here is the result of this experiment:\n\n\n\nExamples from older versions of R run most of the time successfully on the current version of R\n\n\nThis graph shows the following: for each version of R, starting with R version 0.6.0 (released in 1997), how well the examples that came with a standard installation of R run on the current version of R (version 4.2.2 as of writing). These are the examples from the default packages like {base}, {stats}, {stats4}, and so on. Turns out that more than 75% of the example code from version 0.6.0 still work on the current version of R. A small fraction output a message (which doesn’t mean the code doesn’t work), some 5% raise a warning, which again doesn’t necessarily mean that the code doesn’t work, and finally around 20% or so errors. As you can see, the closer we get to the current release, the less errors get raised (if you want to run the code for yourself, check out this Github repository).\n(But something important should be noted: just because some old piece of code runs without error, doesn’t mean that the result is exactly the same. There might be cases where the same function returns different results on different versions of R.)\nBut while this is evidence of R itself being quite stable through time, there are studies that show a less rosy picture. In a recent study (Trisovic et al. (2022) 1), some researchers tried to rerun up to 9000 R scripts downloaded from the Harvard Dataverse. There were several issues when trying to rerun the scripts, which lead to, and I quote the paper here, “[…] 74% of R files [failing] to complete without error in the initial execution, while 56% failed when code cleaning was applied, showing that many errors can be prevented with good coding practices”.\nThe take-away message is that counting on the language itself being stable through time as a sufficient condition for reproducibility is not enough. We have to set up the code in a way that it actually is reproducible.\nSo what does this all mean? This means that reproducibility is on a continuum, and depending on the constraints you face your project can not very reproducible to totally reproducible. Let’s consider the following list of anything that can influence how reproducible your project truly is:\n\nVersion of the programming language used;\nVersions of the packages/libraries of said programming language used;\nOperating System, and its version;\nVersions of the underlying system libraries (which often go hand in hand with OS version, but not necessarily).\nAnd even the hardware architecture that you run all that software stack on.\n\nSo by “reproducibility is on a continuum”, what I mean is that you could set up your project in a way that none, one, two, three, four or all of the preceding items are taken into consideration when making your project reproducible.\nThis is not a novel, or new idea. Peng (2011) already discussed this concept but named in the reproducibility spectrum. In part 2 of this book, I will reintroduce the idea and call it the “reproducibility iceberg”.\n\n\n\nThe reproducibility spectrum from Peng’s 2011 paper.\n\n\nLet me just finish this introduction by the last item on the previous list: hardware architecture by discussing a fairly recent event in computing. You see, Apple has changed hardware architecture recently, their new computers switched from Intel based hardware to their own proprietary architecture (Apple Silicon) based on the ARM specification. And what does that mean concretely? It means that all the binary packages that were built for Intel based Apple computers cannot work on their new computers. Which means that if you have a recent M1 Macbook and need to install old CRAN packages to rerun a project (and we will learn how to do this later in the book), these need to be compiled to work on M1. You cannot even install older versions of R, unless you also compile those from source! This is because these older versions of R and packages were compiled to run on the previous architecture that Apple used for their computers, and cannot be run on the current architecture. Now I have read about a compatibility layer called Rosetta which enables to run binaries compiled for the Intel architecture on the ARM architecture, and maybe this works well with R and CRAN binaries compiled for Intel architecture. Maybe, I don’t know. But my point is that you never know what might come in the future, and thus needing to be able to compile from source is important, because compiling from source is what requires the least amount of dependencies that are outside of your control. Relying on binaries is not future-proof (and which is again, another reason why open-source tools are a hard requirement for reproducibility).\nAnd for you Windows users, don’t think that the preceding paragraph does not concern you. I think that it is very likely that Microsoft will push in the future for OEM manufacturers to develop more ARM based computers. There is already an ARM version of Windows after all, and it has been around for quite some time, and I think that Microsoft will not kill that version any time in the future. This is because ARM is much more energy efficient than other architectures, and any manufacturer can build its own ARM cpus by purchasing a license, which can be quite interesting. For example in the case of Apple Silicon cpus, Apple can now get exactly the cpus they want for their machines and make their software work seamlessly with it. I doubt that others will pass the chance to do the same.\nAlso, something else that might happen is that we might move towards more and more cloud based computing, but I think that this scenario is less likely than the one from before. But who knows. And in that case it is quite likely that the actual code will be running on Linux servers that will likely be ARM based because of energy costs. Here again, if you want to run your historical code, you’ll have to compile old packages and R versions from source.\nOk, so this might seem all incredibly complicated. How on earth are we supposed to manage all these risks and balance the immediate need for results with the future need of rerunning an old project? And what if rerunning this old project is not even needed in the future?\nThis is where this book will help you. By employing the techniques discussed in this book, not only will it be very easy and quick to set up a project from the ground up that is truly reproducible, the very fact of building the project this way will also ensure that you avoid mistakes and producing results that are wrong. It will be easier and faster to iterate and improve your code, to collaborate, and ultimately to trust the results of your pipelines. So even if no one will rerun that code ever again, you will still benefit from the best practices presented in this book. Let’s dive in!\n\n\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–27.\n\n\nTrisovic, Ana, Matthew K Lau, Thomas Pasquier, and Mercè Crosas. 2022. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9 (1): 60."
  },
  {
    "objectID": "part1_intro.html#introduction",
    "href": "part1_intro.html#introduction",
    "title": "Part 1: Don’t repeat yourself",
    "section": "Introduction",
    "text": "Introduction\nPart 1 will focus on teaching you the fundamental ingredients to reproducibility. By fundamental ingredients I mean those tools that you absolutely need to have in your toolbox before even attempting to make a project reproducible. These tools are so important, that a good chunk of this book is dedicated to them:\n\nVersion control;\nFunctional programming;\nLiterate programming.\n\nYou might already be familiar with these topics, and maybe already use them in your day to day. If that’s the case, you still might want to at least skim part 1 before tackling part 2 of the book, which will focus on another set of tools to actually build reproducible analytical pipelines (RAPs).\nSo this means that part 1 will not teach you how to build reproducible pipelines. But I cannot immediately start building reproducible analytical pipelines without first making sure that you understand the core concepts laid out above. To help us understand these concepts, we will start by analysing some data together. We ar going to download, clean and plot some data, and we will achieve this by writing two scripts. These scripts will be written in a very “typical non software engineery” way, as to mimic how analysts, data scientists or researchers without any formal training in computer science would perform such an analysis. This does not mean that the quality of the analysis will be low. But it means that, typically, these programmers have delievering results fast, and by any means necessary, as the top priority. Our goal with this book is to show you, and hopefully convince you, that by adopting certain simple ideas from software engineering it is possible to deliver just as fast as before, but in a more consistent and robust way.\nLet’s get started!"
  },
  {
    "objectID": "prerequisites.html#rs-anatomy",
    "href": "prerequisites.html#rs-anatomy",
    "title": "2  Before we start",
    "section": "2.1 R’s anatomy",
    "text": "2.1 R’s anatomy\n\nread eval print loop\nconsole, editor\npackages, library\nCRAN\n.Rprofile\nworkspace (don’t save it)\n\nPackages needed to follow along, look at deps.R\nGet comfortable with paths\n\nabsolute paths (no no)\nrelative paths (yes)\n\n{here} can help, but I won’t discuss it.\nWhat is a text editor, and what is text? Word is not a text editor, and work files are not text. Important to know the difference."
  },
  {
    "objectID": "project_start.html#housing-in-luxembourg",
    "href": "project_start.html#housing-in-luxembourg",
    "title": "3  Project start",
    "section": "3.1 Housing in Luxembourg",
    "text": "3.1 Housing in Luxembourg\nWe are going to download data about house prices in Luxembourg. Luxembourg is a little Western European country the author hails from that looks like a shoe and is about the size of .98 Rhode Islands. Did you know that Luxembourg is a constitutional monarchy, and not a kingdom like Belgium, but a Grand-Duchy, and actually the last Grand-Duchy in the World? Also, what you should know to understand what we will be doing is that the country of Luxembourg is divided into Cantons, and each Cantons into Communes. If Luxembourg was the USA, Cantons would be States and Communes would be Counties (or Parishes or Boroughs). What’s confusing is that “Luxembourg” is also the name of a Canton, and of a Commune, which also has the status of a city and is the capital of the country. So Luxembourg the country, is divided into Cantons, one of which is called Luxembourg as well, cantons are divided into communes, and inside the canton of Luxembourg there’s the commune of Luxembourg which is also the city of Luxembourg, sometimes called Luxembourg-City, which is the capital of the country.\n\n\n\nLuxembourg is about as big as the US State of Rhode Island.\n\n\nWhat you should also know is that the population is about 645.000 as of writing (January 2023), half of which are foreigners. Around 400.000 persons work in Luxembourg, of which half do not live in Luxembourg; so every morning from Monday to Friday, 200.000 people enter the country to work, and leave in the evening to go back to either Belgium, France or Germany, the neighbouring countries. As you can imagine, this puts enormous pressure on the transportation system and on the roads, but also on the housing market; everyone wants to live in Luxembourg to avoid the horrible daily commute, and everyone wants to live either in the capital city, or in the second largest urban area in the south, in a city called Esch-sur-Alzette.\nThe plot below shows the value of the House Price Index through time for Luxembourg and the European Union:\n\n\n\n\n\nIf you want to download the data, click here.\nLet us paste the definition of the HPI in here (taken from the HPI’s metadata page):\nThe House Price Index (HPI) measures inflation in the residential property market. The HPI captures price changes of all types of dwellings purchased by households (flats, detached houses, terraced houses, etc.). Only transacted dwellings are considered, self-build dwellings are excluded. The land component of the dwelling is included.\nSo from the plot, we can see that the price of dwellings more than doubled between 2010 and 2021; the value of the index is 214.81 in 2021 for Luxembourg, and 138.92 for the European Union as a whole.\nThere is a lot of heterogeneity though; the capital and the communes immediately next to the capital are much more expensive than communes from the less densely populated north, for example. The south of the country is also more expensive than the north, but not as much as the capital and surrounding communes. Not only is price driven by demand, but also by scarcity; in 2021, .5% of residents owned 50% of the buildable land for housing purposes (Source: Observatoire de l’Habitat, Note 29, archived download link).\nOur project will be quite simple; we are going to download some data, supplied as an Excel file, compiled by the Housing Observatory (Observatoire de l’Habitat, a service from the Ministry of Housing, which monitors the evolution of prices in the housing market, among other useful services like the identification of vacant lots). The advantage of their data when compared to Eurostat’s data is that the data is disaggregated by commune. The disadvantage is that they only supply nominal prices, and no index. Nominal prices are the prices that you read on price tags in shops. The problem with nominal prices is that it is difficult to compare them through time. Ask yourself the following question: would you prefer to have had 500€ (or USDs) in 2003 or in 2023? You probably would have preferred them in 2003, as you could purchase a lot more with $500 then than now. In fact, according to a random inflation calculator I googled, to match the purchasing power of $500 in 2003, you’d need to have $793 in 2023 (and I’d say that we find very similar values for €). But it doesn’t really matter if that calculation is 100% correct: what matters is that the value of money changes, and comparisons through time are difficult, hence why an index is quite useful. So we are going to convert these nominal prices to real prices. Real prices take inflation into account and so allow us to compare prices through time. So we will need to also get some data to achieve this.\nSo to summarise; our goal is to:\n\nGet data trapped inside an Excel file into a neat data frame;\nConvert nominal to real prices using a simple method;\nMake some tables and plots and call it a day (for now).\n\nWe are going to start in the most basic way possible; we are simply going to write a script and deal with each step separately."
  },
  {
    "objectID": "project_start.html#saving-trapped-data-from-excel",
    "href": "project_start.html#saving-trapped-data-from-excel",
    "title": "3  Project start",
    "section": "3.2 Saving trapped data from Excel",
    "text": "3.2 Saving trapped data from Excel\nGetting data from Excel into a tidy data frame can be very tricky. This is because very often, Excel is used as some kind of dashboard, or presentation tool. So data is made human-readable, in contrast to machine readable. Let us quickly discuss this topic as it is essential to grasp the difference between the two (and in our experience, a lot of collective pain inflicted to statisticians and researchers could have been avoided if this concept was more well-known). The picture below shows an Excel made for human consumption:\n\n\n\nAn Excel file meant for human eyes.\n\n\nSo why is this file not machine-readable? Here are some issues:\n\nThe table does not start in the top-left corner of the spreadsheet, which is where most importing tools expect it to be;\nThe spreadsheet starts with a header that contains an image and some text;\nNumbers are text and use “,” as the thousands separator;\nYou don’t see it in the screenshot, but each year is in a separate sheet.\n\nThat being said, this Excel file is still very tame, and going from this Excel to a tidy data frame will not be too difficult. In fact, we suspect that whoever made this Excel file is well aware of the contradicting requirements of human and machine readable formatting of data, and strove to find a compromise. Because more often than not, getting human readable data into a machine readable formatting is a nightmare.\nThis is actually the file that we are going to use for our project, so if you want to follow along, you can download it here (downloaded on January 2023 from the luxembourguish open data portal). But you don’t need to follow along with code, because I will link the scripts for you to download later.\nEach sheet contains a dataset with the following columns:\n\nCommune: the commune\nNombre d’offres: the total number of selling offers\nPrix moyen annoncé en Euros courants: Average selling price in nominal Euros\nPrix moyen annoncé au m2 en Euros courants: Average selling price in square meters in nominal Euros\n\nFor ease of presentation, we are going to show you each function here separately, but we’ll be putting everything together in a single script once we’re done explaining each step. So first, let’s read in the data. The following lines do just that:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nThe code below downloads the data, and puts it in a data frame:\n\nurl <- \"https://github.com/b-rodrigues/rap4all/raw/master/datasets/vente-maison-2010-2021.xlsx\"\n\nraw_data <- tempfile(fileext = \".xlsx\")\n\ndownload.file(url, raw_data)\n\nsheets <- excel_sheets(raw_data)\n\nread_clean <- function(..., sheet){\n  read_excel(..., sheet = sheet) |>\n    mutate(year = sheet)\n}\n\nraw_data <- map(\n  sheets,\n  ~read_clean(raw_data,\n              skip = 10,\n              sheet = .)\n                   ) |>\n  bind_rows() |>\n  clean_names()\n\nNew names:\n• `*` -> `*...3`\n• `*` -> `*...4`\n\nraw_data <- raw_data |>\n  rename(\n    locality = commune,\n    n_offers = nombre_doffres,\n    average_price_nominal_euros = prix_moyen_annonce_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant\n  ) |>\n  mutate(locality = str_trim(locality)) |>\n  select(year, locality, n_offers, starts_with(\"average\"))\n\nIf you are familiar with the {tidyverse} the above code should be quite easy to follow. We start by downloading the raw Excel file and save the sheet names into a variable. We then use a function called read_clean(), which takes the path to the Excel file and the sheet names as an argument to read the required sheet into a data frame. We use skip = 10 to skip the first 10 lines in each Excel sheet because the first 10 lines contain a header. The last thing this function does is add a new column called year which contains the year of the data. We’re lucky, because the sheet names are the years: “2010”, “2011” and so on. We then map this function to the list of sheet names, thus reading in all the data from all the sheets into one list of data frames. We then use bind_rows(), to bind each data frame into a single data frame, by row. Finally, we rename the columns (by translating their names from French to English) and only select the required columns. If you don’t understand each step of what is going on, don’t worry too much about it; this book is not about learning how to use R.\nRunning this code results in a neat data set:\n\nstr(raw_data)\n\ntibble [1,343 × 5] (S3: tbl_df/tbl/data.frame)\n $ year                          : chr [1:1343] \"2010\" \"2010\" \"2010\" \"2010\" ...\n $ locality                      : chr [1:1343] \"Bascharage\" \"Beaufort\" \"Bech\" \"Beckerich\" ...\n $ n_offers                      : num [1:1343] 192 266 65 176 111 264 304 94 119 70 ...\n $ average_price_nominal_euros   : chr [1:1343] \"593698.31000000006\" \"461160.29\" \"621760.22\" \"444498.68\" ...\n $ average_price_m2_nominal_euros: chr [1:1343] \"3603.57\" \"2902.76\" \"3280.51\" \"2867.88\" ...\n\n\nBut there’s a problem: columns that should be of type numeric are of type character instead (average_price_nominal_euros and average_price_m2_nominal_euros). There’s also another issue, which you would eventually catch as you would be exploring the data: naming of the communes is not consistent. Let’s take a look:\n\nraw_data |>\n  filter(grepl(\"Luxembourg\", locality)) |>\n  count(locality)\n\n# A tibble: 2 × 2\n  locality             n\n  <chr>            <int>\n1 Luxembourg           9\n2 Luxembourg-Ville     2\n\n\nWe can see that the city of Luxembourg is spelled in two different ways. It’s the same with another commune, Pétange:\n\nraw_data |>\n  filter(grepl(\"P.tange\", locality)) |>\n  count(locality)\n\n# A tibble: 2 × 2\n  locality     n\n  <chr>    <int>\n1 Petange      9\n2 Pétange      2\n\n\nSo sometimes it is spelled correctly, with an “é”, sometimes not. Let’s write some code to correct this:\n\nraw_data <- raw_data |>\n  mutate(locality = ifelse(grepl(\"Luxembourg-Ville\", locality),\n                           \"Luxembourg\",\n                           locality),\n         locality = ifelse(grepl(\"P.tange\", locality),\n                           \"Pétange\",\n                           locality)\n         ) |>\n  mutate(across(starts_with(\"average\"), as.numeric))\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\n\nNow this is interesting – converting the average columns to numeric resulted in some NA values. Let’s see what happened:\n\nraw_data |>\n  filter(is.na(average_price_nominal_euros))\n\n# A tibble: 290 × 5\n   year  locality                                        n_off…¹ avera…² avera…³\n   <chr> <chr>                                             <dbl>   <dbl>   <dbl>\n 1 2010  Consthum                                             29      NA      NA\n 2 2010  Esch-sur-Sûre                                         7      NA      NA\n 3 2010  Heiderscheid                                         29      NA      NA\n 4 2010  Hoscheid                                             26      NA      NA\n 5 2010  Saeul                                                14      NA      NA\n 6 2010  <NA>                                                 NA      NA      NA\n 7 2010  <NA>                                                 NA      NA      NA\n 8 2010  Total d'offres                                    19278      NA      NA\n 9 2010  <NA>                                                 NA      NA      NA\n10 2010  Source : Ministère du Logement - Observatoire …      NA      NA      NA\n# … with 280 more rows, and abbreviated variable names ¹​n_offers,\n#   ²​average_price_nominal_euros, ³​average_price_m2_nominal_euros\n\n\nIt turns out that there are no prices for certain communes, but that we also have some rows with garbage in there. Let’s go back to the raw data to see what this is about:\n\n\n\nAlways look at your data.\n\n\nSo it turns out that indeed, there are some rows that we need to remove. We can start by removing rows where locality is missing. Then we have a row where locality is equal to “Total d’offres”. This is simply the total of every offer from every commune. We could keep that in a separate data frame, or even remove it. Finally there’s a row, the last one, that states the source of the data, which we can remove.\nIn the screenshot above, we see another row that we don’t see in our filtered data frame: one where n_offers is missing. This row gives the national average for columns average_prince_nominal_euros and average_price_m2_nominal_euros. What we are going to do is create two datasets: one with data on communes, and the other on national prices. Let’s first remove the rows stating the sources:\n\nraw_data <- raw_data |>\n  filter(!grepl(\"Source\", locality))\n\nLet’s now only keep the communes in our data:\n\ncommune_level_data <- raw_data |>\n    filter(!grepl(\"nationale|offres\", locality),\n           !is.na(locality))\n\nAnd let’s create a dataset with the national data as well:\n\ncountry_level <- raw_data |>\n  filter(grepl(\"nationale\", locality)) |>\n  select(-n_offers)\n\noffers_country <- raw_data |>\n  filter(grepl(\"Total d.offres\", locality)) |>\n  select(year, n_offers)\n\ncountry_level_data <- full_join(country_level, offers_country) |>\n  select(year, locality, n_offers, everything()) |>\n  mutate(locality = \"Grand-Duchy of Luxembourg\")\n\nJoining, by = \"year\"\n\n\nNow the data looks clean, and we can start the actual analysis… or can we? Before proceeding, it would be nice to make sure that we got every commune in there. For this, we need a list of communes from Luxembourg. Thankfully, Wikipedia has such a list.\nLet’s scrape and save this list:\n\ncurrent_communes <- \"https://en.wikipedia.org/wiki/List_of_communes_of_Luxembourg\" |>\n  rvest::read_html() |>\n  rvest::html_table() |>\n  purrr::pluck(1) |>\n  janitor::clean_names()\n\nWe scrape the table from the Wikipedia page using {rvest}. rvest::html_table() returns a list of tables from the Wikipedia table, and then we use purrr::pluck() to keep the first table from the website, which is what we need (I made the calls to the packages explicit, because you might not be familiar with these packages). janitor::clean_names() transforms column names written for human eyes into machine friendly names (for example Growth rate in % would be transformed to growth_rate_in_percent).\nLet’s see if we have all the communes in our data:\n\nsetdiff(unique(commune_level_data$locality), current_communes$commune)\n\n [1] \"Bascharage\"          \"Boevange-sur-Attert\" \"Burmerange\"         \n [4] \"Clémency\"            \"Consthum\"            \"Ermsdorf\"           \n [7] \"Erpeldange\"          \"Eschweiler\"          \"Heiderscheid\"       \n[10] \"Heinerscheid\"        \"Hobscheid\"           \"Hoscheid\"           \n[13] \"Hosingen\"            \"Luxembourg\"          \"Medernach\"          \n[16] \"Mompach\"             \"Munshausen\"          \"Neunhausen\"         \n[19] \"Redange-sur-Attert\"  \"Rosport\"             \"Septfontaines\"      \n[22] \"Tuntange\"            \"Wellenstein\"         \"Kaerjeng\"           \n\n\nWe see many communes that are in our commune_level_data, but not in current_communes. There’s one obvious reason: differences in spelling, for example, “Kaerjeng” in our data, but “Käerjeng” in the table from Wikipedia. But there’s also a less obvious reason; since 2010, several communes have merged into new ones. So there are communes that are in our data, say, in 2010 and 2011, but disappear from 2012 onwards. So we need to do several things: first, get a list of all existing communes from 2010 onwards, and then, harmonise spelling. Here again, we can use a list of Wikipedia:\n\nformer_communes <- \"https://en.wikipedia.org/wiki/Communes_of_Luxembourg#Former_communes\" |>  \n  rvest::read_html() |>\n  rvest::html_table() |>\n  purrr::pluck(3) |>\n  janitor::clean_names() |>\n  dplyr::filter(year_dissolved > 2009)\n\nformer_communes\n\n# A tibble: 20 × 3\n   name                year_dissolved reason                         \n   <chr>                        <int> <chr>                          \n 1 Bascharage                    2011 merged to form Käerjeng        \n 2 Boevange-sur-Attert           2018 merged to form Helperknapp     \n 3 Burmerange                    2011 merged into Schengen           \n 4 Clemency                      2011 merged to form Käerjeng        \n 5 Consthum                      2011 merged to form Parc Hosingen   \n 6 Ermsdorf                      2011 merged to form Vallée de l'Ernz\n 7 Eschweiler                    2015 merged into Wiltz              \n 8 Heiderscheid                  2011 merged into Esch-sur-Sûre      \n 9 Heinerscheid                  2011 merged into Clervaux           \n10 Hobscheid                     2018 merged to form Habscht         \n11 Hoscheid                      2011 merged to form Parc Hosingen   \n12 Hosingen                      2011 merged to form Parc Hosingen   \n13 Mompach                       2018 merged to form Rosport-Mompach \n14 Medernach                     2011 merged to form Vallée de l'Ernz\n15 Munshausen                    2011 merged into Clervaux           \n16 Neunhausen                    2011 merged into Esch-sur-Sûre      \n17 Rosport                       2018 merged to form Rosport-Mompach \n18 Septfontaines                 2018 merged to form Habscht         \n19 Tuntange                      2018 merged to form Helperknapp     \n20 Wellenstein                   2011 merged into Schengen           \n\n\nAs you can see, since 2010 many communes have merged to form new ones. We can now combine the list of current and former communes, as well as harmonise their names:\n\ncommunes <- unique(c(former_communes$name, current_communes$commune))\n# we need to rename some communes\n\n# Different spelling of these communes between wikipedia and the data\n\ncommunes[which(communes == \"Clemency\")] <- \"Clémency\"\ncommunes[which(communes == \"Redange\")] <- \"Redange-sur-Attert\"\ncommunes[which(communes == \"Erpeldange-sur-Sûre\")] <- \"Erpeldange\"\ncommunes[which(communes == \"Luxembourg-City\")] <- \"Luxembourg\"\ncommunes[which(communes == \"Käerjeng\")] <- \"Kaerjeng\"\ncommunes[which(communes == \"Petange\")] <- \"Pétange\"\n\nLet’s run our test again:\n\nsetdiff(unique(commune_level_data$locality), communes)\n\ncharacter(0)\n\n\nGreat! When we compare the communes that are in our data with every commune that has existed since 2010, we don’t have any commune that is unaccounted for. So are we done with cleaning the data? Yes, we can now actually start with analysing the data. Take a look here1 to see the finalised script. Also read some of the comments the we’ve added. This is a typical R script, and at first glance, one might wonder what is wrong with it. Actually, not much, but the problem if you leave this script as it is, is that it is very likely that we will have problems rerunning it in the future. As it turns out, this script is not reproducible. But we will discuss this in much more detail later on. For now, let’s analyse our cleaned data."
  },
  {
    "objectID": "project_start.html#analysing-the-data",
    "href": "project_start.html#analysing-the-data",
    "title": "3  Project start",
    "section": "3.3 Analysing the data",
    "text": "3.3 Analysing the data\nWe are now going to analyse the data. The first thing we are going to do is compute a Laspeyeres price index. This price index allows us to make comparisons through time; for example, the index at year 2012 measures how much more expensive (or cheaper) housing became relative to the base year (2010). However, since we only have one good, this index becomes quite simple to compute: it is nothing but the prices at year t divided by the prices in 2010 (if we had a basket of goods, we would need to use the Laspeyeres index formula to compute the index at all periods).\nFor this section, we will perform a rather simple analysis. We will immediately show you the R script: take a look at it here2. For our analysis we selected 5 communes and plotted the evolution of prices compared to the national average.\nThis analysis might seem trivially simple, but it contains all the needed ingredients to illustrate everything else that we’re going to teach you in this book.\nMost analyses would stop here: after all, we have what we need; our goal was to get the plots for the 5 communes of Luxemourg, Esch-sur-Alzette, Mamer, Schengen (which gave its name to the Schengen Area) and Wincrange. However, let’s ask ourselves the following important questions:\n\nHow easy would it be for someone else to rerun the analysis?\nHow easy would it be to update the analysis once new data gets published?\nHow easy would it be to reuse this code for other projects?\nWhat guarantee do we have that if the scripts get run in 5 years, with the same input data, we get the same output?\n\nLet’s answer these questions one by one."
  },
  {
    "objectID": "project_start.html#your-project-is-not-done",
    "href": "project_start.html#your-project-is-not-done",
    "title": "3  Project start",
    "section": "3.4 Your project is not done",
    "text": "3.4 Your project is not done\n\n3.4.1 How easy would it be for someone else to rerun the analysis?\nThe analysis is composed of two R scripts, one to prepare the data, another to actually run the analysis proper. This might seem quite easy, because each script contains comments as to what is going on, and the code is not that complicated. However, we are missing any project-level documentation, that would provide clear instructions as to how to run it. This might seem simple for us who wrote these scripts, but we are familiar with R, and this is still fresh in our brains. Should someone less familiar with R have to run the script, there is no clue for them as to how they should do it. And of course, should the analysis be more complex (suppose it’s composed of a dozens scripts), this gets even worse. It might not even be easy for you to remember how to run this in 5 months!\nAnd what about the required dependencies? Many packages were used in the analysis. How should these get installed? Ideally, the same versions of the packages you used and the same version of R should get used by that person to rerun the analysis.\nAll of this still needs to get documented, but documenting packages and their versions takes quite some time. Thankfully, in part 2, we will learn about the {renv} package to deal with this in a couple lines of code.\n\n\n3.4.2 How easy would it be to update the project?\nIf new data gets published, all the points discussed previously are still valid, plus you need to make sure that the updated data is still close enough to the previous data that it can pass through the data cleaning steps you wrote. You should also make sure that the update did not introduce a mistake in past data, or at least alert you if that is the case. Sometimes, when new years get added, data for previous years also get corrected, so it would be nice to make sure that you know this. Also, in the specific case of our data, communes might get fused into a new one, or maybe even divided into smaller communes (even though this is has not happened in a long time, it is not entirely out of the question).\nIn summary, what is missing from the current project are enough tests to make sure that an update to the data can happen smoothly.\n\n\n3.4.3 How easy would it be to reuse this code for another project?\nSaid plainly, not very easy. With code in this state you have no choice but to copy and paste it into a new script and change it adequately. For re-usability, nothing beats structuring your code into functions and ideally you would even package them. We are going to learn just that in future chapters of this book.\nBut sometimes you might not be interested in reusing code for another project: however, even if that’s the case, structuring your code into functions and packaging them makes it easy to reuse even inside the same project. Look at the last part of the analysis.R script: we copy and pasted the same code 5 times and only slightly changed it. We are going to learn how not to repeat ourselves by using functions and you will immediately see the benefits of writing functions, even when simply to reuse inside the same project.\n\n\n3.4.4 What guarantee do we have that the output is stable through time?\nNow this might seem weird: after all, if we start from the same dataset, does it matter when we run the scripts? We should be getting the same result if we build the project today, in 5 months or in 5 years. Well, not necessarily. While it is true that R is quite stable, this cannot necessarily be said of the packages that get used. There is no guarantee that the authors of the packages will not change the package’s functions to work differently, or take arguments in a different order, or even that the packages will all be available at all in 5 years. And even if the packages are still available and function the same, bugs in the packages might get corrected that could alter the result. This might seem like a non-problem; after all, if bugs get corrected, shouldn’t you be happy to update your results as well? But this depends on what it is we’re talking about. Sometimes it is necessary to reproduce results exactly as they were, even if it they were wrong, for example in the context of an audit.\nSo we also need a way to somehow snapshot and freeze the computational environment that was used to create the project originally."
  },
  {
    "objectID": "project_start.html#conclusion",
    "href": "project_start.html#conclusion",
    "title": "3  Project start",
    "section": "3.5 Conclusion",
    "text": "3.5 Conclusion\nWe now have a basic analysis that has all we need to get started. In the coming chapters, we are going to learn about topics that will make it easy to write code that is more robust, better documented and tested, and most importantly easy to rerun (and thus to reproduce the results). The first step will actually not involve having to start rewriting our scripts though; next we are going to learn about Git, a tool that will make our life easier by versioning our code."
  },
  {
    "objectID": "git.html#installing-git-and-opening-a-github-account",
    "href": "git.html#installing-git-and-opening-a-github-account",
    "title": "4  Version control",
    "section": "4.1 Installing Git and opening a Github account",
    "text": "4.1 Installing Git and opening a Github account\nGit is a program that you install on your computer. If you’re running a Linux distribution, chances are Git is already installed. Try to run the following command in a terminal to see if this is the case:\nwhich git\nIf a path like /usr/bin/git gets shown, congratulations, you can skip the rest of this paragraph. If something like:\n/usr/bin/which: no git in (/home/username/.local/bin:/home/username/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)\ngets shown instead, then this means that Git is not installed on your system. To install Git, use your distribution’s package manager, as it is very likely that Git is packaged for your system. On Ubuntu, arguably the most popular Linux distribution, this means running:\nsudo apt-get update\nsudo apt-get install git\nOn macOS and Windows, follow the instructions from the Git Book. It should be as easy as running an installer.\nDepending on your operating system, a graphical user interface might have been installed with Git, making it possible to interact with Git outside of the command line. It is also possible to use Git from within RStudio and many other editors have interfaces to Git as well.\nWe are not going to use any graphical user interface however. This is because there is no common, universal graphical user interface; they all work slightly differently. The only universal is the command line. Also, learning how to use Git via the command line will make it easier the day you will need to use it from a server, which will very likely happen. It also makes our job easier: it is simpler to tell you which commands to run and explain them to you than littering the book with dozens upon dozens of screenshots that might get outdated as soon as a new version of the interface gets released.\nDon’t worry, using the command line is not as hard as it sounds.\nIf you don’t have already a Github account, now is the time to create one. Just go over to https://github.com/ and simply follow the instructions and select the free tier to open your account.\n\n\n\nThis is your Github dashboard.\n\n\nNow that we have an opened account, we can go to the folder that contains the two scripts we wrote at the start of the book."
  },
  {
    "objectID": "git.html#git-superbasics",
    "href": "git.html#git-superbasics",
    "title": "4  Version control",
    "section": "4.2 Git superbasics",
    "text": "4.2 Git superbasics\nOpen the folder that contains the two scripts in a file explorer. On most Linux desktop environments you should be able to right-click inside that folder anywhere and select an option titled something like “Open Terminal here”. On Windows, do the same, but the option is titled “Open Git Bash here”. On macOS, you need to first activate this option. Simply google for “open terminal at folder macOS” and follow the instructions. It is also possible to drag and drop a folder into a terminal which will then open the correct path in the terminal. Another option, of course, is to simply open a terminal and navigate to the correct folder using cd (change directory:\ncd /home/user/housing/\n(The above command assumes that our project is inside a folder called “housing”). Make sure that you are in the right folder by listing the contents of the folder:\nls\nOne little thing: from now on, the prompt of a terminal (or Git bash terminal on Windows) will start with owner@localhost. This is the user called owner (“owner” simply because that will be the project manager in our examples from now on) and the computer owner uses is called localhost. So here is what happens when owner runs ls on the root directory of the project:\nowner@localhost ➤ ls\nanalysis.R save_data.R\n(on Linux you could also try ll which is often available. It is an alias for ls -l which provides a more detailed view. There’s also ls -la which also lists hidden files).\nIf you want to follow along, create a folder called housing and put the two scripts we developed before in there:\n\nsave_data.R: https://is.gd/7PhUjd\nanalysis.R: https://is.gd/qCJEbi\n\nOpen a terminal in that folder and run ls and make sure that you see the two files listed.\nIt’s now time to start tracking these files using Git. In the same window in which we ran ls, run now the following git command:\ngit init\nowner@localhost ➤ git init\nhint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint: \nhint:   git config --global init.defaultBranch <name>\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint: \nhint:   git branch -m <name>\nInitialized empty Git repository in /home/user/six_to/housing/.git/\nTake some time to read the hints. Many git commands give you hints and it’s always a good idea to read them. This hint here tells us that the default branch name is “master” and that this is subject to change. For example, if you create a repository on Github, they suggest “main” as the name for the default branch. You need to pay attention to this, because when we will start interacting with our Github repository, we need to make sure that we have the right branch name in mind. Also, note that because the “master” branch is the most important branch, it get sometimes referred to as the “trunk”. Some teams that use trunk based development (which I will discuss in the next chapter) even name this branch “trunk”. Let’s now run this other git command:\nowner@localhost ➤ git status\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        analysis.R\n        save_data.R\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nGit tells us quite clearly that it sees two files, but that they’re currently not being tracked. So if we would modify them, Git would not keep track of the changes. So it’s a good idea to just do what Git tells us to do, let’s add them so that Git can track them:\nowner@localhost ➤ git add\nNothing specified, nothing added.\nhint: Maybe you wanted to say 'git add .'?\nhint: Turn this message off by running\nhint: \"git config advice.addEmptyPathspec false\"\nShoot, simply running git add does not do us any good. We need to specify which files we want to add. We can name them one by one, for example git add file1.R file2.txt etc, but if we simply want to track all the files in the folder, we can simply use the . placeholder:\nowner@localhost ➤ git add .\nNo message this time… is that a good thing? Let’s run git status and see what’s going on:\nowner@localhost ➤ git status\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached <file>...\" to unstage)\n        new file:   analysis.R\n        new file:   save_data.R\nNice! Our two files are being tracked now, we can commit the changes. Committing means that we are happy with our work, so we can snapshot it. These snapshots then get uploaded to Github by pushing them. This way, the changes will be available for our coworkers for them to pull. Don’t worry if this is confusing, it won’t be by the end of the chapter. So let’s commit them, but I need to tell you something else first: each commit must have a commit message, and we can write this message as an option to the git commit command:\nowner@localhost ➤ git commit -am \"Project start\"\nApparently the -am option stands for apply mailbox, which I’m sure makes sense to some people, but I prefer to think of -am as standing for add message. All that remains is pushing this commit to Github. But let’s run git status again:\nowner@localhost ➤ git status\nOn branch master\nnothing to commit, working tree clean\nThis means that every change is accounted for in a commit. So if we were to push now, we could then set our computer on fire: every change would be safely backed up on Github.com.\nBefore pushing, let’s see what happens if we change one file. Open “analysis.R” in any editor and simply change the start of the script by adding one line. So go from:\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidyr)\nTo:\n# This script analyses housing data for Luxembourg\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidyr)\nand now run git status again:\nowner@localhost ➤ git status\nOn branch master\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   analysis.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nBecause the file is being tracked, Git can now tell us that something changed and that we did not commit this change. So if our computer would self-combust, these changes would get lost forever. Better commit them and push them to Github.com as soon as possible!\nSo first, we need to add these changes to a commit using git add .:\nowner@localhost ➤ git add .\n(You can run git status at this point to check if the file was correctly added to be committed.)\nThen, we need to commit the changes and add a nice commit message:\nowner@localhost ➤ git commit -am \"Added a comment to analysis.R\"\nTry to keep commit message as short and as explicit as possible. This is not always easy, but it really pays off to strive for short, clear messages. Also, ideally, you would want to keep commits as small as possible. For example, if you’re adding and amending comments in scripts, once you’re done with that make this a commit. Then, maybe clean up some code. That’s another, separate commit. This makes rolling back changes or reviewing them much easier. This will be crucial later on when we will use trunk based development to collaborate with our teammates on a project. It is generally not a good idea to code all day and then only push one single big fat commit at the end of the day.\nBy the way, even if our changes are still not on Github.com, we can still now roll back to previous commits. For example, suppose that I delete the file accidentally by running rm analysis.R:\nowner@localhost ➤ rm analysis.R\nLet’s run git status and look for the changes (it’s a line starting with the word delete):\nOn branch master\nChanges not staged for commit:\n  (use \"git add/rm <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        deleted:    analysis.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nYep, analysis.R is gone. And deleting on the console usually means that the file is gone forever. Well technically no, there are still ways to recover deleted files, but sence we were using Git we can use it to recover the files! Because we did not commit the deletion of the file, we can simple tell Git to ignore our changes. A simple way to achieve this is to stash the changes, and then drop (or delete) the stash:\nowner@localhost ➤ git stash\nSaved working directory and index state WIP on master: ab43b4b Added a comment to analysis.R\nSo the deletion was stashed away, (so in case we want it back we could get it back with git stash pop) and our project was rolled back to the previous commit. Simply take a look at the files:\nowner@localhost ➤ ls\nanalysis.R save_data.R\nThere it is! You can get rid of the stash with git stash drop. But what if we had deleted the file and committed the change? In this scenario we could not use git stash, but we would need to revert to a commit. Let’s try, first let me remove the file:\nowner@localhost ➤ rm analysis.R\nand check the status with git status:\nOn branch master\nChanges not staged for commit:\n  (use \"git add/rm <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        deleted:    analysis.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nLet’s add these changes and commit them:\nowner@localhost ➤ git add .\nowner@localhost ➤ git commit -am \"Removed analysis.R\"\n[master 8e51867] Removed analysis.R\n 1 file changed, 131 deletions(-)\n delete mode 100644 analysis.R\nWhat’s the status now?\nowner@localhost ➤ git status\nOn branch master\nnothing to commit, working tree clean\nNow, we’ve done it! git stash won’t be of any help now. So how to recover our file? For this, we need to know to which commit we want to roll back. Each commit not only has a message, but also an unique identifier that you can access with git log:\nowner@localhost ➤ git log\ncommit 8e51867dc5ae89e5f2ab2798be8920e703f73455 (HEAD -> master)\nAuthor: User <owner@mailbox.com>\nDate:   Sun Feb 5 17:54:30 2023 +0100\n\n    Removed analysis.R\n\ncommit ab43b4b1069cd987685253632827f19d7a402b27\nAuthor: User <owner@mailbox.com>\nDate:   Sun Feb 5 17:41:52 2023 +0100\n\n    Added a comment to analysis.R\n\ncommit df2beecba0101304f1b56e300a3cd713ce7366e5\nAuthor: User <owner@mailbox.com>\nDate:   Sun Feb 5 17:32:26 2023 +0100\n\n    Project start\n\nThe first one from the top is the last commit we’ve made. We would like to go back to the one with the message “Added a comment to analysis.R”. See the very long string of characters after “commit”? That’s the commit’s unique identifier, called hash. You need to copy it (or only like the first 10 or so characters, that’s enough as well). By the way, depending on your terminal and operating system, git log may open less to view the log. less is a program that makes it easy to view long documents. Quit it by simply pressing q on your keyboard. We are now ready to revert to the right commit with the following command:\nowner@localhost ➤ git revert ab43b4b1069cd98768..HEAD\nand we’re done! Check that all is right by running ls to see that the file magically returned, and git log to read the log of what happened:\nowner@localhost ➤ git log\ncommit b7f82ee119df52550e9ca1a8da2d81281e6aac58 (HEAD -> master)\nAuthor: User <owner@mailbox.com>\nDate:   Sun Feb 5 18:03:37 2023 +0100\n\n    Revert \"Removed analysis.R\"\n    \n    This reverts commit 8e51867dc5ae89e5f2ab2798be8920e703f73455.\n\ncommit 8e51867dc5ae89e5f2ab2798be8920e703f73455 (HEAD -> master)\nAuthor: User <owner@mailbox.com>\nDate:   Sun Feb 5 17:54:30 2023 +0100\n\n    Removed analysis.R\n\ncommit ab43b4b1069cd987685253632827f19d7a402b27\nAuthor: User <owner@mailbox.com>\nDate:   Sun Feb 5 17:41:52 2023 +0100\n\n    Added a comment to analysis.R\n\ncommit df2beecba0101304f1b56e300a3cd713ce7366e5\nAuthor: User <owner@mailbox.com>\nDate:   Sun Feb 5 17:32:26 2023 +0100\n\n    Project start\nThis small example illustrates how useful Git is, even without using Github, and even if working alone on a project. At the very least it offers you a way to simply walk back changes and gives you a nice timeline of your project. Maybe this does not impress you much, because we live in a world where cloud services like Dropbox made things like this very accessible. But where Git (with the help of a service like Github) really shines is when collaboration is needed. Git and code housing services like Github make it possible to collaborate at very large scale: thousands of developers contribute to the Linux kernel, arguably the most successful open source project ever, powering most of today’s smartphones, servers, supercomputers and embedded computer,1 and you can use these tools to collaborate at a smaller scale very efficiently as well."
  },
  {
    "objectID": "git.html#git-and-github",
    "href": "git.html#git-and-github",
    "title": "4  Version control",
    "section": "4.3 Git and Github",
    "text": "4.3 Git and Github\nSo we got some work done on our machine and made some commits. We are now ready to push these commits to Github. “Pushing” means essentially uploading these changes to Github. This makes them available to your coworkers if you’re pushing to a private repository, or makes them available to the world if you’re pushing to a public repository.\nBefore pushing anything to Github though, we need to create a new repository. This repository will contain the code for our project, as well as all the changes that Git has been tracking on our machine. So if, for example, a new team member joins, he or she will be able to clone the repository to his or her computer and every change, every commit message and every single bit of history of the project will be accessible. If it’s a public repository, anyone will be able to clone the repository and contribute code to it. We are going to walk you true some examples of how to collaborate with Git using Github in the remained of this chapter.\nSo, let’s first go back to https://github.com/ and create a new repository:\n\n\n\nCreating a new repository from your dashboard.\n\n\nYou will then land on this page:\n\n\n\nName your repository and choose whether it’s a public or private repository.\n\n\nName your repository, and choose whether it should be open to the word or if it should be private and only accessible to your coworkers. We are going to make it a public repository, but you could make it private and follow along, this would change nothing to what we’re going to learn.\nWe then land on this page:\n\n\n\nSome instructions to get you started.\n\n\nWe get some instructions on how to actually get started with our project. The first thing you need to do though is to click on “SSH”:\n\n\n\nMake sure to select ‘SSH’.\n\n\nThis will change the links in the instructions from https to ssh. We will explain why this is important in a couple of paragraphs. For now, let’s read the instructions. Since we have already started working, we need to follow the instructions titled “…or push an existing repository from the command line”. Let’s review these commands. This is what Github suggests we run:\ngit remote add origin git@github.com:rap4all/housing.git\ngit branch -M main\ngit push -u origin main\nWhat’s really important is the first command and last command. The first command adds a remote that we name origin. The link you see is the link to our repository. If you’re following along, you should copy the link from your repository here. It would look exactly the same, but the user name rap4all would be replaced by your Github username.\nAdding the remote links to our folder in our machine to the Github repository online. So now, every time we push, our changes will get uploaded to Github. The second line renames the branch from “master” to “main”. You are of course free to do so. I don’t like changing the defaults from Git, so I will keep using the name “master”. The last command pushes our changes to the “main” branch (but we need to change “main” to “master”).\nLet’s do just that:\nowner@localhost ➤ git remote add origin git@github.com:rap4all/housing.git\nThis produces no output. We’re now ready to push:\nowner@localhost ➤ git push -u origin master\nand it fails:\nERROR: Permission to rap4all/housing.git denied to b-rodrigues.\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.\nThe reason is quite simple: Github has absolutely no idea who we are! Remember, if the repository is public, anyone can clone it. But that doesn’t mean that anyone can simply push code to the repo! This means that we need a way to tell Github that we are the owner of the repository. For this, we need a way to log in securely, and we will do so using a public/private rsa key pair. The idea is quite simple; we are going to generate two files on our computer. These two files form a public/private key pair. We are going to upload the public key to Github; and every time we want to interact with Github, Github will check the public key to the private key that we keep on our machine (never, ever, send the private key to anyone). If they match, Github knows that we are who we claim to be and will let us push to the repository. This is why we switched from https to ssh before. https would allow us to log in by typing a password each time we push (but actually, not anymore, since password login was turned off some years ago). It is much easier to not have to log in manually and let our key pair do the job for us.\nLet’s generate a public/private rsa key pair. Open a terminal on Linux or macOS, or Git Bash on Windows and run the following command:\nowner@localhost ➤ ssh-keygen\nThe following lines will appear in your terminal:\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa): \nSimply leave this empty and press enter. This next message now appears:\nEnter passphrase (empty for no passphrase): \nLeave it empty as well. Entering a passphrase is not really needed, since the ssh key pair itself will deal with the login. In some situations, a passphrase might be useful if you’re worried that someone might get physical access to your machine and push code by impersonating you. But if you work with such sensitive data and code that this is a real worry, maybe don’t use Github?\nSo once you pressed enter, you get asked to confirm the passphrase:\nEnter same passphrase again: \nHere again, simply leave it empty and press enter on your keyboard. Once this is done, you should see this:\nYour identification has been saved in /home/user/.ssh/id_rsa\nYour public key has been saved in /home/user/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:tPZnR7qdN06mV53Mc36F3mASIyD55ktQJFBAVqJXNQw owner@localhost\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .*=E*=.       |\n|   o o.oo.. .    |\n|  . .  o.  o o   |\n|   .  ..o.  . o  |\n|       +S    o.+.|\n|       .o.   o.o*|\n|       . o. + +=*|\n|        .  o ++*=|\n|            ..=oo|\n+----[SHA256]-----+\nIf now you go to the specified path on the first line (so in our case /home/user/.ssh/ you should see two files, id_rsa and id_rsa.pub, the private and public keys respectively. We’re almost done: what you need to do now is copy the contents of the id_rsa.pub file to Github. Go to your profile settings:\n\n\n\nClick on your user profile’s image in the top-right corner.\n\n\nAnd then click on “SSH and GPG keys”:\n\n\n\nGo to your user settings and choose ‘SSH and GPG keys’.\n\n\nand then click on “New SSH key”. Name this key (it’s a good idea to write something that makes recognizing the machine the key was generated easily) and paste the contents of id_rsa.pub in the text box and click on “add SSH key”:\n\n\n\nCopy the contents of the public key here.\n\n\nWe can now go back to our terminal and try to push again:\nowner@localhost ➤ git push -u origin master\nThe following message gets printed:\nThe authenticity of host 'github.com (140.82.121.3)' can't be established.\nED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nType yes and then you should see the following:\nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (9/9), done.\nWriting objects: 100% (10/10), 2.77 KiB | 2.77 MiB/s, done.\nTotal 10 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), done.\nTo github.com:rap4all/housing.git\n * [new branch]      master -> master\nBranch 'master' set up to track remote branch 'master' from 'origin'.\nAnd we’re done! Our commits are now safely backed up on Github. If we go to our repository’s main page, we should see the following:\n\n\n\nFinally!"
  },
  {
    "objectID": "git.html#getting-to-know-github",
    "href": "git.html#getting-to-know-github",
    "title": "4  Version control",
    "section": "4.4 Getting to know Github",
    "text": "4.4 Getting to know Github\nWe have succeeded in installing Git and making it work with our Github account. If you use another machine for development, you will need to generate another rsa key pair on that machine and add the public key to Github. If you use another code hosting platform, you can use the same rsa key pair, but will need to add the public key to this other code hosting platform. You can even use the same key pair as a passwordless authentication method for ssh (for example to log into a server, but this is outside the scope of the present book). Before continuing we are going to take a little tour of Github.\n\n\n\nYou repository’s landing page.\n\n\nOnce you’re on your repository’s landing page you see the same files and folders as in the root directory of the project on your computer. In our case here, we see our two files. Github suggests that we add a README file; we are going to ignore this for now. Take a closer look at the menu at the top, below your repository’s name:\n\n\n\nSeveral options to choose from.\n\n\nMost important for our needs is the “Issues”, “Pull requests”, “Actions” and “Settings” tab.\nIn the next chapter we are going to learn about pull requests which are essential for collaborating using Git and Github.com. We will learn about the “Actions” tab in the second part of the book.\nSo let’s start with “Settings”.\n\n\n\nChoose the ‘Settings’ tab.\n\n\nThere are many options that you can choose from, but what’s important for our purposes is the “Collaborators” option. This is where you can invite people to contribute to the repository. People that are invited in this way can directly push to the repository. Let’s invite the author of this book:\n\n\n\nFollow along to add a collaborator.\n\n\nStart by typing the person’s Github username. You can also invite collaborators by providing their email address.\n\n\n\nLook for your collaborators.\n\n\nClick then on the user’s profile and he or she should get an invitation per email.\nThis is what it looks like from the perspective of Bruno’s account now:\n\n\n\nBruno can now push as if he owned the repository.\n\n\nIt’s important to understand the distinction between inviting someone to contribute to the repository and have someone from outside the project contribute. We are going to explore these two scenarios in the next section, but before that, let’s see what the “Issues” tab is about.\nIf the repository is public, anyone can open an issue to either submit a bug, or suggest some ideas, and if the repository is private, only invited collaborators can do this.\nLet’s open an issue to illustrate how this works:\n\n\n\nClick on ‘New issue’ in the ‘Issues’ tab of your project.\n\n\nYou will land on this interface:\n\n\n\nWrite what the issue’s about here.\n\n\nGive a nice title to the issue (1), add a thorough description (2), (optionally) assign it to someone (3) and (optionally) add a label to it (4), finally click on “Submit new issue” (5) to submit the issue:\n\n\n\nTry to provide as many details as possible.\n\n\nSometimes issues don’t need to be very long, and act more as reminders than anything else. For example here, the owner of the repository didn’t have the time to add a Readme, but didn’t want to forget to add one later on. The author assigned the issue to Bruno: so it’ll be Bruno’s job to add the Readme. Issue-driven project management is a very valid strategy when working asynchronously and in a decentralized fashion.\nIf you encountered a bug and want to open an issue, it is very important that you provide a minimal, reproducible example (MRE). MREs are snippets of code that can be run very easily by someone other than yourself and which produce the bug reliably. Interestingly, if you understand what makes an MRE minimal and reproducible, you understand what will make our pipelines reproducible as well. So what’s important for an MRE?\nFirst, the code needs to be self-contained. For example, if some data is required you need to provide the data. If the data is sensitive, you need to think about the bug in greater detail: is the bug due to the structure of the data, or does the bug manifest itself on any kind of data? If that’s the case, use some of the built-in datasets to R (iris, mtcars, etc) for your MRE.\nDoes your MRE require extra packages to run? Then make this as clear as possible, and not only provide the package names, but also their versions (it is a good idea to copy and paste the output of sessionInfo() at the end of the issue.\nFinally, does your example depend on some object defined in the global state? If yes, you also need to provide the code to create this object.\nThe bar you need to set for an MRE is as follows: bar needed package dependencies that may need to be installed beforehand, people that try to help you should be able to run your script by simply copy-and-pasting it into an R console. Any other manipulation that you require from them is unacceptable: remember that in open source development, developers very often work during their free time, and don’t owe you tech support! And even if they did, it is always a good idea to make it as easy as possible for them to help you, because it simply increases the likelihood that they will actually help.\nAlso, writing an MRE can usually make you actually debug the code yourself. Just like in rubber duck debugging, the fact of simply trying to explain the problem can lead to finding what’s wrong. But by writing an MRE, you’re also reducing the problem into its most basic parts, and removing everything unnecessary. By doing so, you might realize that what you thought was a bug of the library was maybe rather a problem between the keyboard and the chair.\nSo don’t underestimate the usefulness of creating high-quality MREs for your issues! One package that can assist you with this is {reprex} (read about it here)."
  },
  {
    "objectID": "git.html#conclusion",
    "href": "git.html#conclusion",
    "title": "4  Version control",
    "section": "4.5 Conclusion",
    "text": "4.5 Conclusion\nYou should now have your first repository and know the very basics of using Git and Github.com. If you did not understand everything, take some time to rerun the commands from above. Maybe add some more files to your repo, remove them, try to revert certain commits, etc. Create a new repo and try to push some files or scripts to it. Really take the time to understand what is going on and how to use these tools, because they are essential for reproducibility."
  },
  {
    "objectID": "github.html#collaborating-as-a-team-using-trunk-based-development",
    "href": "github.html#collaborating-as-a-team-using-trunk-based-development",
    "title": "5  Collaborating with Github",
    "section": "5.1 Collaborating as a team using trunk-based development",
    "text": "5.1 Collaborating as a team using trunk-based development\n\n5.1.1 TBD basics\nRemember the issue we opened and assigned to Bruno? Bruno will now solve this issue by adding a Readme. This will be also the opportunity to introduce trunk-based development. The idea of trunk-based development is simple; team members should work on separate branches to add features or fix bugs, and then merge their branch to the “trunk” (in our case the master branch) to add their changes back to the main code-base. And this process should happen quickly, ideally every day, or as soon as some code is ready. This way, if conflicts arise, they can be dealt with quickly. This also makes code review much easier, because the reviewer only needs to review little bits of code at a time. The alternative would be for each team member to work on his or her own branch for days or even weeks. Once the branches get merged into the trunk, reviewing all the changes and solving the conflicts that will arise would be very painful. To avoid this, it is best to merge every day or each time a piece of code is added, and, very importantly, this code does not break the whole project (we will be using unit tests for this later).\nSo in summary: to avoid a lot of pain later by merging branches that moved away too much from the trunk, we will create branches, add our code, and merge them to the trunk as soon as possible. As soon as possible can mean several things, but usually this means as soon as the feature was added, bug was fixed, or as soon as we added some code that does not break the whole project, even if the feature we wanted to add is not done yet. The philosophy is that if merging fails, it should fail as early as possible. Early failures are easy to deal with.\nSo, back to our issue. First, Bruno needs to clone the repository:\ngit clone git@github.com:rap4all/housing.git\nBecause Bruno was added as a collaborator, Bruno can work on the repository just like the author.\nBruno will now create a new branch by using the git checkout command with the -b flag:\nbruno@computer ➤ git checkout -b \"add_readme\"\nThe project automatically switches to the new branch:\nSwitched to a new branch 'add_readme'\nWe can also run git status to double-check:\nbruno@computer ➤ git status\nOn branch add_readme\nnothing to commit, working tree clean\nLet’s add a file called README.md and add the following to it:\n# Housing data for Luxembourg\n\nThese scripts for the R programming language download nominal housing prices\nfrom the *Observatoire de l'Habitat* and tidy them up into a flat data frame.\n\n- save_data.R: downloads, cleans, and creates data frames from the data\n- analysis.R: creates plots of the data\nLet’s save this and run git status to see what happened:\nbruno@computer ➤ git status\nGit tells us that the README.md file is not being tracked:\nOn branch add_readme\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        README.md\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nSo next we are going to track it and push the changes. Also, we are going to use a neat trick when pushing: we are going to use the commit message to state the issue was fixed, which will automatically close the issue for us:\nbruno@computer ➤ git add .\nbruno@computer ➤ git commit -am \"fixed #1\"\n#1 refers to the number of the issue. Because it’s the first issue, it can simply be referred to as #1. Bruno will now push:\nbruno@computer ➤ git push origin add_readme\nAs you can see from the command above, Bruno pushes to “add_readme”, not “master”. If he tried to push to “master” a message saying that “master” is up-to-date would get printed. Let’s see the output of pushing to “add_readme”:\nEnumerating objects: 4, done.\nCounting objects: 100% (4/4), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 501 bytes | 501.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0\nremote: \nremote: Create a pull request for 'add_readme' on GitHub by visiting:\nremote:      https://github.com/rap4all/housing/pull/new/add_readme\nremote: \nTo github.com:rap4all/housing.git\n * [new branch]      add_readme -> add_readme\nGit tells us that we now need to create a pull request. What is that? Well, if we want to merge our brunch back to the trunk, we need to do so by using a pull request. Let’s see what Bruno sees on Github:\n\n\n\nBruno sees that the ‘add_readme’ branch has been recently updated.\n\n\nBruno can now decide to continue working on this branch, or, since the purpose of this branch was only to add the readme file, decide instead to do a pull request. By clicking on the “Compare & pull request” button Bruno now sees this:\n\n\n\nThis screen makes it easy to see what changed.\n\n\nBruno can leave a comment, and see what changed (in this case, a single file was added) and most importantly, add a reviewer if needed:\n\n\n\nLet boss decide if this is good enough.\n\n\nThis is what Bruno sees now:\n\n\n\nGithub tells us that this branch can safely be merged.\n\n\nBruno requested the review, but Github tells us that the branch can safely be merged. This is because we added a file and did not touch anything else, and also because the owner of the repository was asleep while Bruno was working, so there was no opportunity for conflicts to arise.\nLet’s see what the owner now sees. The owner should have gotten a notification to review the pull request:\n\n\n\nThe owner was notified to review the pull request.\n\n\nBy clicking on the notification, the owner gets taken to this view:\n\n\n\nTime to review the pull request.\n\n\nHere, the reviewer can check the commit, the files that were changed, and see if there are any conflicts between this code and the code base on the master (or trunk) branch. Github also tells us two interesting things: the owner can add a rule that states that any pull request must be approved, and also that continuous integration has not been set up (we are going to see what this means in the second part of this book).\nLet’s go ahead and add a rule that each pull request has to be approved. By clicking on “Add rule”, the following screen appears:\n\n\n\nChoose how to protect the master branch.\n\n\nBy clicking the first option, more options appear:\n\n\n\nReviews are now required.\n\n\nBy choosing these options, the owner can basically enforces trunk-based development (well, collaborators still have to submit pull requests frequently enough though, because if they don’t, we can be in a situation where merging can be very difficult).\nLet’s choose one last option: by scrolling down, it’s possible to select the option “Do not allow bypassing the above settings”. This makes sure that even administrations (the owners of the project) must abide by the same rules.\nLet’s go back to the pull request. We can see now that the review is required:\n\n\n\nTime to review.\n\n\nSo now the owner actually has to go and see the files that were changed:\n\n\n\nCheck the code, and add comments if needed.\n\n\nIt’s possible to add comments to single lines if needed:\n\n\n\nIt’s possible to add comments to lines.\n\n\nBy clicking on the plus sign, a box appears and it’s possible to leave a comment. In this case, everything is fine, so the owner is going to click on the “Viewed” button:\n\n\n\nGood job!\n\n\nThen, by clicking on “Review changes”, it’s possible to either add a general comment, approve the pull request, or request changes that must be addressed before merging. Let’s go ahead and approve:\n\n\n\nNothing to complain about.\n\n\nBy submitting the review, the reviewer is taken back to the issue:\n\n\n\nWe’re done, we can merge the pull request.\n\n\nThe reviewer can now merge the pull request by clicking on the “Merge pull request” button. Github even suggests we deleted the branch, which served its purpose:\n\n\n\nLet’s get rid of this branch.\n\n\nLet’s delete it (it’s always possible to restore it).\n\n\n5.1.2 Handling conflicts\nAs mentioned in the previous chapter, Git makes it easy to handle conflicts. Well, let’s be clear; it can be very tricky sometimes to resolve conflicts. But you should know that when solving a conflict with Git is difficult, this usually means that it would be impossible to do any other way, and would inevitably result in someone having to reconcile the files by hand. What makes handling conflicts easier with Git though, is that Git is able to tell you where you can find clashes on a per-line basis. So for instance, if you change the ten first lines of a script, and I change the ten next lines, there would be no conflict, and Git will automatically merge both our contributions into a single file. Other tools, like Dropbox, would fail in a situation like this, because these tools can only handle conflicts on a per file basis. The same file was changed by two different persons? Regardless of where these changes happened, you now have a conflict to deal with on your hand… and worse, you don’t even know where! You will need to scan the two resulting copies of the file by hand. Git, in the case where the same lines were changed, highlights them very clearly so that you can quickly find them and deal with the problems.\nWe will see all of this in the coming sections.\nSo how do conflicts happen in practice? Let’s imagine the following scenario. Both Bruno and the project owner will create a branch, and edit the same file. Perhaps they talked over the phone and decided to add a feature or correct a bug. Perhaps they decided that it wasn’t worth it to open an issue on Github and assign someone to do it. After all, they discussed this on the phone and decided that Bruno should do it. Or was it the owner who needed to solve the issue? No one remembers now. Either way, they both did, and changed the same file, so a conflict will ensue.\nFirst, Bruno needs to switch back to the master branch on his computer:\nbruno@computer ➤ git checkout master\nSwitched to branch 'master'\nYour branch is behind 'origin/master' by 2 commits, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\nGit tells us to update the code on our computer by running git pull. We use git push to upload code to Github, and use git pull to download code from Github. Let’s run it and see what happens:\nbruno@computer ➤ git pull\nUpdating b7f82ee..c774ebf\nFast-forward\n README.md | 7 +++++++\n 1 file changed, 7 insertions(+)\n create mode 100644 README.md\nThe owner of the project (called owner, remember?) can do the same and will see the same. Now, Bruno creates a new branch to work on the new feature:\nbruno@computer ➤ git checkout -b add_cool_feature\nAnd the project owner also create a new branch:\nowner@localhost ➤ git checkout -b add_nice_feature\nThey now edit the same file, analysis.R. Bruno added this function:\n\nmake_plot <- function(country_level_data,\n                      commune_level_data,\n                      commune){\n\n  filtered_data <- commune_level_data %>%\n    filter(locality == commune)\n\n  data_to_plot <- bind_rows(\n    country_level_data,\n    filtered_data\n  )\n\n  ggplot(data_to_plot) +\n    geom_line(aes(y = pl_m2,\n                  x = year,\n                  group = locality,\n                  colour = locality))\n}\n\nThis way, Bruno could delete the repeating code and create plots like this:\n\nlux_plot <- make_plot(country_level_data,\n                      commune_level_data,\n                      communes[1])\n\n\n# Esch sur Alzette\n\nesch_plot <- make_plot(country_level_data,\n                       commune_level_data,\n                       communes[2])\n\n# and so on...\n\nThe end effect is the same, but by using this function, the code is now shorter, and clearer. Also, if someone wants to change, say, the theme of the plot, now this only needs to be changed in one place and not for each commune. Now, what did the owner change? The owner started by removing the line that loaded the {purrr} package, as no function from the package was used in the script, and then also changed every %>% to |>. It seems that much more than just who would make the changes got lost in translation… Anyways, both now push their changes to their respective branches. This is Bruno:\nbruno@computer ➤ git add .\nbruno@computer ➤ git commit -am \"make_plot() for plotting\"\nbruno@computer ➤ git push origin add_cool_feature\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 647 bytes | 647.00 KiB/s, done.\nTotal 3 (delta 1), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote: \nremote: Create a pull request for 'add_cool_feature' on GitHub by visiting:\nremote:      https://github.com/rap4all/housing/pull/new/add_cool_feature\nremote: \nTo github.com:rap4all/housing.git\n * [new branch]      add_cool_feature -> add_cool_feature\nand this is the owner:\nowner@localhost ➤ git add .\nowner@localhost ➤ git commit -am \"cleanup\"\nowner@localhost ➤ git push origin add_sweet_feature\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 449 bytes | 449.00 KiB/s, done.\nTotal 3 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote: \nremote: Create a pull request for 'add_sweet_feature' on GitHub by visiting:\nremote:      https://github.com/rap4all/housing/pull/new/add_sweet_feature\nremote: \nTo github.com:rap4all/housing.git\n * [new branch]      add_sweet_feature -> add_sweet_feature\nSo, let’s think about what just happened: two developers changed the same file, analysis.R in two separate branches. They did what they had to do, and now these two branches need to be merged back to the trunk. So Bruno does a pull request:\n\n\n\nBruno opens a pull request after finishing his changes.\n\n\nFirst, Bruno selects the feature branch (1), then clicks on “Contribute” (2) and then “Open pull request” (3). Bruno gets taken to this screen:\n\n\n\nNo conflicts, for now…\n\n\nNow Bruno can click on “Create pull request”, but remember, because reviews are required, automatic merging is disabled.\nIf now we go see what happens from the project owner’s side of things, first of all, there’s now a notification for a pending review:\n\n\n\nLet’s get rid of this branch.\n\n\nBy clicking on it, the project owner can review the pull request and decide what to do with it. So at this point, the owner did not open a pull request for the feature he or she worked on yet. And maybe that’s a good thing, because now the project owner can see that the changes that Bruno made on the file will conflict with the project owner’s changes.\nSo how to move forward? Simple: the project owner can decide to approve the pull request, which will merge Bruno’s changes into the master branch (or the trunk). Then, instead of opening a pull request for merging his or her changes into trunk, which will cause a conflict, the project owner can instead merge the changes from the trunk into his or her feature branch. This will also create a conflict, but now the project owner can easily deal with it on his or her machine, and then push a new commit with both changes integrated gracefully. The image below illustrates this workflow:\n\n\n\nConflict solving with trunk-based development.\n\n\nFirst step, the owner reviews and approves Bruno’s pull request:\n\n\n\nFirst, let’s approve the changes.\n\n\nThe pull request can get merged and Bruno’s feature branch deleted. Now, it wouldn’t make sense for the project owner to create a pull request to merge his or her changes. They would conflict with what Bruno did. So the project owner goes back to the computer and essentially updates the code in his or her feature branch by merging master into it.\nSo, the project owner checks that he or she is working on the feature branch:\nowner@localhost ➤ git status\nOn branch add_sweet_feature\nnothing to commit, working tree clean\nOk, so now let’s get the updated code from master, by pulling from master:\nowner@localhost ➤ git pull origin master\nThe owner now sees this:\nremote: Enumerating objects: 6, done.\nremote: Counting objects: 100% (6/6), done.\nremote: Compressing objects: 100% (3/3), done.\nremote: Total 4 (delta 1), reused 3 (delta 1), pack-reused 0\nUnpacking objects: 100% (4/4), 1.23 KiB | 418.00 KiB/s, done.\nFrom github.com:rap4all/housing\n * branch            master     -> FETCH_HEAD\n   c774ebf..a43c68f  master     -> origin/master\nAuto-merging analysis.R\nCONFLICT (content): Merge conflict in analysis.R\nAutomatic merge failed; fix conflicts and then commit the result.\nGit detect that there’s some conflicts and tells the owner to fix them, and then commit the results. So let’s open analysis.R and see how it looks like (you can view the file online on this link1. First of all, you will see Git deals with conflicts on a per line basis. So each line that the owner changed that does not conflict with Bruno’s change gets immediately updated to reflect the owner’s changes. For example, remember that the owner removed the line that loaded the {purrr} package? This line was also removed by pulling the changes from master into the feature branch. Also, you should notice that every %>% was changed into |> as well.\nThen, you should understand what happens when a conflict gets detected on some lines. For example, this is the first conflict you should see:\n<<<<<<< HEAD\nfiltered_data <- commune_level_data |>\n  filter(locality == communes[1])\n=======\n  filtered_data <- commune_level_data %>%\n    filter(locality == commune)\n>>>>>>> a43c68f5596563ffca33b3729451bffc762782c3\nWe both see how the lines look on the owner’s computer and how they look in the master branch (or trunk). These are the lines between <<<<<<< HEAD and =======. The lines between ======= and >>>>>>> a43c68f5596563ffca33b3729451bffc762782c3 are how they look in the master branch (or trunk). This very long chain of characters that starts with a43c68f is the hash of the commit from which these lines come from.\nSo this makes things quite easy; one simply needs to remove the outdated code, and then commit and push the fixed file! The project owner only needs to remove <<<<<<< HEAD and ======= and what’s between these lines, as well as the lines that show the hash commit. The project owner can now commit and push the changes, open a pull request, ask Bruno to review the changes one last time and merge everything back to master.\n\n\n\nThe conflict has been gracefully solved.\n\n\nIn (1) we see the commit that deals with the conflict, in (2) the owner asks Bruno for a review and then in (3) we see that Bruno reviewed and approved. Finally, the pull request can be merged (4) and the feature branch deleted.\n\n\n5.1.3 Make sure you blame the right person\nIf many people contribute to a single project, it might sometimes be difficult to know who changed what and when exactly. This is where the git blame command is useful. If you want to know who changed the file called analysis.R for example, simply run:\nowner@localhost ➤ git blame analysis.R\nand you will see a detailed history, line by line, with the user name of the contributors and a date stamp:\nb7f82ee1 (Bruno   2023-02-05 18:03:37 +0100 24) #Let’s also compute it for the whole country:\nb7f82ee1 (Bruno   2023-02-05 18:03:37 +0100 25) \n55804ccb (Owner   2023-02-11 22:33:20 +0000 26) country_level_data <- country_level_data |>\n55804ccb (Owner   2023-02-11 22:33:20 +0000 27)   mutate(p0 = ifelse(year == \"2010\", average_price_nominal_euros, NA)) |>\nWe can see that Bruno edited lines 24 and 25 on the 5th of February as part of the commit with the hash b7f82ee1, while the owner of the repository changed lines 26 and 27 on the 11th of February as part of the commit with the hash 55804ccb.\nTake advantage of git blame to have a clear overview of each file’s changes.\n\n\n5.1.4 Simplified trunk-based development\nThe workflow that we showed here may seem a bit too rigid for smaller teams (below 4 or 5 contributors). It is possible to adopt a simplified version of trunk-based development, where contributors don’t have to open pull requests to merge their feature branches into the trunk, and no reviewer is needed. In cases like this, Git forces you to pull changes if someone already merged his or her feature branch into the trunk before you could. This way, when pulling, conflicts (if any) arise at that point. It is then your responsibility to solve the conflicts (and this works just like in the previous section) and then commit and push the commits with the conflicts resolved. Another contributor who then wishes to merged his or fear feature branch into to the trunk will have to pull again, ensuring that conflicts get resolved before he or she can merge. If no conflicts arise (for example, you both worked on different files, or on different lines of the same files), then no resolution is needed and the feature branch can be merged into master.\n\n\n5.1.5 Conclusion\nThe main ideas of trunk-based development are:\n\nEach contributor opens a new branch to develop a feature or fix a bug, and works alone on his or her own little branch;\nAt the end of the day at the latest (or a previously agreed upon duration), branches need all to get merged;\nConflicts need to be taken care of at that point;\nIf adding a feature would take more time than just one day, then the task needs to be split in a manner that small contributions can be merged daily. In the beginning, these contributions can be simple placeholders that will be gradually enriched with functioning code until the feature is successfully implemented. This strategy is called branching by abstraction;\nThe master branch (or trunk) contains always working, production-grade code;\nTo enforce discipline, it might be worth it to make opening pull requests mandatory for merging back to the trunk, and require a review."
  },
  {
    "objectID": "github.html#contributing-to-public-repositories",
    "href": "github.html#contributing-to-public-repositories",
    "title": "5  Collaborating with Github",
    "section": "5.2 Contributing to public repositories",
    "text": "5.2 Contributing to public repositories\nIn this last section, we are going to briefly discuss how to contribute to a project when we are not a team member of that project. For example, maybe we use an R package and notice a bug, and want to propose a fix. Or maybe we simply spotted a typo in the README of said package, and want to propose a correction. Whatever it may be, if the repository is public, anyone can propose a fix. For example, consider this repository:\n\n\n\nA public repository.\n\n\nThis repository contains code written by a fellow called “rap4all”, and Bruno uses this code daily. However, Bruno notices a typo in the readme, and wants to propose a fix.\nFirst, Bruno visits the repository on Github (since it’s a public repository, anyone can view it online) and creates a fork:\n\n\n\nBruno needs to create a fork of the repository.\n\n\nForking creates a copy of the repository to Bruno’s account:\n\n\n\nBruno goes ahead with forking.\n\n\nBruno now sees the fork on his account as well:\n\n\n\nBruno’s fork.\n\n\nSo now, Bruno can clone this repository and work on it, because he is working on a copy of the repository that he owns. Anything Bruno does on this copy will not affect the original repository.\nbruno@computer ➤ git clone git@github.com:b-rodrigues/my_cool_project.git \nBruno now fixes the typo in the README.md file, commits and pushes to his fork:\n\n\n\nBruno fixed the typo in his fork.\n\n\nAs you can see, Bruno’s fork is now ahead of the original repo by one commit. By clicking on “Contribute”, Bruno can open a pull request to propose his fix to the original repository. This pull request will be opened over at the original repository:\n\n\n\nBruno opens a pull request to contribute his fix upstream.\n\n\nWhat does the owner of the original repository, “rap4all” see? The pull request Bruno opened is now in the original repository’s “Pull request” menu, and the owner can check what the contribution is, if it breaks code or not, etc. This is essentially the same workflow as the one presented before in trunk-based development with pull requests and reviews before merging (minus the forking of the repository).\n\n\n\nThe owner of the original repository can now accept Bruno’s fix.\n\n\nBy merging the fix, the owner can now benefit from a grammatically correct Readme file as well:\n\n\n\nThe beauty of open source."
  },
  {
    "objectID": "github.html#further-reading",
    "href": "github.html#further-reading",
    "title": "5  Collaborating with Github",
    "section": "5.3 Further reading",
    "text": "5.3 Further reading\nTo know everything about trunk-based development, check out Hammant (2020). A free, online, version of the book is available at https://trunkbaseddevelopment.com/.\n\n\n\n\nHammant, Paul. 2020. Trunk-Based Development and Branch by Abstraction. Leanpub."
  },
  {
    "objectID": "fprog.html#introduction",
    "href": "fprog.html#introduction",
    "title": "6  Functional programming",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nRemember that the philosophy of part one of this book is “don’t repeat yourself”. In this chapter we will see how we can reduce the amount of code as much as possible. In the previous chapter we’ve seen how Bruno was able to get rid of many lines of code (that were all the same) by writing a single function:\n\nmake_plot <- function(country_level_data,\n                      commune_level_data,\n                      commune){\n\n  filtered_data <- commune_level_data %>%\n    filter(locality == commune)\n\n  data_to_plot <- bind_rows(\n    country_level_data,\n    filtered_data\n  )\n\n  ggplot(data_to_plot) +\n    geom_line(aes(y = pl_m2,\n                  x = year,\n                  group = locality,\n                  colour = locality))\n}\n\nNow we are going to go one step further and not only learn how to write good functions, but also how we can push the concept of “not repeating oneself” to the extreme by using higher-order fuctions and function factories.\nYou are very likely already familiar with at least two elements of functional programming: functions and lists. But functional programming is a complete programming paradigm, so using functional programming is more than simply using functions and lists (which you can use with other programming paradigms as well). Programming paradagims are ways to structure programs (or scripts).\nFunctional programming is a paradigm that relies exclusively on the evaluation of functions to achieve the desired result. If you have already written your own functions in the past, what follows will not be very new. But in order to write a good functional program, the functions that you write and evaluate have to have certain properties. Before discussing these properties, let’s start with state.\n\n6.1.1 The state of your program\nLet’s suppose that you start a fresh R session, and immediately run this line:\n\nls()\n\nIf you did not modify any of R’s configuration files that get automatically loaded on startup, you should see the following:\n\ncharacter(0)\n\nLet’s suppose that now you load some data:\n\ndata(mtcars)\n\nand define a variable a:\n\na <- 1\n\nRunning ls() now shows the following:\n\n[1] \"a\"      \"mtcars\"\n\nYou have just altered the state of your program. You can think of the state as a box that holds everything that gets defined by the user and is accessible at any time. Let’s now define a simple function that prints a sentence:\n\nf <- function(name){\n  print(paste0(name, \" likes lasagna\"))\n}\n\nf(\"Bruno\")\n\nand here’s the output:\n\n[1] \"Bruno likes lasagna\"\n\nLet’s run ls() again:\n\n[1] \"a\"      \"f\"      \"mtcars\"\n\nFunction f() is now listed there as well. This function has two nice properties:\n\nFor a given input, it always returns exactly the same output. So f(\"Bruno\") will always return “Bruno likes lasagna”.\nWhen running this function, the state of the program does not get altered in any way.\n\n\n\n6.1.2 Predictable functions\nLet’s now define another function called g(), which does not have the same properties as f(). First, let’s define a function which does not always return the same output given a particular input:\n\ng <- function(name){\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n  print(paste0(name, \" likes \", food))\n}\n\nFor the same input, “Bruno”, this function now produces (potentially) a different output:\n\ng(\"Bruno\")\n[1] \"Bruno likes lasagna\"\n\n\ng(\"Bruno\")\n[1] \"Bruno likes feijoada\"\n\nAnd now let’s consider function h() that modifies the state of the program:\n\nh <- function(name){\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  if(exists(\"food_list\")){\n    food_list <<- append(food_list, food)\n  } else {\n    food_list <<- append(list(), food)\n  }\n\n  print(paste0(name, \" likes \", food))\n}\n\nThis function uses the <<- operator. This operator saves definitions that are made inside the body of functions[^The body of a function are all the instructions that go between the curly braces.] in the global environment. Before calling this function, run ls() again. You should see the same objects as before, plus the new functions we’ve defined:\n\n[1] \"a\"         \"f\"          \"g\"         \"h\"         \"mtcars\"   \n\nLet’s now run h() once:\n\nh(\"Bruno\")\n[1] \"Bruno likes feijoada\"\n\nAnd now ls() again:\n\n[1] \"a\"         \"f\"         \"food_list\" \"g\"         \"h\"         \"mtcars\" \n\nRunning h() did two things: it printed the message, but also created a variable called “food_list” in the global environment with the following contents:\n\nfood_list\n\n\n[[1]]\n[1] \"feijoada\"\n\nLet’s run h() again:\n\nh(\"Bruno\")\n[1] \"Bruno likes cassoulet\"\n\nand let’s check the contents of “food_list”:\n\nfood_list\n\n\n[[1]]\n[1] \"feijoada\"\n\n[[2]]\n[1] \"cassoulet\"\n\nIf you keep running h(), this list will continue growing. Let me just say that I hesitated to show you this; this is because if you didn’t know <<-, you might find the example above useful. But while useful, it is quite dangerous as well. Generally, we want to avoid using functions that change the state as much as possible because these functions are unpredictable, especially if randomness is involved. It is much safer to define h() like this instead:\n\nh <- function(name, food_list = list()){\n\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  food_list <- append(food_list, food)\n\n  print(paste0(name, \" likes \", food))\n\n  food_list\n}\n\nThe difference now is that we made food_list the second argument of the function. Also, we defined it as being optional by writing:\n\nfood_list = list()\n\nThis means that if we omit this argument, the empty list will get used by default. This avoids the users from having to manually specify it.\nWe can call it like this:\n\nfood_list <- h(\"Bruno\", food_list) # since food_list is already defined,\n                                   # we don't need to start with an empty list\n\n\n[1] \"Bruno likes feijoada\"\n\nWe save the output back to food_list. Let’s now check its contents:\n\nfood_list\n\n\n[[1]]\n[1] \"feijoada\"\n\n[[2]]\n[1] \"cassoulet\"\n\n[[3]]\n[1] \"feijoada\"\n\nThe only thing that we need now to deal with is the fact that the food item gets chosen randomly. I’m going to show you the simple way of dealing with this, but later in this chapter we are going to use the {withr} package for situations like this. Let’s redefine h() one last time:\n\nh <- function(name, food_list = list(), seed = 123){\n\n  # We set the seed, making sure that we get the same\n  # selection of food for a given seed\n\n  set.seed(seed)\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  # We now need to unset the seed, because if we don't,\n  # guess what, the seed will stay set for the whole session!\n\n  set.seed(NULL)\n\n  food_list <- append(food_list, food)\n\n  print(paste0(name, \" likes \", food))\n\n  food_list\n}\n\nLet’s now call h() several times with its default arguments:\n\nh(\"Bruno\")\n\n\n[1] \"Bruno likes feijoada\"\n[[1]]\n[1] \"feijoada\"\n\n\nh(\"Bruno\")\n\n\n[1] \"Bruno likes feijoada\"\n[[1]]\n[1] \"feijoada\"\n\n\nh(\"Bruno\")\n\n\n[1] \"Bruno likes feijoada\"\n[[1]]\n[1] \"feijoada\"\n\nAs you can see, every time this function runs, it now outputs the same result. Users can change the seed to have this function output, consistently, another result.\n\n\n6.1.3 Referentially transparent and pure functions\nA referentially transparent function is a function that does not use any variable that is not also one of its inputs. For example, the following function:\n\nbad <- function(x){\n  x + y\n}\n\nis not referentially transparent, because y is not one of the function’s inputs. What happens if you run bad() is that bad() needs to look for y. Because y is not one of its inputs, bad() then looks for it in the global environment. If y is defined there, it then gets used. Defining and using such functions must be avoided at all costs because these functions are unpredictable. For example:\n\ny <- 10\n\nbad <- function(x){\n  x + y\n}\n\nbad(5)\n\nThis will return 15. But if y <- 45 then bad(5) would this time around return 50. It is much safer, and clearer to make y an explicit input of the function instead of having to keep track of y’s value (and it’s so easy to do, why just not do it):\n\ngood <- function(x, y){\n  x + y\n}\n\ngood() is a referentially transparent function; it is much safer than bad(). good() is also a pure function, because it’s a function that does not interact in any way with the global environment. It does not write anything to the global environment, nor requires anything from the global environment. Function h() from the previous section was not pure, because it created an object and wrote it to the global environment (the food_list object). Turns out that pure functions are thus necessarily referentially transparent.\nSo the first lesson in your functional programming journey that you have to remember is to only use pure functions."
  },
  {
    "objectID": "fprog.html#writing-good-functions",
    "href": "fprog.html#writing-good-functions",
    "title": "6  Functional programming",
    "section": "6.2 Writing good functions",
    "text": "6.2 Writing good functions\n\n6.2.1 Functions are first-class objects\nIn a functional programming language, functions are first-class objects. Contrary to what the name implies, this means that functions, especially the ones you define yourself, are nothing special. A function is an object like any other, and can thus be manipulated as such. Think of anything that you can do with any object in R, and you can do the same thing with a function. For example, let’s consider the +() function. It takes two numeric objects and returns their sum:\n\n1 + 5.3\n\n[1] 6.3\n\n# or alternatively: `+`(1, 5.3)\n\nYou can replace the numbers with functions that return numbers:\n\nsqrt(1) + log(5.3)\n\n[1] 2.667707\n\n\nIt’s also possible to define a function that explicitly takes another function as an input:\n\nh <- function(number, f){\n  f(number)\n}\n\nYou can call then use h() as a wrapper for f():\n\nh(4, sqrt)\n\n[1] 2\n\nh(10, log10)\n\n[1] 1\n\n\nBecause h() takes another function as an argument, h() is called a higher-order function.\nIf you don’t know how many arguments f(), the function you’re wrapping, has, you can use the ...:\n\nh <- function(number, f, ...){\n  f(number, ...)\n}\n\n... are simply a placeholder for any potential additional argument that f() might have:\n\nh(c(1, 2, NA, 3), mean, na.rm = TRUE)\n\n[1] 2\n\nh(c(1, 2, NA, 3), mean, na.rm = FALSE)\n\n[1] NA\n\n\nna.rm is an argument of mean(). As the developer of h(), I don’t necessarily know what f() might be, or maybe I know f() and know all its arguments, but don’t want to have to rewrite them all to make them arguments of h(), so I can use ... instead. The following is also possible:\n\nw <- function(...){\npaste0(\"First argument: \", ..1, \", second argument: \", ..2, \", last argument: \", ..3)\n}\n\nw(1, 2, 3)\n\n[1] \"First argument: 1, second argument: 2, last argument: 3\"\n\n\nIf you want to learn more about ..., type ?dots in an R console.\nBecause functions are nothing special, you can also write functions that return functions. As an illustration, we’ll be writing a function that converts warnings to errors. This can be quite useful if you want your functions to fail early, which often makes debugging easier. For example, try running this:\n\nsqrt(-5)\n\nWarning in sqrt(-5): NaNs produced\n\n\n[1] NaN\n\n\nThis only raises a warning and returns NaN (Not a Number). This can be quite dangerous, especially when working non-interactively, which is what we will be doing a lot later on. It is much better if a pipeline fails early due to an error, than dragging a NaN value. This also happens with log():\n\nsqrt(-10)\n\nWarning in sqrt(-10): NaNs produced\n\n\n[1] NaN\n\n\nSo it could be useful to redefine these functions to raise an error instead, for example like this:\n\nstrict_sqrt <- function(x){\n\n  if(x <= 0) stop(\"x is negative\")\n\n  sqrt(x)\n\n}\n\nThis function now throws an error for negative x:\n\nstrict_sqrt(-10)\n\nError in strict_sqrt(-10) : x is negative\nHowever, it can be quite tedious to redefine every function that we need in our pipeline, and remember, we don’t want to repeat ourselves. The other thing you need to remember is that functions are nothing special. Which means that we can define a function that takes a function as an argument, converts any warning thrown by that function into an error, and returns a new function. For example:\n\nstrictly <- function(f){\n  function(...){\n    tryCatch({\n      f(...)\n    },\n    warning = function(warning)stop(\"Can't do that chief\"))\n  }\n}\n\nThis function makes use of tryCatch() which catches warnings raised by an expression (in this example the expression is f(...)) and then raises an error instead with the stop() function. It is now possible to define new functions like this:\n\ns_sqrt <- strictly(sqrt)\n\n\ns_sqrt(-4)\n\nError in value[[3L]](cond) : Can't do that chief\n\ns_log <- strictly(log)\n\n\ns_log(-4)\n\nError in value[[3L]](cond) : Can't do that chief\nFunctions that return functions are called function factories and they’re incredibly useful. I use this so much that I’ve written a package, available on CRAN, called {chronicler}, that does this:\n\ns_sqrt <- chronicler::record(sqrt)\n\n\nresult <- s_sqrt(-4)\n\nresult\n\nNOK! Value computed unsuccessfully:\n---------------\nNothing\n\n---------------\nThis is an object of type `chronicle`.\nRetrieve the value of this object with pick(.c, \"value\").\nTo read the log of this object, call read_log(.c).\n\n\nBecause the expression above resulted in an error, Nothing is returned. Nothing is a special value defined in the {maybe} package (check it out, very interesting package!). We can then even read the log to see what went wrong:\n\nchronicler::read_log(result)\n\n[1] \"Complete log:\"                                                                                \n[2] \"NOK! sqrt() ran unsuccessfully with following exception: NaNs produced at 2023-02-28 21:28:13\"\n[3] \"Total running time: 0.00122332572937012 secs\"                                                 \n\n\nThe {purrr} package also comes with function factories that you might find useful ({possibly}, {safely} and {quietly}).\nIn part 2 we will also learn about assertive programming, another way of making our functions safer, as an alternative to using function factories.\n\n\n6.2.2 Optional arguments\nIt is possible to make functions’ arguments optional, by using NULL. For example:\n\ng <- function(x, y = NULL){\n  if(is.null(y)){\n    print(\"optional argument y is NULL\")\n    x\n  } else {\n    if(y == 5) print(\"y is present\"); x+y\n  }\n}\n\nCalling g(10) prints the message “Optional argument y is NULL”, and returns 10. Calling g(10, 5) however, prints “y is present” and returns 15. It is also possible to use missing():\n\ng <- function(x, y){\n  if(missing(y)){\n    print(\"optional argument y is missing\")\n    x\n  } else {\n    if(y == 5) print(\"y is present\"); x+y\n  }\n}\n\nI however prefer the first approach, because it is clearer which arguments are optional, which is not the case with the second approach, where you need to read the body of the function.\n\n\n6.2.3 Safe functions\nIt is important that your functions are safe and predictable. You should avoid writing functions that behave like nchar(), a base R function. Let’s see why this function is not safe:\n\nnchar(\"10000000\")\n\n[1] 8\n\n\nIt returns the expected result of 8. But what if I remove the quotes?\n\nnchar(10000000)\n\n[1] 5\n\n\nWhat is going on here? I’ll give you a hint: simply type 10000000 in the console:\n\n10000000\n\n[1] 1e+07\n\n\n10000000 gets represented as 1e+07 by R. This number in scientific notation gets then converted into the character “1e+07” by nchar(), and this conversion happens silently. nchar() then counts the number of characters, and correctly returns 5. The problem is that it doesn’t make sense to provide a number to a function that expects a character. This function should have returned an error message, or at the very least raised a warning that the number got converted into a character. Here is how you could rewrite nchar() to make it safer:\n\nnchar2 <- function(x, result = 0){\n\n  if(!isTRUE(is.character(x))){\n    stop(paste0(\"x should be of type 'character', but is of type '\",\n                typeof(x), \"' instead.\"))\n  } else if(x == \"\"){\n    result\n  } else {\n    result <- result + 1\n    split_x <- strsplit(x, split = \"\")[[1]]\n    nchar2(paste0(split_x[-1],\n                     collapse = \"\"), result)\n  }\n}\n\nThis function now returns an error message if the input is not a character:\n\nnchar2(10000000)\n\nError in nchar2(10000000) : x should be of type 'character', but is of type 'integer' instead. \nThis section is in a sense an introduction to assertive programming. As mentioned in the section on function factories, we will be learning about assertive programming in greater detail in part 2 of the book.\n\n\n6.2.4 Recursive functions\nYou may have noticed in the last lines of nchar2() defined above, that nchar2() calls itself. A function that calls itself in its own body is called a recursive function. It is sometimes easier to define a function in its recursive form than in an iterative form. The most common example is the factorial function. However, there is an issue with recursive functions (in the R programming language, other programming languages may not have the same problem, like Haskell): while it is sometimes easier to write a function using a recursive algorithm than an iterative algorithm, like for the factorial function, recursive functions in R are quite slow. Let’s take a look at two definitions of the factorial function, one recursive, the other iterative:\n\nfact_iter <- function(n){\n  result = 1\n  for(i in 1:n){\n    result = result * i\n    i = i + 1\n  }\n  result\n}\n\nfact_recur <- function(n){\n  if(n == 0 || n == 1){\n  result = 1\n  } else {\n    n * fact_recur(n-1)\n  }\n}\n\nUsing the {microbenchmark} package we can benchmark the code:\n\nmicrobenchmark::microbenchmark(\n  fact_recur(50), \n  fact_iter(50)\n)\n\nUnit: microseconds\n           expr    min     lq     mean median      uq    max neval\n fact_recur(50) 21.501 21.701 23.82701 21.901 22.0515 68.902   100\n  fact_iter(50)  2.000  2.101  2.74599  2.201  2.3510 21.000   100\nWe see that the recursive factorial function is 10 times slower than the iterative version. In this particular example it doesn’t make much of a difference, because the functions only take microseconds to run. But if you’re working with more complex functions, this is a problem. If you want to keep using the recursive function and not switch to an iterative algorithm, there are ways to make them faster. The first is called trampolining. I won’t go into details, but if you’re interested, there is an R package that allows you to use trampolining with R, aptly called {trampoline}. Another solution is using the {memoise} package.\n\n\n6.2.5 Anonymous functions\nIt is possible to define a function and not give it a name. For example:\n\nfunction(x)(x+1)(10)\n\nSince R version 4.1, there is even a shorthand notation for anonymous functions:\n\n(\\(x)(x+1))(10)\n\nBecause we don’t name them, we cannot reuse them. So why is this useful? Anonymous functions are useful when you need to apply a function somewhere inside a pipe once, and don’t want to define a function just for this. This will become clearer once we learn about lists, but before that, let’s philosophize a bit.\n\n\n6.2.6 The Unix philosophy applied to R\n\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.\n\nDoug McIlroy, in A Quarter Century of Unix1\nWe can take inspiration from the Unix philosophy and rewrite it like this for our purposes:\nWrite functions that do one thing and do it well. Write functions that work together. Write functions that handle lists, because that is a universal interface.\nStrive for writing simple functions that only perform one task. Don’t hesitate to split a big function into smaller ones. Small functions that only perform one task are easier to maintain, test, document and debug. These smaller functions can then be chained using the |> operator. In other words, it is preferable to have something like:\na |> f() |> g() |> h() \nwhere a is for example a path to a data set, and where f(), g() and h() successively read, clean, and plot the data, than having something like:\nbig_function(a)\nthat does all the steps above in one go.\nThis idea of splitting the problem into smaller chunks, each chunk in turn split into even smaller units that can be handled by functions and then the results of these function combined into a final output is called composition.\nThe advantage of splitting big_function() into f(), g() and h() is that you can eat the elephant one bite at a time, and also reuse these smaller functions in other projects more easily. So what’s important is that you can make small functions work together by sharing a common interface. The list is usually a good candidate for this."
  },
  {
    "objectID": "fprog.html#lists-a-powerful-data-structure",
    "href": "fprog.html#lists-a-powerful-data-structure",
    "title": "6  Functional programming",
    "section": "6.3 Lists: a powerful data-structure",
    "text": "6.3 Lists: a powerful data-structure\nLists are the second important ingredient of functional programming. In the R philosophy inspired by the UNIX philosophy, I stated that lists are a universal interface in R, so our functions should handle lists. This of course depends on what it is you’re doing. If you need functions to handle numbers, then there’s little value in placing these numbers inside lists. But in practice, you will very likely manipulate objects that are more complex than numbers, and this is where lists come into play.\n\n6.3.1 Lists all the way down\nLists are extremely flexible, and most of the very complex objects classes that you manipulate are actually lists, but just fancier. For example, a data frame is a list:\n\ndata(mtcars)\n\ntypeof(mtcars)\n\n[1] \"list\"\n\n\nA fitted model is a list:\n\nmy_model <- lm(hp ~ mpg, data = mtcars)\n\ntypeof(my_model)\n\n[1] \"list\"\n\n\nA ggplot is a list:\n\nlibrary(ggplot2)\n\nmy_plot <- ggplot(data = mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\ntypeof(my_plot)\n\n[1] \"list\"\n\n\nIt’s lists all the way down, and it’s not a coincidence. It’s because, as stated, lists are very powerful. So it’s important to know what you can do with lists.\n\n\n6.3.2 Lists can hold many things\nIf you write a function that needs to return many objects, the only solution is to place them inside a list. For example, consider this function:\n\nsqrt_newton <- function(a, init = 1, eps = 0.01, steps = 1){\n    stopifnot(a >= 0)\n    while(abs(init**2 - a) > eps){\n        init <- 1/2 *(init + a/init)\n        steps <- steps + 1\n    }\n    list(\n      \"result\" = init,\n      \"steps\" = steps\n    )\n}\n\nThis function returns the square root of a number using Newton’s algorithm, as well as the number of steps, or iterations, it took to reach the solution:\n\nresult_list <- sqrt_newton(1600)\n\nresult_list\n\n$result\n[1] 40\n\n$steps\n[1] 10\n\n\nIt is quite common to instead print the number of steps to the console instead of returning them. But the issue with a function that prints something to the console instead of returning it, is that such a function is not pure, as it changes something outside of its scope. And if you need the information that got printed (for example, if you want to count all the steps it took to run the script from start to finish), it is lost. It gets printed, and that’s it. It is preferable to instead make the function pure by returning everything inside a neat list. It is then possible to separately save these objects if needed:\n\nresult <- result_list$result\n\nresult_steps <- result_list$steps\n\nOr you could define functions that know how to deal with the list:\n\nf <- function(result_list){\n  list(\n    \"result\" = result_list$result * 10,\n    \"steps\" = result_list$steps + 1\n    )\n}\n\nf(result_list)\n\n$result\n[1] 400\n\n$steps\n[1] 11\n\n\nIt all depends on what you want to do. But it is usually better to keep everything neatly inside a list.\nLists can also hold objects of different types:\n\nlist(\n  \"a\" = head(mtcars),\n  \"b\" = ~lm(y ~ x)\n  )\n\n$a\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n$b\n~lm(y ~ x)\n\n\nThe list above has two elements, the first is the head of the mtcars data frame, the second is a formula object. Lists can even hold other lists:\n\nlist(\n  \"a\" = head(mtcars),\n  \"b\" = list(\n    \"c\" = sqrt,\n    \"d\" = my_plot # Remember this ggplot object from before?\n    )\n  )\n\n$a\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n$b\n$b$c\nfunction (x)  .Primitive(\"sqrt\")\n\n$b$d\n\n\n\n\n\nUse this to your advantage.\n\n\n6.3.3 Lists as the cure to loops\nLoops are incredibly useful, and you are likely familiar with them. The problem with loops is that they are a concept from iterative programming, not functional programming, and this is a problem because loops rely on changing the state of your program to run. For example, let’s suppose that you wish to use a for-loop to compute the sum of the first 100 integers:\n\nresult <- 0\nfor (i in 1:100){\n  result <- result + i\n}\n\nprint(result)\n\n[1] 5050\n\n\nIf you run ls() now, you should see that there’s a variable i in your global environment. This could cause issues further down in your pipeline if you need to re-use i. Also, writing loops is, in my opinion, quite error prone. But how can we avoid using loops? For looping in a functional programming language, we need to use higher-order functions and lists. A reminder: a higher-order function is a function that takes another function as an argument. Looping is a task like any other, so we can write a function that does the looping for us. We will write a function, and call it looping(), which will take a function as an argument, as well as a list. The list will serve as the container to hold our numbers:\n\nlooping <- function(a_list, a_func, init = NULL, ...){\n\n  # If the user does not provide an `init` value,\n  # set the head of the list as the initial value\n  if(is.null(init)){\n    init <- a_list[[1]]\n    a_list <- tail(a_list, -1)\n  }\n\n  # Separate the head from the tail of the list\n  # and apply the function to the initial value and the head of the list\n  head_list = a_list[[1]]\n  tail_list = tail(a_list, -1)\n  init = a_func(init, head_list, ...)\n\n  # Check if we're done: if there is still some tail,\n  # rerun the whole thing until there's no tail left\n  if(length(tail_list) != 0){\n    looping(tail_list, a_func, init, ...)\n  }\n  else {\n    init\n  }\n}\n\nNow, this might seem much more complicated than a for loop. However, now that we have abstracted the loop away inside a function, we can keep reusing this function:\n\nlooping(as.list(seq(1:100)), `+`)\n\n[1] 5050\n\n\nOf course, because this is so useful, looping() actually ships with R, and is called Reduce():\n\nReduce(`+`, seq(1:100)) # the order of the arguments is `function` then `list` for `Reduce()`\n\n[1] 5050\n\n\nBut this is not the only way that we can loop. We can also write a loop that applies a function to each element of a list, instead of operating on the whole list:\n\nresult <- as.list(seq(1:5))\nfor (i in seq_along(result)){\n  result[[i]] <- sqrt(result[[i]])\n}\n\nprint(result)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 2.236068\n\n\nHere again, we have to pollute the global environment by first creating a vessel for our results, and then apply the function at each index. We can abstract this process away in a function:\n\napplying <- function(a_list, a_func, ...){\n\n  head_list = a_list[[1]]\n  tail_list = tail(a_list, -1)\n  result = a_func(head_list, ...)\n\n  # Check if we're done: if there is still some tail, rerun the whole thing until there's no tail left\n  if(length(tail_list) != 0){\n    append(result, applying(tail_list, a_func, ...))\n  }\n  else {\n    result\n  }\n}\n\nOnce again this might seem complicated, and I would agree. Abstraction is complex. But once we have it, we can focus on the task at hand, instead of having to always tell the computer what we want:\n\napplying(as.list(seq(1:5)), sqrt)\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nOf course, R ships with its own, much more efficient, implementation of this function:\n\nlapply(list(seq(1:5)), sqrt)\n\n[[1]]\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nIn other programming languages, lapply() is often called map(). The {purrr} package ships with other such useful higher-order functions that abstract loops away. For example, there’s the function called map2(), that maps a function of two arguments to each element of two atomic vectors or lists, two at a time:\n\nlibrary(purrr)\n\nmap2(\n  .x = seq(1:5),\n  .y = seq(1:5),\n  .f = `+`\n  )\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 10\n\n\nIf you have more than two lists, you can use pmap() instead.\n\n\n6.3.4 Data frames\nAs mentioned in the introduction of this section, data frames are a special type of list of atomic vectors. This means that just as I can use lapply() to compute the square root of the elements of an atomic vector, as in the previous example, I can also operate on all the columns of a data frame. For example, it is possible to determine the class of every column of a data frame like this:\n\nlapply(iris, class)\n\n$Sepal.Length\n[1] \"numeric\"\n\n$Sepal.Width\n[1] \"numeric\"\n\n$Petal.Length\n[1] \"numeric\"\n\n$Petal.Width\n[1] \"numeric\"\n\n$Species\n[1] \"factor\"\n\n\nUnlike a list however, the elements of a data frame must be of the same length. Data frames remain very flexible though, and using what we have learned until now it is possible to use the data frame as a structure for all our computations. For example, suppose that we have a data frame that contains data on unemployment for the different subnational divisions of the Grand-Duchy of Luxembourg, the country the author of this book hails from. Let’s suppose that I want to generate several plots, per subnational division and per year. Typically, we would use a loop for this, but we can use what we’ve learned here, as well as some functions from the {dplyr}, {purrr}, {ggplot2} and {tidyr} packages. I will be downloading data that I made available inside a package, but instead of installing the package, we will download the .rda file directly (which is the file format of packaged data) and then load that data into our R session:\n\n# Create a temporary file\nunemp_path <- tempfile(fileext = \".rda\")\n\n# Download the data and save it to the path of the temporary file\ndownload.file(\"https://github.com/b-rodrigues/myPackage/raw/main/data/unemp.rda\",\n              destfile = unemp_path)\n\n# Load the data. The data is now available as 'unemp'\nload(unemp_path)\n\nLet’s load the required packages and take a look at the data:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\nglimpse(unemp)\n\nRows: 472\nColumns: 9\n$ year                         <dbl> 2013, 2013, 2013, 2013, 2013, 2013, 2013,…\n$ place_name                   <chr> \"Luxembourg\", \"Capellen\", \"Dippach\", \"Gar…\n$ level                        <chr> \"Country\", \"Canton\", \"Commune\", \"Commune\"…\n$ total_employed_population    <dbl> 223407, 17802, 1703, 844, 1431, 4094, 214…\n$ of_which_wage_earners        <dbl> 203535, 15993, 1535, 750, 1315, 3800, 187…\n$ of_which_non_wage_earners    <dbl> 19872, 1809, 168, 94, 116, 294, 272, 113,…\n$ unemployed                   <dbl> 19287, 1071, 114, 25, 74, 261, 98, 45, 66…\n$ active_population            <dbl> 242694, 18873, 1817, 869, 1505, 4355, 224…\n$ unemployment_rate_in_percent <dbl> 7.947044, 5.674773, 6.274078, 2.876870, 4…\n\n\nColumn names are self-descriptive, but the level column needs some explanations. level contains the adiministrative divisions of the country, so the country of Luxembourg, then the Cantons and then the Communes.\nRemember that Luxembourg can refer to the country, the canton or the commune of Luxembourg. Now let’s suppose that I want a separate plot for the three communes of Luxembourg, Esch-sur-Alzette and Wiltz. Instead of creating three separate data frames and feeding them to the same ggplot code, I can instead take advantage of the fact that data frames are lists, and are thus quite flexible. Let’s start with filtering:\n\nfiltered_unemp <- unemp %>%\n  filter(\n    level == \"Commune\",\n    place_name %in% c(\"Luxembourg\", \"Esch-sur-Alzette\", \"Wiltz\")\n   )\n\nglimpse(filtered_unemp)\n\nRows: 12\nColumns: 9\n$ year                         <dbl> 2013, 2013, 2013, 2014, 2014, 2014, 2015,…\n$ place_name                   <chr> \"Esch-sur-Alzette\", \"Luxembourg\", \"Wiltz\"…\n$ level                        <chr> \"Commune\", \"Commune\", \"Commune\", \"Commune…\n$ total_employed_population    <dbl> 12725, 39513, 2344, 13155, 40768, 2377, 1…\n$ of_which_wage_earners        <dbl> 12031, 35531, 2149, 12452, 36661, 2192, 1…\n$ of_which_non_wage_earners    <dbl> 694, 3982, 195, 703, 4107, 185, 710, 4140…\n$ unemployed                   <dbl> 2054, 3855, 318, 1997, 3836, 315, 2031, 3…\n$ active_population            <dbl> 14779, 43368, 2662, 15152, 44604, 2692, 1…\n$ unemployment_rate_in_percent <dbl> 13.898099, 8.889043, 11.945905, 13.179778…\n\n\nWe are now going to use the fact that data frames are lists, and that lists can hold any type of object. For example, remember this list from before where one of the elements is a data frame, and the second one a formula:\n\nlist(\n  \"a\" = head(mtcars),\n  \"b\" = ~lm(y ~ x)\n  )\n\n$a\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n$b\n~lm(y ~ x)\n\n\n{dplyr} comes with a function called group_nest() which groups the data frame by a variable (such that the next computations will be performed group-wise) and then nests the other columns into a smaller data frame. Let’s try it and see what happens:\n\nnested_unemp <- filtered_unemp %>%\n  group_nest(place_name) \n\nLet’s see what this looks like:\n\nnested_unemp\n\n# A tibble: 3 × 2\n  place_name                     data\n  <chr>            <list<tibble[,8]>>\n1 Esch-sur-Alzette            [4 × 8]\n2 Luxembourg                  [4 × 8]\n3 Wiltz                       [4 × 8]\n\n\nnested_unemp is a new data frame of 3 rows, one per commune (“Esch-sur-Alzette”, “Luxembourg”, “Wiltz”), and of two columns, one for the names of the communes, and the other contains every other variable inside a smaller data frame. So this is a data frame that has one column where each element of that column is itself a data frame. Such a column is called a list-column. This is essentially a list of lists.\nLet’s now think about this for a moment. If the column titled data is a list of data frames, it should be possible to use a function like map() or lapply() to apply a function on each of these data frames. Remember that map() or lapply() require a list of elements of whatever type and a function that accepts objects of this type as input. So this means that we could apply a function that plots the data to each element of the column titled data. Since each element of this column is a data frame, this functions needs a data frame as an input. As a first, simple, example to illustrate this, let’s suppose that we want to determine the number of rows of each data frame. This is how we would do it:\n\nnested_unemp %>%\n  mutate(nrows = map(data, nrow))\n\n# A tibble: 3 × 3\n  place_name                     data nrows    \n  <chr>            <list<tibble[,8]>> <list>   \n1 Esch-sur-Alzette            [4 × 8] <int [1]>\n2 Luxembourg                  [4 × 8] <int [1]>\n3 Wiltz                       [4 × 8] <int [1]>\n\n\nThe new column, titled nrows is a list of integers. We can simplify it by converting it directly to an atomic vector of integers by using map_int() instead of map():\n\nnested_unemp %>%\n  mutate(nrows = map_int(data, nrow))\n\n# A tibble: 3 × 3\n  place_name                     data nrows\n  <chr>            <list<tibble[,8]>> <int>\n1 Esch-sur-Alzette            [4 × 8]     4\n2 Luxembourg                  [4 × 8]     4\n3 Wiltz                       [4 × 8]     4\n\n\nLet’s try for a more complex example now. What if we want to filter rows? (The simplest way would of course to filter the rows we need before nesting the data frame). We need to apply the function filter() where its first argument is a data frame and the second argument is a predicate:\n\nnested_unemp %>%\n  mutate(nrows = map(data, \\(x)filter(x, year == 2015)))\n\n# A tibble: 3 × 3\n  place_name                     data nrows           \n  <chr>            <list<tibble[,8]>> <list>          \n1 Esch-sur-Alzette            [4 × 8] <tibble [1 × 8]>\n2 Luxembourg                  [4 × 8] <tibble [1 × 8]>\n3 Wiltz                       [4 × 8] <tibble [1 × 8]>\n\n\nIn this case, we need to use an anonymous function. This is because filter() has two arguments and we need to make clear what it is we are mapping over and what argument stays fixed; we are mapping over, or iterating if you will, data frames, but the predicate year == 2015 stays fixed.\nWe are now ready to plot our data. The best way to continue is to first get the function right by creating one plot for one single commune. Let’s select the dataset for the commune of Luxembourg:\n\nlux_data <- nested_unemp %>%\n  filter(place_name == \"Luxembourg\") %>%\n  unnest(data)\n\nTo plot this data, we can now write the required ggplot2() code:\n\nggplot(data = lux_data) +\n  theme_minimal() +\n  geom_line(\n    aes(year, unemployment_rate_in_percent, group = 1)\n   ) +\n  labs(title = \"Unemployment in Luxembourg\")\n\n\n\n\nTo turn the lines of code above into a function, you need to think about how many arguments that function would have. There is an obvious one, the data itself (in the snippet above, the data is the lux_data object). Another one that is less obvious is in the title:\n\n  labs(title = \"Unemployment in Luxembourg\")\n\n$title\n[1] \"Unemployment in Luxembourg\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nIdeally, we would want that title to change depending on the data set. So we could write the function like so:\n\nmake_plot <- function(x, y){\n  ggplot(data = x) +\n    theme_minimal() +\n    geom_line(\n      aes(year, unemployment_rate_in_percent, group = 1)\n      ) +\n    labs(title = paste(\"Unemployment in\", y))\n}\n\nLet’s try it on our data:\n\nmake_plot(lux_data, \"Luxembourg\")\n\n\n\n\nOk, so now, we simply need to apply this function to our nested data frame:\n\nnested_unemp <- nested_unemp %>%\n  mutate(plots = map2(\n    .x = data,\n    .y = place_name,\n    .f = make_plot\n  ))\n\nnested_unemp\n\n# A tibble: 3 × 3\n  place_name                     data plots \n  <chr>            <list<tibble[,8]>> <list>\n1 Esch-sur-Alzette            [4 × 8] <gg>  \n2 Luxembourg                  [4 × 8] <gg>  \n3 Wiltz                       [4 × 8] <gg>  \n\n\nIf you look at the plots column, you see that it is a list of gg objects: these are our plots. Let’s take a look at them:\n\nnested_unemp$plots\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\nWe could also have used an anonymous function:\n\nnested_unemp %>%\n  mutate(plots2 = map2(\n    .x = data,\n    .y = place_name,\n    .f = \\(.x,.y)(\n                ggplot(data = .x) +\n                  theme_minimal() +\n                  geom_line(\n                    aes(year, unemployment_rate_in_percent, group = 1)\n                   ) +\n                  labs(title = paste(\"Unemployment in\", .y))\n                  )\n           )\n         ) %>%\n  pull(plots2)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\nThis list-column based workflow is extremely powerful and I highly advise you to take the required time to master it. Remember, we never want to have to repeat ourselves. This might seem more complicated that repeating yourself, but imagine that you need to do this for various countries, various variables, etc… What are you going to do, copy and paste code everywhere? This gets very tedious and more importantly, very error-prone, because you will forget to update the code in some places. You could of course use a loop instead of this list-column based workflow. But as mentioned, the issue with loops is that you have to interact with the global environment, which can lead to other issues. But whatever you end up using, you need to avoid copy and pasting at all costs."
  },
  {
    "objectID": "fprog.html#functional-programming-in-r",
    "href": "fprog.html#functional-programming-in-r",
    "title": "6  Functional programming",
    "section": "6.4 Functional programming in R",
    "text": "6.4 Functional programming in R\nUp until now I focused on general concepts than on specifics of the R programming language when it comes to functional programming. In the section, we will be focusing entirely on R-specific capabilities and packages for functional programming.\n\n6.4.1 Base capabilities\nR is a functional programming language, (but not only), and as such it comes with many functions out of the box to write functional code. We have already discussed lapply() and Reduce(). You should know that depending on what you want to achieve, there are other functions that are similar to lapply(): apply(), sapply(), vapply(), mapply() and tapply(). There’s also Map() which is a wrapper around mapply(). Each function performs the same basic task of applying a function over all the elements of a list or list-like structure, but it can be hard to keep them apart and when you should use one over another. This is why {purrr}, which we will discuss in the next section, is quite an interesting alternative to base R’s offering.\nAnother one of the quintessential functional programming functions (alongside Reduce() and Map()) that ships with R is Filter(). If you know dplyr::filter() you should be familiar with the concept of filtering rows of a data frame where the elements of one particular column satisfy a predicate. Filter() works the same way, but focusing on lists instead of data frame:\n\nFilter(is.character,\n       list(\n         seq(1:5),\n         \"Hey\")\n       )\n\n[[1]]\n[1] \"Hey\"\n\n\nThe call above only returns the elements where is.character() evaluates to TRUE.\nAnother useful function is Negate() which is a function factory that takes a boolean function as an input and returns the opposite boolean function. As an illustration, suppose that in the example above we wanted to get everything but the character:\n\nFilter(Negate(is.character),\n       list(\n         seq(1:5),\n         \"Hey\")\n       )\n\n[[1]]\n[1] 1 2 3 4 5\n\n\nThere are some other functions like this that you might want to check out: type ?Negate in console to read more about them.\nBefore continuing with R packages that extend R’s functional programming capabilities it’s also important to stress that just as R is a functional programming language, it is also an object oriented language. In fact, R is what John Chambers called a functional OOP language (Chambers (2014)). We won’t delve too much into what this means (read Wickham (2019) for this), but as a short discussion, consider the print() function. Depending on what type of object the user gives it, it seems as if somehow print() knows what to do with it:\n\nprint(5)\n\n[1] 5\n\nprint(head(mtcars))\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nprint(str(mtcars))\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\nNULL\n\n\nThis works by essentially mixing both functional and object-oriented programming, hence functional OOP. Let’s take a closer look at the source code of print() by simply typing print without brackets, into a console:\n\nprint\n\nfunction (x, ...) \nUseMethod(\"print\")\n<bytecode: 0x564cd666a1b8>\n<environment: namespace:base>\n\n\nQuite unexpectedly, the source code of print() is one line long and is just UseMethod(\"print\"). So all print() does is use a generic method called “print”. If your text editor has autocompletion enabled, you might see that there are actually quite a lot of print() functions. For example, type print.data.frame into a console:\n\nprint.data.frame\n\nfunction (x, ..., digits = NULL, quote = FALSE, right = TRUE, \n    row.names = TRUE, max = NULL) \n{\n    n <- length(row.names(x))\n    if (length(x) == 0L) {\n        cat(sprintf(ngettext(n, \"data frame with 0 columns and %d row\", \n            \"data frame with 0 columns and %d rows\"), n), \"\\n\", \n            sep = \"\")\n    }\n    else if (n == 0L) {\n        print.default(names(x), quote = FALSE)\n        cat(gettext(\"<0 rows> (or 0-length row.names)\\n\"))\n    }\n    else {\n        if (is.null(max)) \n            max <- getOption(\"max.print\", 99999L)\n        if (!is.finite(max)) \n            stop(\"invalid 'max' / getOption(\\\"max.print\\\"): \", \n                max)\n        omit <- (n0 <- max%/%length(x)) < n\n        m <- as.matrix(format.data.frame(if (omit) \n            x[seq_len(n0), , drop = FALSE]\n        else x, digits = digits, na.encode = FALSE))\n        if (!isTRUE(row.names)) \n            dimnames(m)[[1L]] <- if (isFALSE(row.names)) \n                rep.int(\"\", if (omit) \n                  n0\n                else n)\n            else row.names\n        print(m, ..., quote = quote, right = right, max = max)\n        if (omit) \n            cat(\" [ reached 'max' / getOption(\\\"max.print\\\") -- omitted\", \n                n - n0, \"rows ]\\n\")\n    }\n    invisible(x)\n}\n<bytecode: 0x564cd7788810>\n<environment: namespace:base>\n\n\nThis is the print function for data.frame objects. So what print() does is look at the class of its argument x, and then look for the right print function. In more traditional OOP languages, users would type something like:\n\nmtcars.print()\n\nIn these languages, objects encapsulate methods (the equivalent of our functions), so if mtcars is a data frame, it encapsulates a print() method that then does the printing. R is different, because classes and methods are kept separate. If a package developer creates a new object class, then the developer also must implement the required methods. For example in the {chronicler} package, the chronicler class is defined alongside the print.chronicler() function to print these objects.\nAll of this to say that if you want to extend R by writing packages, learning some OOP essentials is also important. But for data analysis, functional programming does the job perfectly. To learn more about R’s different OOP systems (yes, R can do OOP in different ways and the one I sketched here is the simplest, but probably the most used as well), take a look at Wickham (2019).\n\n\n6.4.2 purrr\nThe {purrr} package, developed by Posit (formerly RStudio), contains many functions to make functional programming with R go more smoothly. In the previous section, we discussed the apply() family of function; they all do a very similar thing, which is looping over a list and applying a function to the elements of the list, but it is not quite easy to remember which one does what. Also, for some of these functions like apply(), the list comes first, and then the function, but in the case of mapply(), the function comes first. This type of inconsistencies can be frustrating. Another issue with these functions is that it is not always easy to know what type the output is going to be. List? Atomic vector? Something else?\n{purrr} solves this issue by offering the map() family of functions, which behave in a very consistent way. The basic function is called map() and we’ve already used it:\n\nmap(seq(1:5), sqrt)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 2.236068\n\n\nBut there are many interesting variants:\n\nmap_dbl(seq(1:5), sqrt)\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nmap_dbl() coerces the output to an atomic vector of doubles instead of a list of doubles. Then there’s:\n\nmap_chr(letters, toupper)\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\nfor when the output needs to be an atomic vector of characters.\nThere are many others, so take a look at the document with ?map. There’s also walk() which is used if you’re only interested in the side-effect of the function (for example if the function takes paths as input and saves something to disk).\n{purrr} also has functions to replace Reduce(), simply called reduce() and accumulate(), and there are many, many other useful functions. Read through the documentation of the package and take the time to learn about all it has to offer.\n\n\n6.4.3 withr\n{withr} is a powerful package that makes it easy to “purify” functions that behave in a way that can cause problems. Remember the function from the introduction that randomly gave out a recipe Bruno liked? Here it is again:\n\nh <- function(name, food_list = list()){\n\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  food_list <- append(food_list, food)\n\n  print(paste0(name, \" likes \", food))\n\n  food_list\n}\n\nBecause this function returns results that are not consistent for a fixed input, this function is not referentially transparent. So we improved the function by adding calls to set.seed() like this:\n\nh2 <- function(name, food_list = list(), seed = 123){\n\n  # We set the seed, making sure that we get the same selection of food for a given seed\n  set.seed(seed)\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  # We now need to unset the seed, because if we don't, guess what, the seed will stay set for the whole session!\n  set.seed(NULL)\n\n  food_list <- append(food_list, food)\n\n  print(paste0(name, \" likes \", food))\n\n  food_list\n}\n\nThe problem with this approach is that we need to modify our function. We can instead use withr::with_seed() to achieve the same effect:\n\nwithr::with_seed(seed = 123,\n                 h(\"Bruno\"))\n\n[1] \"Bruno likes feijoada\"\n\n\n[[1]]\n[1] \"feijoada\"\n\n\nIt is also easier to create a wrapper if needed:\n\nh3 <- function(..., seed){\n  withr::with_seed(seed = seed,\n                   h(...))\n}\n\n\nh3(\"Bruno\", seed = 123)\n\n[1] \"Bruno likes feijoada\"\n\n\n[[1]]\n[1] \"feijoada\"\n\n\nIn a previous example we downloaded a dataset and loaded it into memory; we did so by first created a temporary file, then downloading it and then loading it. Suppose that instead of loading this data into our session, we simply wanted to test whether the link was still working. We wouldn’t want to keep the loaded data in our session, so to avoid having to delete it again manually, we could use with_tempfile():\n\nwithr::with_tempfile(\"unemp\", {\n  download.file(\"https://github.com/b-rodrigues/myPackage/raw/main/data/unemp.rda\",\n                destfile = unemp)\n  load(unemp)\n  nrow(unemp)\n  }\n)\n\n[1] 472\n\n\nThe data got downloaded, and then loaded, and then we computed the number of rows of the data, without touching the global environment, or state, of our current session.\nJust like for {purrr}, {withr} has many useful functions which I encourage you to familiarize yourself with."
  },
  {
    "objectID": "fprog.html#conclusion",
    "href": "fprog.html#conclusion",
    "title": "6  Functional programming",
    "section": "6.5 Conclusion",
    "text": "6.5 Conclusion\nIf there is only one thing that you should remember from this chapter, it would be pure functions. Writing pure functions is in my opinion not very difficult to do and comes with many benefits. But, avoiding loops and replacing them with higher-order functions (lapply(), Reduce(), purrr::map() – and its variants –) also pays off. While this chapter stresses the advantages of functional programming, you should not forget that R is not a pure, and solely, functional programming language and that other paradigms, like object-oriented programming, are also available to you. So if your goal is to master the language (instead of “just” using it to solve data analysis problems), then you also need to know about R’s OOP capabilities.\n\n\n\n\nChambers, John M. 2014. “Object-Oriented Programming, Functional Programming and R.” Statistical Science 29 (2): 167–80.\n\n\nWickham, Hadley. 2019. Advanced r. CRC press."
  },
  {
    "objectID": "lit_prog.html#a-quick-history-of-literate-programming",
    "href": "lit_prog.html#a-quick-history-of-literate-programming",
    "title": "7  Literate programming",
    "section": "7.1 A quick history of literate programming",
    "text": "7.1 A quick history of literate programming\nIn literate programming, we mix code and prose together, which makes the output of our programs not just a series of tables, or graphs or predictions, but a complete report that contains the results of our analysis directly. Scripts written using literate programming are also very easy to compile, or render, into a variety of document formats like html, docx, pdf or even pptx. R supports several frameworks for literate programming: Sweave, knitr and Quarto.\nSweave was the first tool available to R (and S) users, and allowed the mixing of R and LaTeX code to create a document. Friedrich Leisch developed Sweave in 2002 and described it in his 2002 paper (Leisch (2002)). As Leisch argues, the traditional way of writing a report as part of a statistical data analysis project uses two separate steps: running the analysis using some software, and then copy and pasting the results into a word processing tool (as illustrated above). To really drive that point home: the problem with this approach is that much time is wasted copy and pasting things, so experimenting with different layouts or data analysis techniques is very time consuming. Copy and paste mistakes will also happen (it’s not a question of if, but when) and updating reports (for example, when new data comes in) means that someone will have, again, to copy and paste the updated results into a new document.\nSweave provided (and still provides, as it is still well functioning!) a way to embed the analysis in the document itself, in this case a LaTeX source file, and R code was executed whenever the document was compiled. This gave researchers considerable time savings when it was time to update a report or drafting a research paper.\nThe snippet below shows the example from Leisch’s paper:\n\\documentclass[a4paper]{article}\n\n\\begin{document}\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a LaTeX document:\n\n<<>>=\ndata (airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n@\n\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n\\begin{center}\n<<fig=TRUE,echo=FALSE>>=\nboxplot(Ozone ~ Month, data = airquality)\n@\n\\end{center}\n\n\\end{document}\nEven if you’ve never seen a LaTeX source file, you should be able to figure out what’s going on. The first line states what type of document we’re writing. Then comes \\begin{document} which tells the compiler where the document starts. Then comes the content. You can see that it’s a mixture of plain English with R code defined inside chunks starting with <<>>= and ending with @. Finally, the documents ends with \\end{document}. Getting a human readable PDF from this source is a two-step process: first this source gets converted into a .tex file and then this .tex file into a PDF. Sweave is included with every R installation since version 1.5.0, and still works to this day. For example, we can test that our Sweave installation works just fine by compiling the example above. This is what the final output looks like:\n\n\n\nMore than 20 years later, the output is still the same.\n\n\nLet us just state that the fact that it is still possible to compile this example more than 20 years later is an incredible testament to how mature and stable this software is (both R, Sweave, and LaTeX). But as impressive as this is, LaTeX has a steep learning curve, and Leisch even advocated the use of the Emacs text editor to edit Sweave files, which also has a very steep learning curve (but this is entirely optional; for example we edited and compiled the example on the Rstudio IDE).\nThe next generation of literate programming tools was provided by a package called {knitr} in 2012. From the perspective of the user, the biggest change from Sweave is that {knitr} is able to use many different formats as source files. The one that became very likely the most widely used format is a flavour of the Markdown markup language, R Markdown (Rmd). But this is not the only difference with Sweave:{knitr} can also run code chunks for other languages, such as Python, Perl, Awk, Haskell, bash and more (Xie (2014)). Since version 1.18, {knitr} uses the {reticulate} package to provide a Python engine for the Rmd format. To illustrate the Rmd format, let’s rewrite the example from Leisch’s Sweave paper into it:\n---\noutput: pdf_document\n---\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a LaTeX document:\n\n```{r}\ndata (airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n```\n\n\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n```{r, echo = FALSE}\nboxplot(Ozone ~ Month, data = airquality)\n```\nThis is what the output looks like:\n\n\n\nIt’s very close to the Sweave output.\n\n\nJust like in a Sweave document, an Rmd source file also has a header in which authors can define a number of options. Here we only specified that we wanted a pdf document as an output file. We then copy and pasted the contents from the Sweave source, but changed the chunk delimiters from <<>>= and @ to ```{r} to start an R chunk and ``` to end it. Remember; we need to specify the engine in the chunk because {knitr} supports many engines. For example, it is possible to run a bash command by adding this chunk to the source:\n---\noutput: pdf_document\n---\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a LaTeX document:\n\n```{r}\ndata (airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n```\n\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n\n```{r, echo = FALSE}\nboxplot(Ozone ~ Month, data = airquality)\n```\n\n```{bash}\npwd\n```\n\n(bash’s pwd command shows the current working directory). You may have noticed that we also keep two LaTeX commands in the source Rmd, \\texttt{} and LaTeX. This is because Rmd files get first converted into LaTeX files and then into a PDF. If you’re using RStudio, this document can be compiled by clicking a button or using a keyboard shortcut, but you can also use the rmarkdown::render() function. This function does two things transparently: it first converts the Rmd file into a source LaTeX file, and then converts it into a PDF. It is of course possible to convert the document to a Word document as well, but in this case, LaTeX commands will be ignored. Html is another widely used output format.\nIf you’re a researcher and prefer working with LaTeX directly instead of having to switch to Markdown, you can either use Sweave, or use {knitr} but instead of writing your documents using the R Markdown format, you can use the Rnw format which is basically the same as Sweave, but uses {knitr} for compilation. Take a look at this example from the {knitr} github repository for example.\nYou should know that {knitr} makes it possible to author many, many different types of documents. It is possible to write books, blogs, package documentation (and even entire packages, as we shall see later in this book), Powerpoint slides… It is extremely powerful because we can use the same general R Markdown knowledge to build many different outputs:\n\n\n\nOne format to rule them all.\n\n\nFinally, the latest in literate programming for R is a new tool developed by Posit, called Quarto. If you’re an R user and already know {knitr} and the Rmd format, you should be able to immediately use Quarto. So what’s the difference? In practice and for R users not much but there are some things that Quarto is able to do out of the box for which you’d need extensions with {knitr}. Quarto has some nice defaults; in fact this book is written in Quarto’s Markdown flavour and compiled with Quarto instead of {knitr} because the default Quarto output looks nicer than the default {knitr} output. However, there may even be things that Quarto can’t do at all (at least for now) when compared to {knitr}. So why bother switching? Well, Quarto provides sane defaults and some nice features out of the box, and the cost of switching from the Rmd format to Quarto’s Qmd format is basically 0. Also, and this is probably the biggest reason to use Quarto, Quarto is not tied to R. Quarto is actually a standalone tool that needs to be installed alongside your R installation, and works completely independently. In fact, you can use Quarto without having R installed at all, as Quarto, just like {knitr} supports many engines. This means that if you’re primarily using Python, you can author documents with Quarto. Quarto also supports the Julia programming language and Observable JS, making it possible to include interactive visualisations into an Html document. Let’s take a look at how the example from Leisch’s paper looks as a Qmd file:\n---\noutput: pdf\n---\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a LaTeX document:\n\n```{r}\ndata (airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n```\n\n\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n```{r, echo = FALSE}\nboxplot(Ozone ~ Month, data = airquality)\n```\n(I’ve omitted the bash chunk from before, not because Quarto does not support it, but to keep close to the original example from the paper.)\nAs you can see, it’s exactly the same as the Rmd file from before. The only difference is in the header. In the Rmd file we specified the output format as:\n\n---\noutput: pdf_document\n---\n\nwhereas in the Qmd file we changed it to:\n\n---\noutput: pdf\n---\n\nWhile Quarto is the latest option in literate programming, it is quite recent, and as such, I feel it might be better to stick with {knitr} and the Rmd format for now, so that’s what we’re going to use going forward. Also, the {knitr} and the Rmd format are here to stay, so there’s little risk in keeping using it, and anyways, as already stated, if switching to Quarto becomes a necessity, the cost of switch would be very, very low. In what follows, I won’t be focused on anything really {knitr} or Rmd specific, so should you want to use Quarto instead, you should be able to follow along without any problems at all, since the Rmd and Qmd formats have so much overlap.\nIn the next two sections, we will discuss how to set up and use {knitr} as well as give you a quick overview of the R Markdown syntax. However, we will very quickly focus on the templating capabilities of {knitr}: expanding text, using child documents, and parameterised reports. These are advanced topics and not easy to tackle if you’re not comfortable with R already. Just as functions and higher-order functions like lapply() avoid having to repeat yourself, so does templating, but for literate programming. The goal is to write functions that return literal R Markdown code, so that you can loop over these functions to build entire sections of your documents. However, the learning curve for these features is quite steep, but by now, you should have noticed that this book expects a lot from you. Keep going, and you shall be handsomely rewarded."
  },
  {
    "objectID": "lit_prog.html#knitr-basics",
    "href": "lit_prog.html#knitr-basics",
    "title": "7  Literate programming",
    "section": "7.2 {knitr} basics",
    "text": "7.2 {knitr} basics\nThis section will be a very small intro to {knitr}. We are going to teach you just enough to get started, and we are going to focus on the Rmd format. There are many resources out there that you can use if you want to dig deeper, for instance the R Markdown website from Posit, or the R Markdown: The Definitive Guide and R Markdown Cookbook eBooks. We will also not assume that you are using the RStudio IDE and give you instead the lower level commands to render documents. If you use RStudio and want to know how to use it effectively to author Rmd documents, you should take a look at Quick Tour page. In fact, this section will basically focus on the same topics, but without focusing on how to use RStudio.\n\n7.2.1 Set up\nThe first step is to install the {knitr} and the {rmarkdown} packages. That’s easy, just type:\n\ninstall.packages(\"rmarkdown\")\n\nin an R console. Since {knitr} is required to install {rmarkdown}, it gets installed automatically. If you want to compile PDF documents, you should also have a working LaTeX distribution. You can skip this next part if you’re only interested in generating PDF and Word files. For what follows, we will only be rendering Html documents, so no need to install LaTeX (by the way, you don’t even need a working Word installation to compile documents to the docx format). However, if you already have a working LaTeX installation, you shouldn’t have to do anything else to generate PDF documents. If you don’t have a working LaTeX distribution, then Yihui Xie, the creator of {knitr} created an R package called {tinytex} that makes it extremely easy to install a LaTeX distribution. In fact, this is the way I recommend installing LaTeX even if you’re not an R user (it is possible to use the tinytex distribution without R; it’s just that the {tinytex} R package provides many functions that makes installing and maintaining it very easy). Simply run these commands in an R console to get started:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nand that’s it! If you need to install specific LaTeX packages, then refer to the Maintenance section on tinytex’s website. For example, to compile the example from Leisch’s article on Sweave discussed previously, we had to install the grfext LaTeX package (as explained by the error output in the console when we tried compiling). So, we simply needed to run the following command to get it:\n\ntlmgr_install(\"grfext\")\n\nAfter you’ve installed {knitr}, {rmarkdown} and, optionally, {tinytex}, simply try to compile the following document:\n---\noutput: html_document\n---\n\n# Document title\n\n## Section title\n\n### Subsection title\n\nThis is **bold** text. This is *text in italics*.\n\nMy favourite programming language for statistics is ~~SAS~~ R.\nsave this document into a file called rmd_intro.rmd using you’re favourite text editor. Then render it into an Html file by running the following command in the R console:\n\nrmarkdown::render(\"path/to/rmd_test.rmd\")\n\nThis should create a file called rmd_test.html; open it with your web browser and you should see the following:\n\n\n\nIt’s very close to the Sweave output.\n\n\nCongratulations, you just knitted your first Rmd document!\n\n\n7.2.2 Markdown ultrabasics\nR Markdown is a flavour of Markdown, which means that you should know some Markdown to really take full advantage of R Markdown. The example document from before should have already shown you some basics: titles, sections and subsections all start with a # and the depth level is determined by the number of #s. For bold text, simply put the words in between ** and for italics use only one *. If you want bold and italics, use ***. The original designer of Markdown did not think that underlining text was important, so there is no easy way of doing it unfortunately. For this, you need to use a somewhat hidden feature; without going into too much technical details, the program that converts Rmd files to the final output format is called Pandoc, and it’s possible to use some of Pandoc’s features to format text. For example, for underlining:\n[This is some underlined text in a R Markdown document]{.underline}\nThis will underline the text between square brackets.1\nThe next step is actually to mix code and prose. As you’ve seen from Leisch’s canonical example, this is quite easily achieved by using R code chunks. The R Markdown example below shows various code chunks alongside some options. For example, a code chunk that uses the echo = FALSE option will not appear (but the output of the computation will):\n---\ntitle: \"Document title\"\noutput: html_document\ndate: \"2023-01-28\"\n---\n\n# R code chunks\n\nThis below is an R code chunk:\n\n```{r}\ndata(mtcars)\n\nplot(mtcars)\n```\n\n\nThe code chunk above will appear in the final output. The code chunk below will be hidden:\n\n```{r, echo = FALSE}\ndata(iris)\n\nplot(iris)\n```\n\n\nThis next code chunk will not be evaluated:\n\n```{r, eval = FALSE}\ndata(titanic)\n\nstr(titanic)\n```\n\n\nThe last one runs, but code and output from the code is not shown in the final\ndocument. This is useful for loading libraries and hiding startup messages:\n\n```{r, include = FALSE}\nlibrary(dplyr)\n```\nIf you use RStudio and create a new R Markdown file from the menu, a new R Markdown file is generated for you to fill out. The first R chunk is this one:\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\nThis is an R chunk named setup and with the option include = FALSE. Naming chunks is optional, but we are going to make use of this later on. What this chunk runs is one line of code that defines a global option to show all chunks by default (which is the default behaviour). You can change TRUE to FALSE if you want to hide every code chunk instead (if you’re using Quarto, global options are set differently).\nSomething else you might have noticed in the previous example, is that we’ve added some more content in the header:\n---\ntitle: \"Document title\"\noutput: html_document\ndate: \"2023-01-28\"\n---\nThere are several other options available that you can define in the header. Later on, when we’ll be building our project together, we will provide some more options (like having a table of contents).\nThe finish this part on code chunks, you should know about inline code chunks. Take a look at the following example:\n---\ntitle: \"Document title\"\noutput: html_document\ndate: \"2023-01-28\"\n---\n\n# R code chunks\n\n```{r, echo = FALSE}\ndata(iris)\n```\n\n\nThe iris dataset has `r nrow(iris)` rows.\nThe last sentence from this example has an inline code chunk. This quite useful, as it allows to parameterise sentences and paragraphs, and thus avoids needing to copy and paste (and we will go quite far into how to avoid copy and pasting, thanks to more advanced features we will shortly discuss).\nTo finish this crash course, you should know that to use footnotes you need to write the following:\nThis sentence has a footnote.[^1]\n\n[^1]: This is the footnote.\nand that you can write LaTeX formulas as well. For example, add the following into the the example from before and render either a PDF or a html document (don’t put the LaTeX formula below inside a chunk, simply paste it as if it were normal text. This doesn’t work for Word output because Word does not support LaTeX equations):\n\\begin{align*}\nS(\\omega) \n&= \\frac{\\alpha g^2}{\\omega^5} e^{[ -0.74\\bigl\\{\\frac{\\omega U_\\omega 19.5}{g}\\bigr\\}^{\\!-4}\\,]} \\\\\n&= \\frac{\\alpha g^2}{\\omega^5} \\exp\\Bigl[ -0.74\\Bigl\\{\\frac{\\omega U_\\omega 19.5}{g}\\Bigr\\}^{\\!-4}\\,\\Bigr] \n\\end{align*}\nThe LaTeX code above results in this equation:\n\n\n\nA rendered LaTeX equation."
  },
  {
    "objectID": "lit_prog.html#keeping-it-dry",
    "href": "lit_prog.html#keeping-it-dry",
    "title": "7  Literate programming",
    "section": "7.3 Keeping it DRY",
    "text": "7.3 Keeping it DRY\nRemember; we never, ever, want to have to repeat ourselves. Copy and pasting is forbidden. Striving for this 0 copy and pasting will make our code much more robust and likely to be correct.\nWe started by using functions, as discussed in the previous chapter, but we can much farther than that. For example, suppose that we need to write a document that has the following structure:\n\nA title\nA section\nA table inside this section\nAnother section\nAnother table inside this section\nYet another section\nYet another table inside this section\n\nIs there a way to automate the creation of such a document by taking advantage of the repeating structure? Of course there is. The question is not, is it possible to do X?, but how to do X?.\n\n7.3.1 Generating R Markdown code from code\nThe example below is a fully working minimal example of this. Copy it inside a document titled something like rmd_templating.Rmd and render it. You will see that the output contains more sections than defined in the source. This is because we use templating at the end. Take some time to read the document, as the text inside explains what is going on:\n---\ntitle: \"Templating\"\noutput: html_document\ndate: \"2023-01-27\"\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n\n## A function that creates tables\n\n```{r}\ncreate_table <- function(dataset, var){\n  table(dataset[var]) |>\n    knitr::kable()\n}\n```\n\n\nThe function above uses the `table()` function to create frequency tables, \nand then this gets passed to the `knitr::kable()` function that produces a \ngood looking table for our rendered document:\n\n```{r}\ncreate_table(mtcars, \"am\")\n```\n\n\nLet’ suppose that we want to generate a document that would look like this:\n\n- first a section title, with the name of the variable of interest\n- then the table\n\nSo it would look like this:\n\n## Frequency table for variable: \"am\"\n\n```{r}\ncreate_table(mtcars, \"am\")\n```\n\n\nWe don’t want to create these sections for every variable by hand.\n\nInstead, we can define a function that returns the R markdown code required\nto create this. This is this function:\n\n```{r}\nreturn_section <- function(dataset, var){\n  a <- knitr::knit_expand(text = c(\"## Frequency table for variable: {{variable}}\",   \n                                   create_table(dataset, var)),\n                          variable = var)\n  cat(a, sep = \"\\n\")\n}\n```\n\n\nThis new function, `return_section()` uses `knitr::knit_expand()` to generate R\nMarkdown code. Words between `{{}}` get replaced by the provided `var` argument\nto the function. So when we call `return_section(\"am\")`, `{{variable}}` is\nreplaced by `\"am\"`. `\"am\"` then gets passed down to `create_table()` and the\nfrequency table gets generated. We can now generate all the section by simply\napplying our function to a list of column names:\n\n```{r, results = \"asis\"}\ninvisible(lapply(colnames(mtcars), return_section, dataset = mtcars))\n```\nThe last function, named return_section() uses knitr::knit_expdand(), which is the function that does the heavy lifting. This function returns literal R Markdown code. It returns ## Frequency table for variable: {{variable}} which creates a level 2 section title with the text Frequency table for variable: xxx where the xxx will get replaced by the variable passed to return_section(). So calling return_section(mtcars, \"am\") will print the following in your console:\n## Frequency table for variable: am\n|am | Freq|\n|:--|----:|\n|0  |   19|\n|1  |   13|\nWe now simply need to find a clever way to apply this function to each variable in the mtcars dataset. For this, we are going to use lapply() which implements a for loop (you could use purrr::map() just as well for this):\n\ninvisible(lapply(colnames(mtcars),\n                 return_section,\n                 dataset = mtcars))\n\nThis will create, for each variable in mtcars, the same R Markdown code as above. Notice that the R Markdown chunk where the call to lapply() is has the option results = \"asis\". This is because the function returns literal Markdown code, and we don’t want the parser to have to parse it again. We tell the parser “don’t worry about this bit of code, it’s already good”. As you see, the call to lapply() is wrapped inside invisible(). This is because return_section() does not return anything, it just prints something to the console. No object is returned. return_section() is a function with only a side-effect: it changes something outside its scope. So if you don’t wrap the call to lapply() inside invisible(), then a bunch of NULLs will also get printed (NULLs get returned by functions that don’t return anything). To avoid this, use invisible() (and use purrr::walk() rather than purrr::map() if you want to use tidyverse packages and functions).\nClick here to see the output.\nThis is not an easy topic, so take the time to play around with the example above. Try to print another table, try to generate more complex Markdown code, remove the call to invisible() and knit the document and see what happens with the output, replace the call to lapply() with purrr::walk() or purrr::map(). Really take the time to understand what is going on.\nWhile extremely powerful, this approach using knitr::knit_expand() only works if your template only contains text. If you need to print something more complicated in the document, you need to use child documents instead. For example, suppose that instead of a table we wanted to show a plot made using {ggplot2}. This would not work, because a ggplot object is not made of text, but is a list with many elements. The print() method for ggplot objects then does some magic and prints a plot. But if you want to show plots using knitr::knit_expand(), then the contents of the list will be shown, not the plot itself. This is where child documents come in. Child documents are exactly what you think they are: they’re smaller documents that get knitted and then embedded into the parent document. You can define anything within these child documents, and as such you can even use them to print more complex objects, like a ggplot object. Let’s go back to the example from before and make use of a child document (for ease of presentation, we will not use a separate Rmd file, but will inline the child document into the main document). Read the Rmd example below carefully, as all the steps are explained:\n---\ntitle: \"Templating with child documents\"\noutput: html_document\ndate: \"2023-01-27\"\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(ggplot2)\n```\n\n## A function that creates ggplots\n\n```{r}\ncreate_plot <- function(dataset, aesthetic){\n\n  ggplot(dataset) +\n    geom_point(aesthetic)\n\n}\n```\n\nThe function above takes a dataset and an aesthetic made using `ggplot2::aes()` to\ncreate a plot:\n\n```{r}\ncreate_plot(mtcars, aes(y = mpg, x = hp))\n```\n\nLet’s suppose that we want to generate a document that would look like this:\n\n- first a section title, with the dataset used;\n- then a plot\n\nSo it would look like this:\n\n## Dataset used: \"mtcars\"\n\n```{r}\ncreate_plot(mtcars, aes(y = mpg, x = hp))\n```\n\nWe don’t want to create these sections for every aesthetic by hand.\n\nInstead, we can make use of a child document that gets knitted separately\nand then embedded in the parent document. The chunk below makes use of this trick:\n\n```{r, results = \"asis\"}\n\nx <- list(aes(y = mpg, x = hp),\n          aes(y = mpg, x = hp, size = am))\n\nres <- lapply(x,\n              function(dataset, x){\n\n  knitr::knit_child(text = c(\n\n    '\\n',\n    '## Dataset used: `r deparse(substitute(dataset))`',\n    '\\n',\n    '```{r, echo = F}',\n    'print(create_plot(dataset, x))',\n    '```'\n\n     ),\n     envir = environment(),\n     quiet = TRUE)\n\n}, dataset = mtcars)\n\n\ncat(unlist(res), sep = \"\\n\")\n```\n\nThe child document is the `text` argument to the `knit_child()` function. `text`\nis literal R Markdown code: we define a level 2 header, and then an R chunk.\nThis child document gets knitted, so we need to specify the environment in which\nit should get knitted. This means that the child document will get knitted in\nthe same environment as the parent document (our current global environment).\nThis way, every package that gets loaded and every function or variable that got\ndefined in the parent document will also be available to the child document.\n\nTo get the dataset name as a string, we use the `deparse(substitute(dataset))`\ntrick; this substitutes \"dataset\" by its bound value, so `mtcars`. But `mtcars`\nis an expression and we don’t want it to get evaluated, or the contents of the\nentire dataset would be used in the title of the section. So we use `deparse()`\nwhich turns unevaluated expressions into strings.\n\nWe then use `lapply()` to loop over two aesthetics with an anonymous function\nthat encapsulates the child document. So we get two child documents that get\nknitted, one per aesthetic. This gets saved into variable `res`. This is thus a\nlist of knitted Markdown.\n\nFinally, we need unlist `res` to actually merge the Markdown code from the child\ndocuments into the parent document.\nClick here to take a look at the output.\nHere again, take some time to play with the above example. Change the child document, try to print other types of output, really take your time to understand this. To know more about child documents, take a look at this section of the R Markdown Cookbook (Xie, Dervieux, and Riederer (2020)).\n\n\n7.3.2 Tables in R Markdown documents\nGetting tables right in Rmd documents is not always an easy task. There are several packages specifically made just for this task.\nIn this short section, we want to point you towards two packages that check the following boxes:\n\nWork the same way regardless of output format we want to knit our document into:\nWork for any type of table: summary tables, regression tables, two-way tables, etc.\n\nLet’s start with the simplest type of table, which would be showing the head of a dataset for example. {knitr} comes with the kable() function, but this function generates a very plain looking output. For something publication-worthy, we recommend the {flextable} package, developed by Gohel and Skintzos (2023):\n\nlibrary(flextable)\n\nmy_table <- head(mtcars)\n\nflextable(my_table) |>\n  set_caption(caption = \"Head of the mtcars dataset\") |>\n  theme_booktabs()\n\n\n\n\nThe output of the code above.\n\n\nWe won’t go into much detail on how {flextable} works, but it is very powerful, and the fact that it works for PDF, Html, Word and Powerpoint outputs is really a massive plus. If you want to learn more about {flextable}, there’s a whole, free, ebook on it. {flextable} can create very complicated tables, so really take the time to dig in!\nThe next package is {modelsummary}, by Arel-Bundock (2022), and this one focuses on regression and summary tables. It is extremely powerful as well, and just like {flextable}, works for any type of output. It is very simple to get started:\n\nlibrary(modelsummary)\n\nmodel_1 <- lm(mpg ~ hp + am, data = mtcars)\nmodel_2 <- lm(mpg ~ hp, data = mtcars)\n\nmodels <- list(\"Model 1\" = model_1,\n               \"Model 2\" = model_2)\n\nmodelsummary(models)\n\n\n\n\nThe output of the code above.\n\n\nHere again, we won’t got into much detail, but recommend instead that you read the package’s website which has very detailed documentation.\nThese packages can help you keeping it DRY, so take some time to learn them.\nAnd one last thing: if you’re a researcher, take a look at the {rticles} package, which provides Rmd templates to write articles for many scientific journals.\n\n\n7.3.3 Parametrized reports\nTemplating and child documents are very powerful, but sometimes you don’t want to have one section dedicated to each unit of analysis within the same report, but rather, you want a complete separate report by unit of analysis. This is also possible thanks to parameterised reports.\nLet’s modify the example from before, which consisted in creating one section per column of the mtcars dataset and a frequency table, and make it now one separate report for each column. The R Markdown file will look like this:\n---\ntitle: \"Report for column `r params$var` of dataset `r params$dataset`\"\noutput: html_document\ndate: \"2023-01-27\"\nparams:\n  dataset: mtcars\n  var: \"am\"\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n\n## Frequency table for `r params$var`\n\n```{r, echo = F}\ncreate_table <- function(dataset, var){\n\n  dataset <- get(dataset)\n\n  table(dataset[var]) |>\n    knitr::kable()\n}\n```\n\n\nThe table below is for variable `r params$var` of dataset `r params$dataset`.\n\n```{r}\ncreate_table(params$dataset, params$var)\n```\n\n```{r, eval = FALSE, echo = FALSE}\n# Run these lines to compile the document\n# Set eval and echo to FALSE, so that this does not appear\n# in the output, and does not get evaluated when knitting\nrmarkdown::render(\n             input = \"param_report_example.Rmd\",\n             params = list(\n               dataset = \"mtcars\",\n               var = \"cyl\"\n             )\n           )\n\n```\nSave the code above into an Rmd file titled something like param_report_example.Rmd (preferably inside its own folder). At the end of the document, we wrote the lines to render this document inside a chunk that does not get shown to the reader, nor gets evaluated:\n```{r, eval = F, echo = FALSE}\nrmarkdown::render(\n             input = \"param_report_example.Rmd\",\n             params = list(\n               dataset = \"mtcars\",\n               var = \"cyl\"\n             )\n           )\n```\nYou need to run these lines yourself to knit the document.\nThis will pass the list params with elements “mtcars” and “cyl” down to the report. Every params$dataset and params$var in the report gets replaced by “mtcars” and “cyl” respectively. Also, notice that in the header of the document, we defined default values for the params. Something else you need to be aware of, is that the function create_table() inside the report is slightly different than before. It now starts with the following line:\n\ndataset <- get(dataset)\n\nLet’s break this down. params$dataset contains the string “mtcars”. I made the decision to pass the dataset as a string, so that I could use it in the title of the document. But then, inside the create_table() function, I have the following code:\n\ndataset[var]\n\ndataset can’t be a string here, but needs to be a variable name, so mtcars and not “mtcars”. This means that I need to convert that string into a name. get() searches an object by name, and then makes it possible to save it to a new variable called dataset. The rest of the function is then the same as before. This little difficulty can be avoided by hard-coding the dataset inside the R Markdown file, or by passing the dataset as the params$dataset and not the string, in the render function. However, if you pass down the name of the dataset as a variable instead of the dataset name as a string, then you need to covert it to a string if you want to use it in the text (so mtcars to “mtcars”, using deparse(substitute(dataset)) as in child documents example).\nIf you instead want to create one report per variable, you could compile all the documents at once with:\n```{r, eval = F, echo = F}\ncolumns <- colnames(mtcars)\n\nlapply(columns,\n  (\\(x)rmarkdown::render(\n                    input = \"param_report_example.Rmd\",\n                    output_file = paste0(\"param_report_example_\", x, \".html\"),\n                    params = list(\n                      dataset = \"mtcars\",\n                      var = x\n                    )\n                  )\n  )\n)\n```\nBy now, this should not intimidate you anymore; we use lapply() to loop over a list of column names (that we get using colnames()). Because we don’t want to overwrite the report we need to change the name of the output file. We do so by using paste0() which creates a new string that contains the variable name, so each report gets its own name. x inside the paste0() function is each element, one after the other, of the columns variable we defined first. Think of it as the i in a for loop. We then must also pass this to the params list, hence the var = x. The complete call to rmarkdown::render() is wrapped inside an anonymous function, because we need to use the argument x (which is each column defined in the columns list) in different places."
  },
  {
    "objectID": "lit_prog.html#conclusion",
    "href": "lit_prog.html#conclusion",
    "title": "7  Literate programming",
    "section": "7.4 Conclusion",
    "text": "7.4 Conclusion\nBefore continuing, I highly recommend that you try running this yourself, and also that you try to build your own little parameterised reports. Maybe start by replacing “mtcars” by “iris” in the code to compile the reports and see what happens, and then when you’re comfortable with parameterised reports, try templating inside a parameterised report!\nIt is important to not fall to the temptation of copy and pasting sections of your report, or parts of your script, instead of using these more advanced features provided by the language. It is tempting, especially under time pressure, to just copy and paste bits of code and get things done instead of writing what seems to be unnecessary code to finally achieve the same thing. The problem however, is that in practice copy and pasting code to simply get things done will come bite you sooner rather than later. Especially when you’re still in the exploration/drafting phase of the project. It make take more time to set up, but once you’re done, it is much easier to experiment with different parameters, test the code or even re-use the code for other projects. Not only that, but forcing you to actually think about how to set up your code in a way that avoids repeating yourself also helps with truly understanding the problem at hand. What part of the problem is constant and does not change? What does change? How often, and why? Can you also fix these parts or not? What if instead of five sections that I need to copy and paste, I had 50 sections? How could I scale that up?\nAsking yourself these questions, and solving them, will ultimately make you better programmer.\nRemember: don’t repeat yourself!\n\n\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23.\n\n\nGohel, David, and Panagiotis Skintzos. 2023. Flextable: Functions for Tabular Reporting.\n\n\nLeisch, Friedrich. 2002. “Sweave: Dynamic Generation of Statistical Reports Using Literate Data Analysis.” In Compstat, edited by Wolfgang Härdle and Bernd Rönz, 575–80. Physica-Verlag HD.\n\n\nXie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible Research in R.” In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Chapman; Hall/CRC."
  },
  {
    "objectID": "part1_conclusion.html",
    "href": "part1_conclusion.html",
    "title": "8  Conclusion of part 1",
    "section": "",
    "text": "We’re at the end of part 1, and I need to congratulate you for making it this far. If you took the time to digest what we’ve learned up until now, you should be ready for what’s coming, which should be a bit easier, at least some of the parts.\nBut before continuing, let’s quickly summarise what we’ve learned so far.\nWe started our journey with two scripts that download and analyse data about housing in Luxembourg. We then learned about tools and programming paradigms that we will now use in part 2 to make our scripts more robust:\n\nVersion control;\nFunctional programming;\nLiterate programming.\n\nIn some ways, you might think that the we’ve made our life unnecessarily complicated for very little gain. For example, functional programming seems to be only about putting restrictions on how you code. Same with using trunk-based development; why make it so restrictive?\nWhat you need to understand is that these restrictions actually play a role. They force us to work in a much more structured way, which then ensures that our projects will be well-managed and ultimately reproducible. So while these techniques come with a cost, the benefits are far greater.\nWe will start part 2 by rewriting our scripts using what we’ve learned, and then, we will think about approaching the core problem differently, and structuring our project not as a series of scripts (or R Markdown files in the case of literate programming) but instead as a pipeline. Because until now, there’s no pipeline still.\nWe will also learn about tools that capture the computational environment that was used to set up this pipeline and how to use them effectively to make sure that our project is reproducible."
  },
  {
    "objectID": "part2_intro.html#the-reproducibility-iceberg",
    "href": "part2_intro.html#the-reproducibility-iceberg",
    "title": "Part 2: Reproducibility",
    "section": "The reproducibility iceberg",
    "text": "The reproducibility iceberg\nWe are done with the first part of the book, and I think it is time to reflect on why we bothered with it at all. Why not just go straight to the reproducibility part?\nRemember the introduction, where I talked about the reproducibility continuum or spectrum? It is now time to discuss this in greater detail. I propose a new analogy, the reproducibility iceberg:\n\n\n\nThe reproducibility iceberg.\n\n\nWhy an iceberg? Because the parts of the iceberg that you see, those that are obvious, are like running your analyses in a click-based environment like Excel. This is what’s obvious, what’s easy. No special knowledge or even training is required. All that’s required is time, so people using these tools are not efficient and thus compensate by working insane hours (I can’t go home and enjoy time with my family I have to stay at the office and update the spreadsheeeeeeeeet clicks furiously).\nLet’s go one level deeper: let’s write a script. This is where we started. Our script was not too bad, it did the job. Unlike a click-based workflow, we could at least re-read it, someone else could read it, and it would be possible to run in the future but likely with some effort unless we’re lucky. By that, I mean that for such a script to run successfully in the future, that script cannot rely on packages that got updated in such a way that the script cannot run anymore (for example, if functions get renamed, or if their arguments get renamed). Furthermore, if that script relies on a data source, the original authors also have to make sure that the same data source stays available. Another issue is collaborating when writing this script. Without any version control tools nor code hosting platform, collaborating on this script can very quickly turn into a nightmare.\nThis is where Git and Github came into play, one more level deeper. The advantage now is that collaboration was streamlined. The commit history is available to all the teammates and it is possible to revert changes, experiment with new features using branches and overall manage the project. In this layer we also employ new programming paradigms to make the code of the project less verbose, using functional programming, with the added benefits of making it easier to test, document and share (which we will discuss to its fullest in this part of the book). Using literate programming, it is also much easier to go to our final output (which is usually a report).\nAt this depth, we are at a pivotal moment: in many cases, analysts may want to stop here because there is no more time or budget left. After all, the results were obtained and shared with higher-ups. It can be difficult, in some contexts, to justify spending more time to go deeper and write tests, documentation and otherwise ensure total reproducibility. So at this stage, we will see what we can do that is very cheap (in both time and effort) to ensure the minimal amount of reproducibility, which is recording packages versions. Recording packages means that the exact same versions of the packages that were used originally will get used regardless of when in the future we rerun the analysis.\nBut if budget and time allows we can still go deeper, and definitely should. We also want to make running the script as easy as possible, and ideally, *as non-interactively as possible**. Any human interaction with the analysis is a source of errors, so that’s why we also need to thouroughly and systematically test our code. These tests also need to run non-interactively.\nThe other problem with freezing packages’s version is that in practice, it is very often not enough. This is because installing older versions of packages can be a challenge. This can be the case for two reasons:\n\nThese older packages need also an older version of R, and installing old versions of R can be tricky, depending on your operating system;\nThese older packages might need to get compiled and thus depend themselves on older versions of development libraries needed for compilation.\n\nSo to solve this issue, we will also need a way to freeze the computational environment itself, and this is where we will use Docker.\nFinally, and this is the last level of the iceberg and not part of this book, is the need to make the building of the computational environment reproducible as well. Guix is the tool that enables one to do just that. However, this is a very deep topic unto itself, and there are workarounds to achieve this using Docker, so that’s why we will not show be discussing Guix.\nWe will travel down the iceberg in the coming chapters. First, we will use what we’ve learned up until now to rewrite our project using functional and literate programming. Our project will not be two scripts anymore, but two Rmd files that we can knit and that we can then read and also send to non-technical stakeholders.\nThen, we are going to turn these two Rmds files into a package. This will be done by using Sébastien Rochette’s package {fusen}. {fusen} makes it very easy to go from our Rmd files to a package, by using what Sébastien named the Rmarkdown first method. If at this stage it’s not clear why you would want to turn your analysis into a package, don’t worry, it’ll be once we’re done with this chapter.\nOnce we have a package, we can then leverage {testthat} and {assertthat}, which are packages for unit and assertive testing respectively. At this stage, our code should be well-documented, easy to share, and thoroughly tested.\nOnce this is achieved, we can build a true pipeline using {targets}, an incredibly useful library for build automation.\nOnce we reached this stage, this is when we can finally start introducing reproducibility concretely. The reason it will take so long to actually make our pipeline reproducible is that we need solid foundations. There is no point in making a shaky analysis reproducible."
  },
  {
    "objectID": "project_rewrite.html#an-rmd-for-cleaning-the-data",
    "href": "project_rewrite.html#an-rmd-for-cleaning-the-data",
    "title": "9  Rewriting our project",
    "section": "9.1 An Rmd for cleaning the data",
    "text": "9.1 An Rmd for cleaning the data\nSo, let’s start with the save_data.R script. Since we are going to use functional programming and literate programming, we are going to start from an empty .Rmd file. So open an empty .Rmd file and start with the following lines:\n---\ntitle: \"Nominal house prices data in Luxembourg - Data cleaning\"\nauthor: \"Put your name in here\"\ndate: \"`r Sys.Date()`\"\n---\n\n```{r, warning=FALSE, message=FALSE}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(janitor)\nlibrary(purrr)\nlibrary(readxl)\nlibrary(rvest)\nlibrary(stringr)\n```\n\n\n## Downloading the data\nSo we start by writing a header to define the title of the document, the name of the author and the current date (yes, we can use inline R code to always get current date), or you can hardcode the date as a string if you prefer.\nWe then load packages in a chunk with options warning=FALSE and message=FALSE which will avoid showing packages’ startup messages in the knitted document.\nThen we start with a new section called ## Downloading the data. We then add a paragraph explaining from where and how we are going to download the data:\nThis data is downloaded from the luxembourguish [Open Data\nPortal](https://data.public.lu/fr/datasets/prix-annonces-des-logements-par-commune/)\n(the data set called *Série rétrospective des prix annoncés des maisons par\ncommune, de 2010 à 2021*), and the original data is from the \"Observatoire de\nl'habitat\". This data contains prices for houses sold since 2010 for each\nluxembourguish commune. \n\nThe function below uses the permanent URL from the Open Data Portal to access\nthe data, but I have also rehosted the data, and use my link to download the\ndata (for archival purposes):\nThis is much more detailed than using comments. Then comes a function to download and get the data. This function simply wraps the lines from our original script that did the downloading and the cleaning. As a reminder, here are the lines from the original script:\n\nurl <- \"https://github.com/b-rodrigues/rap4all/raw/master/datasets/vente-maison-2010-2021.xlsx\"\n\nraw_data <- tempfile(fileext = \".xlsx\")\n\ndownload.file(url, raw_data)\n\nsheets <- excel_sheets(raw_data)\n\nread_clean <- function(..., sheet){\n  read_excel(..., sheet = sheet) |>\n    mutate(year = sheet)\n\n  raw_data <- map(\n    sheets,\n    ~read_clean(raw_data,\n                skip = 10,\n                sheet = .)\n  ) |>\n    bind_rows() |>\n    clean_names()\n\n  raw_data <- raw_data |>\n    rename(\n      locality = commune,\n      n_offers = nombre_doffres,\n      average_price_nominal_euros = prix_moyen_annonce_en_courant,\n      average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,\n      average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant\n    ) |>\n    mutate(locality = str_trim(locality)) |>\n    select(year, locality, n_offers, starts_with(\"average\"))\n\nand here is the same code, but as a function:\n```{r, eval = F}\nget_raw_data <- function(url = \"https://data.public.lu/fr/datasets/r/14b0156e-ff87-4a36-a867-933fc9a6f903\"){\n\n  raw_data <- tempfile(fileext = \".xlsx\")\n\n  download.file(url,\n                raw_data,\n                mode = \"wb\") # for compatibility with Windows\n\n  sheets <- excel_sheets(raw_data)\n\n  read_clean <- function(..., sheet){\n    read_excel(..., sheet = sheet) %>%\n      mutate(year = sheet)\n  }\n\n  raw_data <- map_dfr(sheets,\n                      ~read_clean(raw_data,\n                                  skip = 10,\n                                  sheet = .)) %>%\n    clean_names()\n\n  raw_data %>%\n    rename(locality = commune,\n           n_offers = nombre_doffres,\n           average_price_nominal_euros = prix_moyen_annonce_en_courant,\n           average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,\n           average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant\n           ) %>%\n    mutate(locality = str_trim(locality)) %>%\n    select(year, locality, n_offers, starts_with(\"average\"))\n\n}\n```\nAs you see, it’s almost exactly the same code. So why use a function? Our function has the advantage that it uses the url of the data as an argument. Which means that we can use it on other datasets (let’s remember that we are here focusing on prices of houses, but there’s another dataset of prices of apartments) or the same, but updated dataset (let’s also remember that this is a dataset that gets updated yearly). We can now more easily re-use this function later on (especially once we’ve turned this Rmd into a package in the next chapter). You can decide to show the source code of the function or hide it with the chunk option include=FALSE or echo=FALSE (the difference between include and echo is that include hides both the source code chunk and the output of that chunk). The next part of the Rmd file is simply using the function we just wrote:\n```{r}\nraw_data <- get_raw_data(url = \"https://github.com/b-rodrigues/rap4all/raw/master/datasets/vente-maison-2010-2021.xlsx\")\n```\nWe can now continue by explaining what’s wrong with the data and what cleaning steps need to be taken:\nWe need clean the data: \"Luxembourg\" is \"Luxembourg-ville\" in 2010 and 2011,\nthen \"Luxembourg\". \"Pétange\" is also spelled non-consistently, and we also need\nto convert columns to right type. We also directly remove rows where the\nlocality contains information on the \"Source\":\n\n```{r}\nclean_raw_data <- function(raw_data){\n  raw_data %>%\n    mutate(locality = ifelse(grepl(\"Luxembourg-Ville\", locality),\n                             \"Luxembourg\",\n                             locality),\n           locality = ifelse(grepl(\"P.tange\", locality),\n                             \"Pétange\",\n                             locality)\n           ) %>%\n    filter(!grepl(\"Source\", locality)) %>%\n    mutate(across(starts_with(\"average\"), as.numeric))\n}\n```\n\n```{r}\nflat_data <- clean_raw_data(raw_data)\n```\nThe chunk above explains what we’re doing and why we’re doing it, and so we write a function (based on what we already wrote). Here again, the advantage of having this as a function will make it easier to run on updated data.\nWe now continue with establishing a list of communes:\nWe now need to make sure that we got all the communes/localities in there. There\nwere mergers in 2011, 2015 and 2018. So we need to account for these localities.\n\nWe’re now scraping data from wikipedia of former Luxembourguish communes:\n\n```{r}\nget_former_communes <- function(url = \"https://en.wikipedia.org/wiki/Communes_of_Luxembourg#Former_communes\",\n                                min_year = 2009,\n                                table_position = 3){\n  read_html(url) %>%\n    html_table() %>%\n    pluck(table_position) %>%\n    clean_names() %>%\n    filter(year_dissolved > min_year)\n}\n\n```\n\n```{r}\nformer_communes <- get_former_communes()\n```\n\nWe can scrape current communes:\n\n```{r}\nget_current_communes <- function(url = \"https://en.wikipedia.org/wiki/List_of_communes_of_Luxembourg\",\n                                 table_position = 1){\n  read_html(url) %>%\n    html_table() %>%\n    pluck(table_position) %>%\n    clean_names()\n}\n\n```\n\n```{r}\ncurrent_communes <- get_current_communes()\n```\nThis is quite a long chunk, but there is nothing new in here, so I won’t explain it line by line. What’s important is to notice that the code doing the actual work is all being wrapped inside functions. I reiterate: this will make reusing, testing and documenting much easier later on. Using the object former_communes and current_communes we can now build the complete list:\nLet’s now create a list of all communes:\n\n```{r}\nget_test_communes <- function(former_communes, current_communes){\n\n  communes <- unique(c(former_communes$name, current_communes$commune))\n  # we need to rename some communes\n\n  # Different spelling of these communes between wikipedia and the data\n\n  communes[which(communes == \"Clemency\")] <- \"Clémency\"\n  communes[which(communes == \"Redange\")] <- \"Redange-sur-Attert\"\n  communes[which(communes == \"Erpeldange-sur-Sûre\")] <- \"Erpeldange\"\n  communes[which(communes == \"Luxembourg-City\")] <- \"Luxembourg\"\n  communes[which(communes == \"Käerjeng\")] <- \"Kaerjeng\"\n  communes[which(communes == \"Petange\")] <- \"Pétange\"\n\n  communes\n}\n\n```\n\n```{r}\nformer_communes <- get_former_communes()\ncurrent_communes <- get_current_communes()\n\ncommunes <- get_test_communes(former_communes, current_communes)\n```\nOnce again, we write a function for this. We need to merge these two lists, and need to make sure that the spelling of the communes’ names is unified between this list and between the communes’ names in the data.\nWe now run the actual test:\nLet’s test to see if all the communes from our dataset are represented.\n\n```{r}\nsetdiff(flat_data$locality, communes)\n```\n\nIf the above code doesn’t show any communes, then this means that we are\naccounting for every commune.\nThis test is quite simple, and we will see how to create something a bit more robust and useful later on.\nNow, let’s extract the national average from the data and create a separate dataset with the national level data:\n\nLet’s keep the national average in another dataset:\n\n```{r}\nmake_country_level_data <- function(flat_data){\n  country_level <- flat_data %>%\n    filter(grepl(\"nationale\", locality)) %>%\n    select(-n_offers)\n\n  offers_country <- flat_data %>%\n    filter(grepl(\"Total d.offres\", locality)) %>%\n    select(year, n_offers)\n\n  full_join(country_level, offers_country) %>%\n    select(year, locality, n_offers, everything()) %>%\n    mutate(locality = \"Grand-Duchy of Luxembourg\")\n\n}\n\n```\n\n```{r}\ncountry_level_data <- make_country_level_data(flat_data)\n```\nand finally, let’s do the same but for the commune level data:\nWe can finish cleaning the commune data:\n\n```{r}\nmake_commune_level_data <- function(flat_data){\n  flat_data %>%\n    filter(!grepl(\"nationale|offres\", locality),\n           !is.na(locality))\n}\n\n```\n\n```{r}\ncommune_level_data <- make_commune_level_data(flat_data)\n```\nWe can finish with a chunk to save the data to disk:\nWe now save the dataset in a folder for further analysis (keep chunk option to\n`eval = F` to avoid running it when knitting):\n\n```{r, eval = F}\nwrite.csv(commune_level_data,\n          \"datasets/house_prices_commune_level_data.csv\",\n          row.names = FALSE)\nwrite.csv(country_level_data,\n          \"datasets/house_prices_country_level_data.csv\",\n          row.names = FALSE)\n```\nThis last chunk is something I like to add to my Rmd files, but instead of showing it in the final document but not evaluating its contents using the chunk option eval = F you could hide it completely as well, so it doesn’t appear in the compiled document. The first time you compile this document, you could change the option to eval = T, so that the data gets written to disk, and then change it to eval = F to avoid overwriting the data on subsequent knittings. This is up to you, and also who the audience of the knitted output is (do they want to see this chunk at all?).\nOk, and that’s it. You can take a look at the finalised file here3. You can now remove the save_data.R script, as you have successfully ported the code over to a Rmd.\nIf you have not done it yet, you can commit these changes and push.\nLet’s now do the same thing for the analysis script."
  },
  {
    "objectID": "project_rewrite.html#an-rmd-for-analysing-the-data",
    "href": "project_rewrite.html#an-rmd-for-analysing-the-data",
    "title": "9  Rewriting our project",
    "section": "9.2 An Rmd for analysing the data",
    "text": "9.2 An Rmd for analysing the data\nWe will follow the same steps as before to convert the analysis script into an analysis markdown. Instead of showing the whole file here, I will instead show you two important points.\nThe first point is removing redundancy. In the original script, we had the following lines:\n\n#Let’s compute the Laspeyeres index for each commune:\n\ncommune_level_data <- commune_level_data %>%\n  group_by(locality) %>%\n  mutate(p0 = ifelse(year == \"2010\", average_price_nominal_euros, NA)) %>%\n  fill(p0, .direction = \"down\") %>%\n  mutate(p0_m2 = ifelse(year == \"2010\", average_price_m2_nominal_euros, NA)) %>%\n  fill(p0_m2, .direction = \"down\") %>%\n  ungroup() %>%\n  mutate(pl = average_price_nominal_euros/p0*100,\n         pl_m2 = average_price_m2_nominal_euros/p0_m2*100)\n\n\n#Let’s also compute it for the whole country:\n\ncountry_level_data <- country_level_data %>%\n  mutate(p0 = ifelse(year == \"2010\", average_price_nominal_euros, NA)) %>%\n  fill(p0, .direction = \"down\") %>%\n  mutate(p0_m2 = ifelse(year == \"2010\", average_price_m2_nominal_euros, NA)) %>%\n  fill(p0_m2, .direction = \"down\") %>%\n  mutate(pl = average_price_nominal_euros/p0*100,\n         pl_m2 = average_price_m2_nominal_euros/p0_m2*100)\n\nAs you can see, this is almost exactly twice the same code. The only difference is that we need to group by commune when computing the Laspeyeres index for the communes (remember, this index will make it easy to make comparisons). Instead of repeating 99% of the lines, we can create a function that will group the data if the data is the commune level data, and not group the data if it’s the national data. Here is this function:\n\nget_laspeyeres <- function(dataset, start_year = \"2010\"){\n\n  which_dataset <- deparse(substitute(dataset))\n\n  group_var <- if(grepl(\"commune\", which_dataset)){\n                 quo(locality)\n               } else {\n                 NULL\n               }\n  dataset %>%\n    group_by(!!group_var) %>%\n    mutate(p0 = ifelse(year == start_year,\n                       average_price_nominal_euros,\n                       NA)) %>%\n    fill(p0, .direction = \"down\") %>%\n    mutate(p0_m2 = ifelse(year == start_year,\n                          average_price_m2_nominal_euros,\n                          NA)) %>%\n    fill(p0_m2, .direction = \"down\") %>%\n    ungroup() %>%\n    mutate(pl = average_price_nominal_euros/p0*100,\n           pl_m2 = average_price_m2_nominal_euros/p0_m2*100)\n\n}\n\nSo, the first step is naming the function. We’ll call it get_laspeyeres(), and it’ll be a function of two arguments. The first is the data (commune or national level data) and the second is the starting date of the data. This second argument is has a default value of “2010”. This is the year the data starts, and thus the year the Laspeyeres index will have a value of 100.\nThe following lines are probably the most complicated:\n\nwhich_dataset <- deparse(substitute(dataset))\n\ngroup_var <- if(grepl(\"commune\", which_dataset)){\n               quo(locality)\n             } else {\n               NULL\n             }\n\nThe first line replaces the variable dataset by its bound value (that’s what substitute() does) for example, commune_level_data, and then converts this variable name into a string (using deparse()). So when the user provides commune_level_data, which_dataset will defined as equal to \"commune_level_data\". We then use this string to detect whether the data needs to be grouped or not. So if we detect the word “commune” in the which_dataset variable, we set the grouping variable to locality, if not to NULL. But you might have a question: why is locality given as an input to quo(), and what is quo().\nA simple explanation: locality is a variable in the commune_level_dataset. If we don’t quote it using quo(), our function will look for a variable called locality in body of the function, but since there is no variable defined that is called locality in there, the function will look for this variable in the global environment. But this is not a variable defined in the global environment, it is a column in our dataset. So we need a way to tell this to the column: don’t worry about evaluating this just yet, I’ll tell you when it’s time.\nSo by using quo(), we can delay evaluation. So how can we tell the function that it’s time to evaluate locality? This is where we need !!. If you take a look at the line where we group the data in the function:\n\ngroup_by(!!group_var)\n\nSo if we are calling the function on commune_level_dataset, then group_var is equal to locality, if not it’s NULL. !!group_var means that now it’s time to evaluate group_var (or rather, locality). Because !!group_var gets replaced by quo(locality), and because group_by() is a {dplyr} function that knows how to deal with quoted variables, locality gets looked up among the columns of the data frame. If it’s NULL nothing happens, so the data doesn’t get grouped.\nThis is a big topic unto itself, so if you want to know more you can start by reading the famous {dplyr} vignette called Programming with dplyr here4 In case you use {dplyr} a lot, I recommend you do because mastering tidy evaluation (the name of this framework) is key to become comfortable with programming using {dplyr} (and other tidyverse packages). You can also read the chapter I wrote on this in my other free ebook.\nThe next lines of the script that we need to port over to the Rmd are quite standard, we write code to create some plots (which were already refactored into a function in the chapter on collaborating on Github). But remember, we want to have an Rmd file that can be compiled into a document that can be read by humans. This means that to make the document clear, I suggest that we create one subsection by commune that we plot. Thankfully, we have learned all about child documents in the literate programming chapter, and this is what we will be using to avoid having to repeat ourselves. The first part is simply the function that we’ve already wrote:\n```{r}\nmake_plot <- function(commune){\n\n  commune_data <- commune_level_data %>%\n    filter(locality == commune)\n\n  data_to_plot <- bind_rows(\n    country_level_data,\n    commune_data\n  )\n\n  ggplot(data_to_plot) +\n    geom_line(aes(y = pl_m2,\n                  x = year,\n                  group = locality,\n                  colour = locality))\n}\n\n```\nNow comes the interesting part:\n```{r, results = \"asis\"}\nres <- lapply(communes, function(x){\n\n  knitr::knit_child(text = c(\n\n    '\\n',\n    '## Plot for commune: `r x`',\n    '\\n',\n    '```{r, echo = F}',\n    'print(make_plot(x))',\n    '```'\n\n     ),\n     envir = environment(),\n     quiet = TRUE)\n\n})\n\ncat(unlist(res), sep = \"\\n\")\n\n```\nI won’t explain this now in great detail, since that was already done in the chapter on literate programming. Before continuing, really make sure that you understand what is going on here. Take a look at the finalised file here5."
  },
  {
    "objectID": "project_rewrite.html#conclusion",
    "href": "project_rewrite.html#conclusion",
    "title": "9  Rewriting our project",
    "section": "9.3 Conclusion",
    "text": "9.3 Conclusion\nThis chapter was short, but quite dense, especially when we converted the analysis script to an Rmd, because we’ve had to use two advanced concepts, tidy evaluation and Rmarkdown child documents. Tidy evaluation is not a topic that I wanted to discuss in this book, because it doesn’t have anything to do with the main topic at hand. However, part of building a robust, reproducible pipeline is to avoid repetition. In this sense, programming with {dplyr} and tidy evaluation are quite important. As suggested before, take a look at the linked vignette above, and then the chapter from my other (free) ebook. This should help get you started.\nThe end of this chapter marks an important step: many analyses stop here, and this can be due to a variety of reasons. Maybe there’s no time left to go further, and, after all, we got the results we wanted. Maybe this analysis is useful, but we don’t necessarily need it to be reproducible in 5, 10 years, so all we want is to make sure that we can at least rerun it in some months or a couple of years (but be careful with this assessment, sometimes an analysis that wasn’t supposed to be reproducible turns out it needs to be reproducible for way longer than expected…)\nBecause I want this book to be a pragmatic guide, we will now talk about inputting the least amount of effort to make your current analysis reproducible, and this is by freezing package versions, which we will do in the next chapter."
  },
  {
    "objectID": "repro_intro.html#recording-packages-version-with-renv",
    "href": "repro_intro.html#recording-packages-version-with-renv",
    "title": "10  Introduction to reproducibility",
    "section": "10.1 Recording packages’ version with {renv}",
    "text": "10.1 Recording packages’ version with {renv}\nSo now that you’ve used functional and literate programming, we need to start thinking about the infrastructure surrounding our code. By infrastructure I mean:\n\nthe R version;\nthe packages used for the analysis;\nand otherwise the whole computational environment, even the computer hardware itself.\n\n{renv} is a package that takes care of point number 2: it allows you to easily record the packages that were used for a specific project. This record is a file called renv.lock which will appear at the root of your project once you’ve set up {renv} and executed it. You can use {renv} once you’re done with an analysis like in our case, or better yet, immediately at the start, as soon as you start writing library(somePackage). You can keep updating the renv.lock file as you add or remove packages from your analysis. The renv.lock file can then be used to restore the exact same package library that was used for your analysis on another computer, or on the same computer but in the future.\nThis works because {renv} does more than simply create a list of the used packages and recording their versions inside the renv.lock file: it actually creates a per-project library (remember, the library is the set of packages installed on your computer) that is completely isolated for the main, default, R library on your machine, but also from the other {renv} libraries that you might have set up for your other projects. {renv}** enables you to create **R**eproducible **Env**ironments. To save time when setting up an{renv}` library, packages simply get copied over from your main library instead of being re-downloaded and re-installed (if the required packages are already installed in your default library).\nTo get started, install the {renv} package (make sure to start a fresh R session):\n\ninstall.packages(\"renv\")\n\nand then go to the folder containing the Rmds we wrote together in the previous chapter. Make sure that you have the two following files in that folder:\n\nsave_data.Rmd, the script that downloads and prepares the data;\nanalyse_data.Rmd, the script that analyses the data.\n\nAlso, make sure that the changes are correctly backed up on Github.com, so if you haven’t already, commit and push any change.\nOnce this is done, start an R session, and simply type the following in a console:\n\nrenv::init()\n\nYou should see the following:\n\n* Initializing project ...\n* Discovering package dependencies ... Done!\n* Copying packages into the cache ... [76/76] Done!\nThe following package(s) will be updated in the lockfile:\n\n# CRAN ===============================\n***and then a long list of packages***\n\nThe version of R recorded in the lockfile will be updated:\n- R              [*] -> [4.2.2]\n\n* Lockfile written to 'path/to/housing/renv.lock'.\n* Project 'path/to/housing' loaded. [renv 0.16.0]\n* renv activated -- please restart the R session.\n\nLet’s take a look at the files that were created (if you prefer using your file browser, feel free to do so, but I prefer the command line):\nowner@localhost ➤ ls -la\ntotal 1070\ndrwxr-xr-x 1 LLP685 Domain Users     0 Feb 27 12:44 .\ndrwxr-xr-x 1 LLP685 Domain Users     0 Feb 27 12:35 ..\n-rw-r--r-- 1 LLP685 Domain Users    27 Feb 27 12:44 .Rprofile\ndrwxr-xr-x 1 LLP685 Domain Users     0 Feb 27 12:40 .git\n-rw-r--r-- 1 LLP685 Domain Users   306 Feb 27 12:35 README.md\n-rw-r--r-- 1 LLP685 Domain Users  2398 Feb 27 12:38 analyse_data.Rmd\ndrwxr-xr-x 1 LLP685 Domain Users     0 Feb 27 12:44 renv\n-rw-r--r-- 1 LLP685 Domain Users 20502 Feb 27 12:44 renv.lock\n-rw-r--r-- 1 LLP685 Domain Users  6378 Feb 27 12:38 save_data.Rmd\nAs you can see, there are two new files and one folder. The files are the renv.lock file that I mentioned before and a file called .Rprofile. The folder is simply called renv. The renv.lock is the file that lists all the packages used for the analysis. .Rprofile files are files that get read by R automatically at startup (as discussed at the very beginning of part one of this book). You should have a system-wide one that gets read on startups of R, but if R discovers an .Rprofile file in the directory it starts on, then that file gets read instead. Let’s see the contents of this file (you can open this file in any text editor, like Notepad on Windows, but then again I prefer the command line):\ncat .Rprofile\nsource(\"renv/activate.R\")\nThis file runs a script on startup called activate.R, which you can find in the renv folder. Let’s take a look at the contents of this folder:\nls renv\nactivate.R  library  settings.dcf\nSo inside the renv folder, there is another folder called library: this is the folder that contains our isolated library for just this project. activate.R is the script that tells R to use the packages from the library folder at startup. The other file, settings.dcf contains general settings for {renv} which you can safely ignore.\nLet’s start a fresh R session in our project’s directory; you should see the following startup message:\n\n* Project 'path/to/housing' loaded. [renv 0.16.0]\n\nThis means that this R session will use the packages installed in the isolated library we’ve just created. Let’s now take a look at the renv.lock file:\ncat renv.lock\n{\n  \"R\": {\n    \"Version\": \"4.2.2\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://packagemanager.rstudio.com/all/latest\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"MASS\": {\n      \"Package\": \"MASS\",\n      \"Version\": \"7.3-58.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"762e1804143a332333c054759f89a706\",\n      \"Requirements\": []\n    },\n    \"Matrix\": {\n      \"Package\": \"Matrix\",\n      \"Version\": \"1.5-1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"539dc0c0c05636812f1080f473d2c177\",\n      \"Requirements\": [\n        \"lattice\"\n      ]\n\n      ***and many more packages***\nThe renv.lock file is a json file listing all the packages, as well as their dependencies that are used for the project, but it started by stating the R version that was used when it was generated. It is important to remember that when you’ll use {renv} to restore a project’s library on a new machine, the R version will not be restored: so you will be running these old packages on a newer version of R, which may sometimes be a problem (but we’re going to discuss this later).\nSo… that’s it. You’ve generated the renv.lock file, which means that future you, or someone else can restore the library that you used to write this analysis. All that’s required is for that person (or future you) to install {renv} and then use the renv.lock file that you generated to restore the library. Let’s see how this works by cloning the following Github repository on this link1 (forked from this one here2):\ngit clone git@github.com:b-rodrigues/targets-minimal.git\nYou should see a targets-minimal folder on your computer now. Start an R session in that folder and type the following command:\n\nrenv::restore()\n\nYou should be prompted to activate the project before restoring:\n\nThis project has not yet been activated.\nActivating this project will ensure the project library is used during restore.\nPlease see `?renv::activate` for more details.\n\nWould you like to activate this project before restore? [Y/n]: \n\nType Y and you should see a list of packages that need to be installed. You’ll get asked once more if you want to proceed, type y and watch as the packages get installed. If you pay attention to the links, you should see that many of them get pulled from the CRAN archive, for example:\nRetrieving 'https://cloud.r-project.org/src/contrib/Archive/vroom/vroom_1.5.5.tar.gz' ...\nNotice the word “Archive” in the url? That’s because this project uses {vroom} 1.5.5, but as of writing (early 2023), {vroom} is at version 1.6.1.\nNow, maybe you’ve run renv::restore(), but the installation of the packages failed. If that’s the case, let me explain what likely happened.\nI tried restoring the project’s library on two different machines: a Windows laptop and a Linux workstation. renv::restore() failed on the Windows laptop, but succeeded on the Linux workstation.\nWhy does that happen? Well in the case of the Windows laptop, compilation of the {dplyr} package failed. This is likely because my Windows laptop does not have the right version of Rtools installed. If you look inside the renv.lock file that came with the targets-minimal project, you should notice that the recorded R version is 4.1.0, but I’m running R 4.2.2 on my laptop. So libraries get compiled using Rtools 4.2 and not Rtools 4.0 (which includes the libraries for R 4.1 as well).\nSo in order to run this project successfully, I should install the right version of R and Rtools, and this is usually not so difficult, especially on Windows. But that might be a problem on other operating systems. Does that mean that {renv} is useless? No, not at all.\nAt a minimum, {renv} ensures that a project’s library doesn’t interfere with another project’s library. This is especially useful if you’re working on a project for some time (say, several months at least) and want to make sure that you can keep working on other projects in parallel. That’s because what often happens is that you update your packages to use that sweet new feature from some package but when you go back to your long-term project and try to run, it, lo and behold it doesn’t work anymore. This is because another function coming from some other package that also got updated and that you use in your long-term project got removed, or renamed, or simply works differently now. In this scenario, you wouldn’t be troubled by trying to restore the project, since you’re simply using {renv} to isolate the project’s library (but even if you had to restore the library, that would work since you’re using the same R version).\nBut also, apart from that already quite useful feature, renv.lock files provide a very useful blueprint for Docker, which we are going to explore in a future chapter. Only to give you a little taste of what’s coming: since the renv.lock file lists the R version that was used to record the packages, we can start from a Docker image that contains the right version of R. From there, restoring the project using renv::restore() should succeed without issues. If you have no idea what this all means, do not worry, you will know by the end of the book, so hang in there.\nSo should you use {renv}? I see two scenarios where it makes sense:\n\nYou’re done with the project and simply want to keep a record of the packages used. Simply call renv::init() at the end of the project and commit and push the renv.lock file on Github.\nYou want to use {renv} from the start to isolate the project’s library from your whole R installation’s library to avoid any interference (I would advise you to do it like this).\n\nIn the next section, we’ll quickly review how to use {renv} on a “daily basis”.\n\n10.1.1 Daily {renv} usage\nSo let’s say that you start a new project and want to use {renv} right from the start. You start with an empty directory, and add a template .Rmd file, and let’s say it looks like this:\n---\ntitle: \"My new project\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nlibrary(dplyr)\n```\n\n\n## Overview\n\n## Analysis\nBefore continuing, make sure that it correctly compiles into a HTML file by running rmarkdown::render(\"test.Rmd\") in the correct directory.\nIn the setup chunk you load the packages that you need. Now, save this file, and start a fresh session in the same directory and run renv::init(). You should see the familiar prompts described above, as well as the renv.lock file (which will only contain {dplyr} and its dependencies).\nNow, after the library(dplyr) line, add the following library(ggplot2) (or any other package that you use on a daily basis). Make sure to save the .Rmd file and try to render it again by using rmarkdown::render(\"test.Rmd\") (or if you’re using RStudio, by clicking the right button), but, spoiler alert, it won’t work. Instead you should see this:\nQuitting from lines 7-9 (my_new_project.Rmd) \nError in library(ggplot2) : there is no package called 'ggplot2'\nDon’t be confused: remember that {renv} is now activated, and that each project where {renv} is enabled has its own project-wide library. You may have {ggplot2} installed on your system-wide library, but this project does not have it yet. This means that you need to install {ggplot2} for your project. To do so, simply start an R session within your project and run install.packages(\"ggplot2\"). If the version installed on your system-wide library is the latest version available on CRAN, the package will simply be copied over, if not, the latest version will be installed on your project’s library. You can now update the renv.lock file. This is done using renv::snapshot(); this will show you a list of new packages to record inside the renv.lock file and ask you to continue:\n\n**list of many packages over here**\n\nDo you want to proceed? [y/N]: \n* Lockfile written to 'path/to/my_new_project/renv.lock'.\n\nIf you now open the renv.lock file, and look for the string \"ggplot2\" you should see it listed there alongside its dependencies. Let me reiterate: this version of {ggplot2} is now unique to this project. You can work on other projects with other versions of {ggplot2} without interfering with this one. You can even install arbitrary versions of packages using renv::install(). For example, to install an older version of {data.table}:\n\nrenv::install(\"AER@1.0-0\") # this is a version from August 2008\n\nBut just like in the previous section, where we wanted to restore an old project that used {renv}, installation of older packages may fail. If you need to use old packages, there are approaches that work better, which we are also going to going to explore in this chapter.\nBack to daily usage of {renv}: keep installing the required packages for your project and calling renv::snapshot() to keep a record of the library for reproducibility purposes. Once you’re done with your project, you have two possibilities:\n\nYou can renv::snapshot() one last time to make sure that every dependency is correctly accounted for;\nYou update every package in the library and in the lockfile and make sure your project runs with the latest versions of every package. You then provide this updated renv.lock file for future use.\n\nThe second option can be interesting if your project took some time to be developed, and you want to deliver something that depends on current packages. However, only do so if you have written enough tests to detect if a package update could break your project, or else you run the risk of providing a lock file that will install packages with which your project can’t actually run! If you want to play it safe, simply go for the first option.\n\n\n10.1.2 Collaborating with {renv}\n{renv} is also quite useful when collaborating. You can start the project and generate the lock file, and when your team-mates clone the repository from Github, they can get the exact same package versions as you. You all only need to make sure that everyone is running the same R version to avoid any issues.\nThere is a vignette on just this that I invite you to read for more details, see here3.\n\n\n10.1.3 {renv}’s shortcomings\nIn the next section, I’m going to go over two packages that make it easy to install old packages, which can be useful for reproducibility as well. But before that, let’s discuss {renv}’s shortcomings (which we already alluded to before). It is quite important to understand what {renv} does and what it doesn’t do, and why {renv} alone is not enough.\nThe first problem, and I’m repeating myself here, is that {renv} only records the R version used for the project, but does not restore it when calling renv::restore(). You need to install the right R version yourself. On Windows this should be fairly easy to do, but you then need to make sure that you’re running the right version of R with the right scripts, which can get confusing.\nThere is the {rig} package that makes it easy to install and switch between R versions that you could check out4 if you’re interested. However, I don’t think that {rig} should be used for our purposes. I believe that it is safer to use Docker instead, and we shall see how to do so in the coming chapters.\nThe other issue of using {renv} is that future you, or your team-mates or people that want to reproduce your results need to install packages that may be quite difficult to install, either because they’re very old by now, or because their dependencies are difficult to satisfy. Have you ever tried to install a package thet depended on {rJava}? Or the {rgdal} package? Installing these packages can be quite challenging, because they need specific system requirements that may be impossible for you to install (either because you don’t have admin rights on your workstation, or because the required version of these system dependencies is not available anymore). Having to install these packages (and potentially quite old versions at that) can really hinder the reproducibility of your project. Here again, Docker provides a solution. Future you, your team-mates or other people simply need to be able to run a Docker container, which is a much lower bar than installing these old libraries.\nI want to stress that this does not mean that {renv} is useless: we will keep using it, but together with Docker to ensure the reproducibility of our project. As I’ve written above alread, at a minimum {renv} ensures that a project’s library doesn’t interfere with another project’s library and this is in itself already quite useful.\nLet’s now quickly disuss two other packages before finishing this chapter, which provide an answer to the question: how to rerun an old analysis if no renv.lock file is available?"
  },
  {
    "objectID": "repro_intro.html#becoming-an-r-cheologist",
    "href": "repro_intro.html#becoming-an-r-cheologist",
    "title": "10  Introduction to reproducibility",
    "section": "10.2 Becoming an R-cheologist",
    "text": "10.2 Becoming an R-cheologist\nSo let’s say that you need to run an old script, and there’s no renv.lock file around for you to restore the packages as they were. There might still be a solution (apart from running the script on the current version on R and packages, and hope that everything goes well), but for this you need to at least know roughly when that script was written. Let’s say that you know that this script was written back in 2017, somewhere around October. If you know that, you can use the {rang} and {groundhog} packages to download the packages as of October 2018 in a separate library and then run your script.\n{rang} is fairly recent as of writing (February 2023) so I won’t go into too much detail now, as it is likely that the package will keep evolving rapidly in the coming weeks. So if you want to use it already and follow its development, take a look at its Github repository here5.\n{groundhog} is another option that has been around for more time and is fairly easy to use. Suppose that you have a script from October 2018 that looks like this:\n\nlibrary(purrr)\nlibrary(ggplot2)\n\ndata(mtcars)\n\nmyplot <- ggplot(mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\nggsave(\"/home/project/output/myplot.pdf\", myplot)\n\nIf you want to run this script with the versions that were current in October 2017 for the {purrr} and {ggplot2} packages, you can achieve this by simply changing the library() calls:\n\ngroundhog::groundhog.library(\"\n    library(purrr)\n    library(ggplot2)\",\n    \"2017-10-04\"\n    )\n\ndata(mtcars)\n\nmyplot <- ggplot(mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\nggsave(\"/home/project/output/myplot.pdf\", myplot)\n\nbut you will get the following message:\n---------------------------------------------------------------------------\n|IMPORTANT.\n|    Groundhog says: you are using R-4.2.2, but the version of R current \n|    for the entered date, '2017-10-04', is R-3.4.x. It is recommended \n|    that you either keep this date and switch to that version of R, or \n|    you keep the version of R you are using but switch the date to \n|    between '2022-04-22' and '2023-01-08'. \n|\n|    You may bypass this R-version check by adding: \n|    `tolerate.R.version='4.2.2'`as an option in your groundhog.library() \n|    call. Please type 'OK' to confirm you have read this message. \n|   >ok\nSo here again, we are advised to switch to the version of R that was current at that time. If we follow the message’s advice, and add tolerate.R.version = '4.2.2', we may get the script to run:\n\ngroundhog.library(\"\n    library(purrr)\n    library(ggplot2)\",\n    \"2017-10-04\",\n    tolerate.R.version = \"4.2.2\")\n\ndata(mtcars)\n\nmyplot <- ggplot(mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\nggsave(\"/home/project/output/myplot.pdf\", myplot)\n\nBut just like for {renv} (or {rang}), installation of the packages can fail, and for the same reasons (unmet system requirements most of the time).\nSo here again, the solution is to take care of the missing piece of the reproducibility puzzle, which is the whole computational environment itself."
  },
  {
    "objectID": "repro_intro.html#conclusion",
    "href": "repro_intro.html#conclusion",
    "title": "10  Introduction to reproducibility",
    "section": "10.3 Conclusion",
    "text": "10.3 Conclusion\nIn this chapter you had a first (maybe a bit sour taste) of reproducibility. This is because while the tools presented here are very useful, they will not be sufficient if we want our project to be truly reproducible. There are too many things that can go wrong when re-installing old package versions, so we must instead provide a way for users to not have to do it at all. This is where Docker is going to be helpful. But before that, we need to hit the development bench again. We are actually not quite done with our project; before going to full reproducibility, we should turn our analysis into a package. And so you will see, this is going to be much, much, easier than you might expect. You already did 95% of the job! There are many advantages to turning our analysis into a package, and not only from a reproducibility perspective."
  },
  {
    "objectID": "packages.html#benefits-of-packages",
    "href": "packages.html#benefits-of-packages",
    "title": "11  Packaging your code",
    "section": "11.1 Benefits of packages",
    "text": "11.1 Benefits of packages\nLet’s first go over the benefits of turning your analysis into a package once again, as this is crucial.\nThe main point is not to turn the analysis into a package to publish on CRAN (but you can, if you want to). The point is that when you analyse data, more often than not you have to write a lot of custom code, and very often, you don’t expect to write that much custom code. Let’s think about our little project: all we wanted was to create some plots from Luxembourguish houses’ price data. And yet, we had to scrape Wikipedia on two occasion, clean an Excel file, write a test… the project was quite modest, and yet, the amount of code (and thus opportunities to make mistakes) is quite large. But, that’s not something that we could have anticipated, hence why we never really start by writing a package, but a script (or rather, an .Rmd) instead. But then as this script grows larger and larger, we realise that we might need something else that a simple .Rmd file.\nThe other benefit of turning all this code into a package is that we get a clear separation between the code that we wrote purely to get our analysis going (what I called the software development part before) from the analysis itself (which would then typically consist in computing descriptive statisics, run regression or machine learning models, and visualisation). This then in turn means that we can more easily maintain and update each part separately. So the pure software development part goes into the package, which then gives us the possibility to use many great tools to ensure that our code is properly documented and tested, and then the analysis can go inside a purely reproducible pipeline."
  },
  {
    "objectID": "packages.html#fusen-quickstart",
    "href": "packages.html#fusen-quickstart",
    "title": "11  Packaging your code",
    "section": "11.2 {fusen} quickstart",
    "text": "11.2 {fusen} quickstart\nIf you haven’t already, install the {fusen} package:\n\ninstall.packages(\"fusen\")\n\n{fusen} makes the documentation first method proposed by Sébastien Rochette, {fusen} author, reality. The idea is to start from documentation in the form of an .Rmd file and go from there to a package. Let’s dive right into it by starting from a template included in the {fusen} package. Start an R session from your home (or Documents) directory and run the following:\n\nfusen::create_fusen(path = \"fusen.test\", template = \"minimal\")\n\nThis will create a directory called fusen.test inside your home (or Documents) directory. Inside that folder you will find another folder called dev/. Let’s see what’s inside:\nowner@localhost ➤ ls dev/\n\n0-dev_history.Rmd  flat_minimal.Rmd\ndev/ contains two .Rmd files, 0-dev_history.Rmd and flat_miminal.Rmd. They’re both important, so let me explain what they do:\n\nflat_minimal.Rmd is only an example, in practice, we will be using the Rmd file(s) that we have written before (analyse_data.Rmd and save_data.Rmd) instead.\n0-dev_history.Rmd contains lines of code that you typically run when you’re developing a package. For example, a line to initialise Git for the project, a line to add some dependencies, etc, etc. The idea is to write down everything that you type in the console in this file. This leaves a trace of what you have been doing, and also acts as a checklist so that you can make sure that you didn’t forget anything.\n\nBefore describing these files into detail, I want to show you this image taken from {fusen}’s website1:\n\n\n\nfusen takes care of the boring stuff for you!\n\n\nOn the left hand side of the image we see our two .Rmd files. 0-dev_history.Rmd contains a chunk called description. This is the main chunk in that file that we need to execute to get started with {fusen}. Running this chunk will create the package’s DESCRIPTION file (don’t worry if you don’t know about this, I will explain). Then, the second file flat_minimal.Rmd (or our very own .Rmd files) contain functions, tests, examples, and everything we need for our analysis. When we inflate the Rmd file, {fusen} places every piece from this .Rmd file at the right place: the functions get copied into the package’s R/ folder, tests go into the tests/ folder, and so on. {fusen} simply takes care of everything for us!\nBut, for {fusen} to be able to work its magic, we do need to prepare our .Rmd file a bit. But don’t worry, it is mostly simply giving adequate names to our code chunks."
  },
  {
    "objectID": "packages.html#document-your-package",
    "href": "packages.html#document-your-package",
    "title": "11  Packaging your code",
    "section": "11.3 Document your package (?)",
    "text": "11.3 Document your package (?)\nI guess fusen makes this process easy and leverages roxygen?"
  },
  {
    "objectID": "packages.html#managing-package-dependencies",
    "href": "packages.html#managing-package-dependencies",
    "title": "11  Packaging your code",
    "section": "11.4 Managing package dependencies (?)",
    "text": "11.4 Managing package dependencies (?)\nDiscuss NAMESPACE and DESCRIPTION and all that. I think it’s important to also discuss here how to define dependencies from remotes, not just CRAN."
  },
  {
    "objectID": "packages.html#unit-testing",
    "href": "packages.html#unit-testing",
    "title": "11  Packaging your code",
    "section": "11.5 Unit testing",
    "text": "11.5 Unit testing\nThis is where I think we should discuss unit testing"
  },
  {
    "objectID": "packages.html#pkgdown",
    "href": "packages.html#pkgdown",
    "title": "11  Packaging your code",
    "section": "11.6 pkgdown",
    "text": "11.6 pkgdown"
  },
  {
    "objectID": "testing.html#assertive-programming",
    "href": "testing.html#assertive-programming",
    "title": "12  Testing your code",
    "section": "12.1 Assertive programming",
    "text": "12.1 Assertive programming\nThe analysis is still in Quarto, so how could the readers of this book test their code? Copying here what Miles wrote on the subject:\n‘Assertive programming’ is a topic that might be missing from the book. I think of it as a kind of dual of unit testing. Unit testing is for more generally applicable packaged code. But when you have functions in your analysis pipeline that operate on a very specific kind of input data, unit testing becomes kind of nonsensical because you’re left to dream up endless variations of your input dataset that may never occur. It’s a bit easier to flip the effort to validating the assumptions you have about your input and output data, which you can do in the pipeline functions themselves rather than separate unit testing ones. This is nice because it ensures the validation is performed in the pipeline run, and so is backed by the same reproducibility guarantees.\nI think at the end of the chapter we should hint at unit testing, but leave it as a subsection of the next chapter that deals with packaging code.\nhttps://www.brodrigues.co/blog/2022-05-26-safer_programs/"
  },
  {
    "objectID": "targets.html",
    "href": "targets.html",
    "title": "13  Build automation",
    "section": "",
    "text": "Why build automation: removes cognitive load, is a form of documentation in and of itself, as Miles said\nIt is possible to communicate a great deal of domain knowledge in code, such that it is illuminating beyond the mere mechanical number crunching. To do this well the author needs to make use of certain styles and structures that produce code that has layers of domain specific abstraction a reader can traverse up and down as they build their understanding of the project. Functional programming style, coupled with a dependency graph as per {targets} are useful tools in this regard."
  },
  {
    "objectID": "repro_cont.html#first-steps-with-docker",
    "href": "repro_cont.html#first-steps-with-docker",
    "title": "14  Advanced topics in reproducibility",
    "section": "14.1 First steps with Docker",
    "text": "14.1 First steps with Docker\nTo write your own Dockerfile, you need some familiarity with the Linux cli, so here’s…"
  },
  {
    "objectID": "repro_cont.html#a-primer-on-the-linux-command-line",
    "href": "repro_cont.html#a-primer-on-the-linux-command-line",
    "title": "14  Advanced topics in reproducibility",
    "section": "14.2 A primer on the Linux command line",
    "text": "14.2 A primer on the Linux command line"
  },
  {
    "objectID": "repro_cont.html#dockrizing-your-project",
    "href": "repro_cont.html#dockrizing-your-project",
    "title": "14  Advanced topics in reproducibility",
    "section": "14.3 Dockrizing your project",
    "text": "14.3 Dockrizing your project"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in\nR.” Journal of Statistical Software 103\n(1): 1–23.\n\n\nChambers, John M. 2014. “Object-Oriented\nProgramming, Functional Programming and R.”\nStatistical Science 29 (2): 167–80.\n\n\nGohel, David, and Panagiotis Skintzos. 2023. Flextable: Functions\nfor Tabular Reporting.\n\n\nHammant, Paul. 2020. Trunk-Based Development and Branch by\nAbstraction. Leanpub.\n\n\nLeisch, Friedrich. 2002. “Sweave: Dynamic Generation of\nStatistical Reports Using Literate Data Analysis.” In\nCompstat, edited by Wolfgang Härdle and Bernd Rönz, 575–80.\nPhysica-Verlag HD.\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27.\n\n\nTrisovic, Ana, Matthew K Lau, Thomas Pasquier, and Mercè Crosas. 2022.\n“A Large-Scale Study on Research Code Quality and\nExecution.” Scientific Data 9 (1): 60.\n\n\nWickham, Hadley. 2019. Advanced r. CRC press.\n\n\nXie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible\nResearch in R.” In Implementing Reproducible\nComputational Research, edited by Victoria Stodden, Friedrich\nLeisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R\nMarkdown Cookbook. Chapman; Hall/CRC."
  }
]