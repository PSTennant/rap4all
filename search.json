[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building reproducible analytical pipelines with R",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "1  Introduction",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites\nYou should be comfortable with the R programming language. This book will assume that you have been using R for some projects already, and want to improve not only your knowledge of the language itself, but also how to successfully manage complex projects."
  },
  {
    "objectID": "intro.html#what-is-reproducibility",
    "href": "intro.html#what-is-reproducibility",
    "title": "1  Introduction",
    "section": "1.2 What is reproducibility?",
    "text": "1.2 What is reproducibility?"
  },
  {
    "objectID": "intro.html#are-there-different-types-of-reproducibility",
    "href": "intro.html#are-there-different-types-of-reproducibility",
    "title": "1  Introduction",
    "section": "1.3 Are there different types of reproducibility?",
    "text": "1.3 Are there different types of reproducibility?\nReproducibility is on a continuum."
  },
  {
    "objectID": "part1_intro.html#introduction",
    "href": "part1_intro.html#introduction",
    "title": "Part 1: learning the fundamental ingredients of reproducibility",
    "section": "Introduction",
    "text": "Introduction\nPart 1 will focus on teaching you the fundamental ingredients to reproducibility. By fundamental ingredients we mean those tools that you absolutely need to have in your toolbox to before even attempting to make a project reproducible. These tools are so important, that a good chunk of this book is dedicated to them:\n\nVersion control;\nFunctional programming;\nTesting your code;\nLiterate programming.\n\nYou might already be familiar with some of these tools, and maybe already use them in your day to day. If that’s the case, you still might want to at least skim these chapter before tackling part 2 of the book, which will focus on another set of tools to actually build reproducible pipelines.\nSo this means that part 1 will not teach you how to build reproducible pipelines. But we cannot immediately start building reproducible analytical pipelines without first making sure that we understand the core concepts laid out above. To help us understand these concepts, we will start by analysing some data. We are going to download, clean and plot some data, and we will achieve this by writing two scripts. These scripts will be written in a very “typical non software engineery” way, as to mimic how analysts, data scientists or researchers without any formal training in computer science would perform such an analysis. This does not mean that the quality of the analysis will be low. But it means that, typically, these programmers have delievering results fast, and by any means necessary, as the top priority. Our goal with this book is to show you, and hopefully convince you, that by adopting certain simple ideas from software engineering we can actually deliver just as fast as before, but in a more consistent and robust way.\nAnalysing data is not just about delivering a result, but also about making sure that this result can be reproduced on demand, for instance, for auditing purposes. In scientific research, reproducibility is of course extremely important. There is not one researcher that disagrees with that. And yet, there is a reproducibility crisis in research. This crisis is not just about the fact that when we reproduce experiments, for which results were already taken for granted, these do not yield the same results.\nThere are many reasons for this crisis; experiments that may not be designed properly, misuse of statistics, sometimes even fraud.\nLet’s get started!"
  },
  {
    "objectID": "project_start.html#housing-in-luxembourg",
    "href": "project_start.html#housing-in-luxembourg",
    "title": "2  Project start",
    "section": "2.1 Housing in Luxembourg",
    "text": "2.1 Housing in Luxembourg\nWe are going to download data about house prices in Luxembourg. Luxembourg is a little Western European country that looks like a shoe and is about the size of .98 Rhode Islands from the author hails from. Did you know that Luxembourg was a constitutional monarchy, and not a kingdom like Belgium, but a Grand-Duchy, and actually the last Grand-Duchy in the Word in the World? Also, what you should know to understand what we will be doing is that the country of Luxembourg is divided into Cantons, and each Cantons into Communes. Basically, if Luxembourg was the USA, Cantons would be States and Communes would be Counties (or Parishes or Boroughs). What’s confusing is that “Luxembourg” is also the name of a Canton, and of a Commune, which also has the status of a city and is the capital of the country. So Luxembourg the country, is divided into Cantons, one of which is called Luxembourg as well, cantons are divided into communes, and inside the canton of Luxembourg there’s the commune of Luxembourg which is also the city of Luxembourg, sometimes called Luxembourg-City, which is the capital of the country.\n\n\n\nLuxembourg is about as big as the US State of Rhode Island\n\n\nWhat you should also know is that the population is about 645.000 as of writing (January 2023), half of which are foreigners. Around 400.000 persons work in Luxembourg, of which half do not live in Luxembourg; so every morning from Monday to Friday, 200.000 people enter the country to work, and leave on the evening to go back to either Belgium, France or Germany, the neighbouring countries. As you can imagine, this puts enormous pressure on the transportation system and on the roads, but also on the housing market; everyone wants to live in Luxembourg to avoid the horrible daily commute, and everyone wants to live either in the capital city, or in the second largest urban area in the south, in a city called Esch-sur-Alzette.\nThe plot below shows the value of the House Price Index through time for Luxembourg and the European Union:\n\n\n\n\n\nIf you want to download the data, click here.\nLet us paste the definition of the HPI in here (taken from the HPI’s metadata page):\nThe House Price Index (HPI) measures inflation in the residential property market. The HPI captures price changes of all types of dwellings purchased by households (flats, detached houses, terraced houses, etc.). Only transacted dwellings are considered, self-build dwellings are excluded. The land component of the dwelling is included.\nSo from the plot, we can see that the price of dwellings more than doubled between 2010 and 2021; the value of the index is 214.81 in 2021 for Luxembourg, and 138.92 for the European Union as a whole.\nThere is a lot of heterogeneity though; the capital and the communes immediately next to the capital are much more expensive that communes from the less urbanised north, for example. The south of the country is also more expensive than the north, but not as much as the capital and surrounding communes. Not only is price driven by hand demand, but also by scarcity; in 2021, .5% of residents owned 50% of the buildable land for housing purposes (Source: Observatoire de l’Habitat, Note 29, archived download link).\nOur project will be quite simple; we are going to download some data, supplied as an Excel file, compiled by the Housing Observatory (Observatoire de l’Habitat), a service from the Ministry of Housing, which monitors the evolution of prices in the housing market, among other useful services like the identification of vacant lots for example. The advantage of their data when compared to Eurostat’s data is that the data is disaggregated by commune. The disadvantage is that they only supply nominal prices, and no index. Nominal prices are the prices that you read on price tags in shops. The problem with nominal prices is that it is difficult to compare them through time. Ask yourself the following question: would you prefer to have had 500€ (or USDs) in 2003 or in 2023? You probably would have preferred them in 2003, as you could purchase a lot more with 500€ then than now. In fact, according to a random inflation calculator I googled, to match the purchasing power of $500 in 2003, you’d need to have $793 in 2023 (and I’d say that we find very similar values for €). But it doesn’t really matter if that calculation is 100% correct: what matters is that the value of money changes, and comparisons through time are difficult, hence why an index is quite useful. So we are going to convert these nominal prices to real prices. Real prices take inflation into account and so allow us to compare prices through time. So we will need to also get some data to achieve this.\nSo to summarise; our goal is to:\n\nGet data trapped inside an Excel file into a neat data frame;\nConvert nominal to real prices using a simple method;\nMake some tables and plots and call it a day (for now).\n\nWe are going to start in the most basic way possible; we are simply going to write a script and deal with each step separately."
  },
  {
    "objectID": "project_start.html#saving-trapped-data-from-excel",
    "href": "project_start.html#saving-trapped-data-from-excel",
    "title": "2  Project start",
    "section": "2.2 Saving trapped data from Excel",
    "text": "2.2 Saving trapped data from Excel\nGetting data from Excel into a tidy data frame can be very tricky. This is because very often, Excel is used as some kind of dashboard, or presentation tool. So data is made human-readable, in contrast to machine readable. Let us quickly discuss this topic as it is essential to grasp the difference between the two (and in our experience, a lot of collective pain inflicted to statisticians and researchers could have been avoided if this concept was more well-known). The picture below shows an Excel made for human consumption:\n\n\n\nAn Excel file meant for human eyes\n\n\nSo why is this file not machine-readable? Here are some issues:\n\nThe table does not start in the top-left corner of the spreadsheet, which is where most importing tools expect it to be;\nThe spreadsheet start with a head that contains an image and some text;\nNumbers are text and use “,” as the thousands separator;\nYou don’t see it in the screenshot, but each year is in a separate sheet.\n\nThat being said, this one is still very nice, and going from this Excel to a tidy data frame will not be too difficult. In fact, we suspect that whoever made this Excel file is well aware of the contradicting requirements of human and machine readable formatting of data, and strove to find a compromise. Because more often than not, getting human readable data into a machine readable formatting is a nightmare.\nThis is actually the file that we are going to use for our project, so if you want to follow along, you can download it here (downloaded on January 2023 from the luxembourguish open data portal).\nEach sheet contains a dataset with the following columns:\n\nCommune: the commune\nNombre d’offres: the total number of selling offers\nPrix moyen annoncé en Euros courants: Average selling price in nominal Euros\nPrix moyen annoncé au m2 en Euros courants: Average selling price in square meters in nominal Euros\n\nFor ease of presentation, we are going to show you each function here separately, but we’ll be putting everything together in a single script once we’re done explaining each step. So first, let’s read in the data. The following lines do just that:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\n\nurl <- \"https://github.com/b-rodrigues/rap4all/raw/master/datasets/vente-maison-2010-2021.xlsx\"\n\nraw_data <- tempfile(fileext = \".xlsx\")\n\ndownload.file(url, raw_data)\n\nsheets <- excel_sheets(raw_data)\n\nread_clean <- function(..., sheet){\n  read_excel(..., sheet = sheet) |>\n    mutate(year = sheet)\n}\n\nraw_data <- map(\n  sheets,\n  ~read_clean(raw_data,\n              skip = 10,\n              sheet = .)\n                   ) |>\n  bind_rows() |>\n  clean_names()\n\nNew names:\n• `*` -> `*...3`\n• `*` -> `*...4`\n\nraw_data <- raw_data |>\n  rename(\n    locality = commune,\n    n_offers = nombre_doffres,\n    average_price_nominal_euros = prix_moyen_annonce_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant\n  ) |>\n  mutate(locality = str_trim(locality)) |>\n  select(year, locality, n_offers, starts_with(\"average\"))\n\nIf you are familiar with the {tidyverse} the above code should be quite easy to follow. We start by downloading the raw Excel file and save the sheet names into a variable. We then use a function called read_clean(), which takes the path to the Excel file and the sheet names as an argument to read the required sheet into a data frame. We use skip = 10 to skip the first 10 lines in each Excel sheet because the first 10 lines contain a header. The last thing this function is add a new column called year which contains the year of the data. We’re lucky, because the sheet names are simply years: “2010”, “2011” and so on. We then map this function to the list of sheet names, thus reading in all the data from all the sheets into one list of data frames, which then bind by row into a new data frame. Finally, we rename the columns (by translating their names from French to English) and only select the required columns.\nRunning this code results in a neat data set:\n\nstr(raw_data)\n\ntibble [1,343 × 5] (S3: tbl_df/tbl/data.frame)\n $ year                          : chr [1:1343] \"2010\" \"2010\" \"2010\" \"2010\" ...\n $ locality                      : chr [1:1343] \"Bascharage\" \"Beaufort\" \"Bech\" \"Beckerich\" ...\n $ n_offers                      : num [1:1343] 192 266 65 176 111 264 304 94 119 70 ...\n $ average_price_nominal_euros   : chr [1:1343] \"593698.31000000006\" \"461160.29\" \"621760.22\" \"444498.68\" ...\n $ average_price_m2_nominal_euros: chr [1:1343] \"3603.57\" \"2902.76\" \"3280.51\" \"2867.88\" ...\n\n\nBut there’s a problem: columns that should be of type numeric are of type character instead (average_price_nominal_euros and average_price_m2_nominal_euros). There’s also another issue, which you would eventually catch as you would be exploring the data: naming of the communes is not consistent. Let’s take a look:\n\nraw_data |>\n  dplyr::filter(grepl(\"Luxembourg\", locality)) |>\n  dplyr::count(locality)\n\n# A tibble: 2 × 2\n  locality             n\n  <chr>            <int>\n1 Luxembourg           9\n2 Luxembourg-Ville     2\n\n\nWe can see that the city of Luxembourg is spelled in two different ways. It’s the same with another commune, Pétange:\n\nraw_data |>\n  dplyr::filter(grepl(\"P.tange\", locality)) |>\n  dplyr::count(locality)\n\n# A tibble: 2 × 2\n  locality     n\n  <chr>    <int>\n1 Petange      9\n2 Pétange      2\n\n\nSo sometimes it is spelled correctly, with an “é”, sometimes not. Let’s write some code to correct this:\n\nraw_data <- raw_data |>\n  mutate(locality = ifelse(grepl(\"Luxembourg-Ville\", locality),\n                           \"Luxembourg\",\n                           locality),\n         locality = ifelse(grepl(\"P.tange\", locality),\n                           \"Pétange\",\n                           locality)\n         ) |>\n  mutate(across(starts_with(\"average\"), as.numeric))\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\n\nNow this is interesting – converting the average columns to numeric resulted in some NA values. Let’s see what happened:\n\nraw_data |>\n  filter(is.na(average_price_nominal_euros))\n\n# A tibble: 290 × 5\n   year  locality                                        n_off…¹ avera…² avera…³\n   <chr> <chr>                                             <dbl>   <dbl>   <dbl>\n 1 2010  Consthum                                             29      NA      NA\n 2 2010  Esch-sur-Sûre                                         7      NA      NA\n 3 2010  Heiderscheid                                         29      NA      NA\n 4 2010  Hoscheid                                             26      NA      NA\n 5 2010  Saeul                                                14      NA      NA\n 6 2010  <NA>                                                 NA      NA      NA\n 7 2010  <NA>                                                 NA      NA      NA\n 8 2010  Total d'offres                                    19278      NA      NA\n 9 2010  <NA>                                                 NA      NA      NA\n10 2010  Source : Ministère du Logement - Observatoire …      NA      NA      NA\n# … with 280 more rows, and abbreviated variable names ¹​n_offers,\n#   ²​average_price_nominal_euros, ³​average_price_m2_nominal_euros\n\n\nIt turns out that there are no prices for certain communes, but that we also have some rows with garbage in there. Let’s go back to the raw data to see what this is about:\n\n\n\nAlways look at your data\n\n\nSo it turns out that indeed, there are some rows that we need to remove. We can start by removing rows where locality is missing. Then we have a row where locality is equal to “Total d’offres”. This is simply the total of every offer from every commune. We could keep that in a separate data frame, or even remove it. Finally there’s a row, the last one, that states the source of the data, which we can remove.\nIn the screenshot above, we see another row that we don’t see in our filtered data frame: one where n_offers is missing. This row gives the national average for columns average_prince_nominal_euros and average_price_m2_nominal_euros. What we are going to do is create two datasets: one with data on communes, and the other on national prices. Let’s first remove the rows stating the sources:\n\nraw_data <- raw_data |>\n  filter(!grepl(\"Source\", locality))\n\nLet’s now only keep the communes in our data:\n\ncommune_level_data <- raw_data |>\n    filter(!grepl(\"nationale|offres\", locality),\n           !is.na(locality))\n\nAnd let’s create a dataset with the national data as well:\n\ncountry_level <- raw_data |>\n  filter(grepl(\"nationale\", locality)) |>\n  select(-n_offers)\n\noffers_country <- raw_data |>\n  filter(grepl(\"Total d.offres\", locality)) |>\n  select(year, n_offers)\n\ncountry_level_data <- full_join(country_level, offers_country) |>\n  select(year, locality, n_offers, everything()) |>\n  mutate(locality = \"Grand-Duchy of Luxembourg\")\n\nJoining, by = \"year\"\n\n\nNow the data looks clean, and we can start the actual analysis… or can we? Before proceeding, it would be nice to make sure that we got every commune in there. For this, we need a list of communes from Luxembourg. Thankfully, Wikipedia has such a list.\nLet’s scrape and save this list:\n\ncurrent_communes <- \"https://en.wikipedia.org/wiki/List_of_communes_of_Luxembourg\" |>\n  rvest::read_html() |>\n  rvest::html_table() |>\n  purrr::pluck(1) |>\n  janitor::clean_names()\n\nWe scrape the table from the Wikipedia page using {rvest}. rvest::html_table() returns a list of tables from the Wikipedia table, and then we use purrr::pluck() to keep the first table from the website, which is what we need.\nLet’s see if we have all the communes in our data:\n\nsetdiff(unique(commune_level_data$locality), current_communes$commune)\n\n [1] \"Bascharage\"          \"Boevange-sur-Attert\" \"Burmerange\"         \n [4] \"Clémency\"            \"Consthum\"            \"Ermsdorf\"           \n [7] \"Erpeldange\"          \"Eschweiler\"          \"Heiderscheid\"       \n[10] \"Heinerscheid\"        \"Hobscheid\"           \"Hoscheid\"           \n[13] \"Hosingen\"            \"Luxembourg\"          \"Medernach\"          \n[16] \"Mompach\"             \"Munshausen\"          \"Neunhausen\"         \n[19] \"Redange-sur-Attert\"  \"Rosport\"             \"Septfontaines\"      \n[22] \"Tuntange\"            \"Wellenstein\"         \"Kaerjeng\"           \n\n\nWe see many communes that are in our commune_level_data, but not in current_communes. There’s one obvious reason: differences in spelling, for example, “Kaerjeng” in our data, but “Käerjeng” in the table from Wikipedia. But there’s also a less obvious reason; since 2010, several communes have merged into new ones. So there are communes that are in our data, say, in 2010 and 2011, but disappear from 2012 onwards. So we need to do several things: first, get a list of all existing communes from 2010 onwards, and then, harmonise spelling. Here again, we can use a list of Wikipedia:\n\nformer_communes <- \"https://en.wikipedia.org/wiki/Communes_of_Luxembourg#Former_communes\" |>  \n  rvest::read_html() |>\n  rvest::html_table() |>\n  purrr::pluck(3) |>\n  janitor::clean_names() |>\n  dplyr::filter(year_dissolved > 2009)\n\nformer_communes\n\n# A tibble: 20 × 3\n   name                year_dissolved reason                         \n   <chr>                        <int> <chr>                          \n 1 Bascharage                    2011 merged to form Käerjeng        \n 2 Boevange-sur-Attert           2018 merged to form Helperknapp     \n 3 Burmerange                    2011 merged into Schengen           \n 4 Clemency                      2011 merged to form Käerjeng        \n 5 Consthum                      2011 merged to form Parc Hosingen   \n 6 Ermsdorf                      2011 merged to form Vallée de l'Ernz\n 7 Eschweiler                    2015 merged into Wiltz              \n 8 Heiderscheid                  2011 merged into Esch-sur-Sûre      \n 9 Heinerscheid                  2011 merged into Clervaux           \n10 Hobscheid                     2018 merged to form Habscht         \n11 Hoscheid                      2011 merged to form Parc Hosingen   \n12 Hosingen                      2011 merged to form Parc Hosingen   \n13 Mompach                       2018 merged to form Rosport-Mompach \n14 Medernach                     2011 merged to form Vallée de l'Ernz\n15 Munshausen                    2011 merged into Clervaux           \n16 Neunhausen                    2011 merged into Esch-sur-Sûre      \n17 Rosport                       2018 merged to form Rosport-Mompach \n18 Septfontaines                 2018 merged to form Habscht         \n19 Tuntange                      2018 merged to form Helperknapp     \n20 Wellenstein                   2011 merged into Schengen           \n\n\nAs you can see, since 2010 many communes have merged to form new ones. We can now combine the list of current and former communes, as well as harmonise their names:\n\ncommunes <- unique(c(former_communes$name, current_communes$commune))\n# we need to rename some communes\n\n# Different spelling of these communes between wikipedia and the data\n\ncommunes[which(communes == \"Clemency\")] <- \"Clémency\"\ncommunes[which(communes == \"Redange\")] <- \"Redange-sur-Attert\"\ncommunes[which(communes == \"Erpeldange-sur-Sûre\")] <- \"Erpeldange\"\ncommunes[which(communes == \"Luxembourg-City\")] <- \"Luxembourg\"\ncommunes[which(communes == \"Käerjeng\")] <- \"Kaerjeng\"\ncommunes[which(communes == \"Petange\")] <- \"Pétange\"\n\nLet’s run our test again:\n\nsetdiff(unique(commune_level_data$locality), communes)\n\ncharacter(0)\n\n\nGreat! When we compare the communes that are in our data with every commune that has existed since 2010, we don’t have any commune that is unaccounted for. So are we done with cleaning the data? Yes, we can now actually start with analysing the data. Take a look here to see the finalized script. Also read some of the comments the we’ve added. This is a typical R script, and at first glance, one might wonder what is wrong with it. Actually, not much, but the problem if you leave this script as it is, is that it is very likely that we will have problems rerunning it in the future. As it turns out, this script is not reproducible. But we will discuss this in much more detail later on. For now, let’s analyze our cleaned data."
  },
  {
    "objectID": "project_start.html#analysing-the-data",
    "href": "project_start.html#analysing-the-data",
    "title": "2  Project start",
    "section": "2.3 Analysing the data",
    "text": "2.3 Analysing the data\nWe are now going to analyse the data. The first thing we are going to do is compute a Laspeyeres price index. This price index allows us to make comparisons through time; for example, the index at year 2012 measures how much more expensive (or cheaper) housing became relative to the base year (2010). However, since we only have one good, this index becomes quite simple to compute: it is nothing but the prices at year t divided by the prices in 2010 (if we had a basket of goods, we would need to use the Laspeyeres index formula to compute the index at all periods).\nFor this section, we will perform a rather simple analysis. We will immediately show you the R script: take a look at it here. For our analysis we selected 5 communes and plotted the evolution of prices compared to the national average.\nThis analysis might seem trivially simple, but it contains all the needed ingredients to illustrate everything else that we’re going to teach you in this book.\nMost analyses would stop here: after all, we have what we need; our goal was to get the plots for the 5 communes of Luxemourg, Esch-sur-Alzette, Mamer, Schengen (which gave its name to the Schengen Area) and Wincrange. However, let’s ask ourselves the following important questions:\n\nHow easy would it be for someone else to rerun the analysis?\nHow easy would it be to update the analysis once new data gets published?\nHow easy would it be to reuse this code for other projects?\nWhat guarantee do we have that if the scripts get run in 5 years, with the same input data, we get the same output?\n\nLet’s answer these questions one by one."
  },
  {
    "objectID": "project_start.html#your-project-is-not-done",
    "href": "project_start.html#your-project-is-not-done",
    "title": "2  Project start",
    "section": "2.4 Your project is not done",
    "text": "2.4 Your project is not done\n\n2.4.1 How easy would it be for someone else to rerun the analysis?\nThe analysis is composed of two R scripts, one to prepare the data, another to actually run the analysis proper. This might seem quite easy, because each script contains comments as to what is going on, and the code is not that complicated. However, we are missing any project-level documentation, that would provide clear instructions as to how to run it. This might seem simple for us who wrote these scripts, but we are familiar with R, and this is still fresh in our brains. Should someone less familiar with R have to run the script, there is no clue for them as to how they should do it. And of course, should the analysis be more complex (suppose it’s composed of a dozens scripts), this gets even worse. It might not even be easy for you to remember how to run this in 5 months!\nAnd what about the required dependencies? Many packages were used in the analysis. How should these get installed? Ideally, the same versions of the packages you used and the same version of R should get used by that person to rerun the analysis.\nAll of this still needs to get documented.\n\n\n2.4.2 How easy would it be to update the project?\nIf new data gets published, all the points discussed previously are still valid, plus you need to make sure that the updated data is still close enough to the previous data that it can pass through the data cleaning steps you wrote. You should also make sure that the update did not introduce a mistake in past data, or at least alert you if that is the case. Sometimes, when new years get added, data for previous years also get corrected, so it would be nice to make sure that you know this. Also, in the specific case of our data, communes might get fused into a new one, or maybe even divided into smaller communes (even though this is has not happened in a long time, it is not entirely out of the question).\nIn summary, what is missing from the current project are enough tests to make sure that an update to the data can happen smoothly.\n\n\n2.4.3 How easy would it be to reuse this code for another project?\nSaid plainly, not very easy. With code in this state you have no choice but to copy and paste it into a new script and change it adequately. For re-usability, nothing beats structuring your code into functions and ideally you would even package them. We are going to learn just that in future chapters of this book.\nBut sometimes you might not be interested in reusing code for another project: however, even if that’s the case, structuring your code into functions and package them makes it easy to reuse even inside the same project. Look at the last part of the analysis.R script: we copy and pasted the same code 5 times and only slightly changed it. We are going to learn how not to repeat ourselves by using functions and you will immediately see the benefits of writing functions, even when simply to reuse inside the same project.\n\n\n2.4.4 What guarantee do we have that the output is stable?\nNow this might seem weird: after all, if we start from the same dataset, does it matter when we run the scripts? We should be getting the same result if we build the project today, in 5 months or in 5 years. Well, not necessarily. While it is true that R is quite stable, this cannot necessarily be said of the packages that get used. There is no guarantee that the authors of the packages will not change the package’s functions to work differently, or take arguments in a different order, or even that the packages will all be available at all in 5 years. And even if the packages are still available and work the same, bugs in the packages might get corrected that could now alter the result. This might seem like a non-problem; after all, if bugs get corrected, should you be happy to update your results as well? But this depends on what it is we’re talking about. Sometimes it is necessary to reproduce results exactly as they were, if it they were wrong.\nSo we also need a way to somehow snapshot and freeze the computational environment that was used to create the project originally."
  },
  {
    "objectID": "project_start.html#conclusion",
    "href": "project_start.html#conclusion",
    "title": "2  Project start",
    "section": "2.5 Conclusion",
    "text": "2.5 Conclusion\nWe now have a basic analysis that has all we need to get started. In the coming chapters, we are going to learn about topics that will make it easy to write code that is more robust, better documented and tested, and most importantly easy to rerun (and thus to reproduce the results). The first step will actually not involve having to start rewriting our scripts though; next we are going to learn about Git, a tool that will make our life easier by versioning our code."
  },
  {
    "objectID": "git.html#installing-git-and-opening-a-github-account",
    "href": "git.html#installing-git-and-opening-a-github-account",
    "title": "3  Version control",
    "section": "3.1 Installing Git and opening a Github account",
    "text": "3.1 Installing Git and opening a Github account\nGit is a program that you install on your computer. If you’re running a Linux distribution, chances are Git is already installed. Try to run the following command in a terminal to see if this is the case:\nwhich git\nIf a path like /usr/bin/git gets shown, congratulations, you can skip the rest of this paragraph. If something like:\n/usr/bin/which: no git in (/home/username/.local/bin:/home/username/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)\ngets shown instead, then this means that Git is not installed on your system. To install Git, use your distribution’s package manager, as it is very likely that Git is packaged for your system. On Ubuntu, arguably the most popular Linux distribution, this means running:\nsudo apt-get update\nsudo apt-get install git\nOn macOS and Windows, follow the instructions from the Git Book. It should be as easy as running an installer.\nDepending on your operating system, a graphical user interface might have been installed with Git, making it possible to interact with Git outside of the command line. It is also possible to use Git from within RStudio and many other editors have interfaces to Git as well.\nWe are not going to use any graphical user interface however. This is because there is no common, universal graphical user interface; they all work slightly differently. The only universal is the command line. Also, learning how to use Git via the command line will make it easier the day you will need to use it from a server, which will very likely happen. It also makes our job easier: it is simpler to tell you which commands to run and explain them to you than littering the book with dozens upon dozens of screenshots that might get outdated as soon as a new version of the interface gets released.\nDon’t worry, using the command line is not as hard as it sounds.\nIf you don’t have already a Github account, now is the time to create one. Just go over to https://github.com/ and simply follow the instructions and select the free tier to open your account.\n\n\n\nThis is your Github dashboard\n\n\nNow that we have an opened account, we can go to the folder that contains the two scripts we wrote at the start of the book."
  },
  {
    "objectID": "git.html#git-superbasics",
    "href": "git.html#git-superbasics",
    "title": "3  Version control",
    "section": "3.2 Git superbasics",
    "text": "3.2 Git superbasics\nOpen the folder that contains the two scripts in a file explorer. On most Linux desktop environments you should be able to right click inside that folder anywhere and select an option titled something like “Open Terminal here”. On Windows, do the same, but the option is titled “Open Git Bash here”. On macOS, you need to first activate this option. Simply google for “open terminal at folder macOS” and follow the instructions. It is also possible to drag and drop a folder into a terminal which will then open the correct path in the terminal. Another option, of course, is to simply open a terminal and navigate to the correct folder using cd (change directory:\ncd /home/user/housing/\n(The above command assumes that our project is inside a folder called “housing”). Make sure that you are in the right folder by listing the contents of the folder:\nls\nuser@localhost ➤ ls\nanalysis.R save_data.R\n(on Linux you could also try ll which is often available. It is an alias for ls -l which provides a more detailed view. There’s also ls -la which also lists hidden files).\nIt’s now time to start tracking these files using Git. In the same window in which we ran ls, run now the following git command:\ngit init\nuser@localhost ➤ git init\nhint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint: \nhint:   git config --global init.defaultBranch <name>\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint: \nhint:   git branch -m <name>\nInitialized empty Git repository in /home/cbrunos/six_to/housing/.git/\nTake some time to read the hints. Many git commands give you hints and it’s always a good idea to read them. This hint here tells us that the default branch name is “master” and that this is subject to change. For example, if you create a repository on Github, they suggest “main” as the name for the default branch. You need to pay attention to this, because when we will start interacting with our Github repository, we need to make sure that we have the right branch name in mind. Let’s now run this other git command:\nuser@localhost ➤ git status\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        analysis.R\n        save_data.R\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nGit tells us quite clearly that it sees two files, but that they’re currently not being tracked. So if we would modify them, Git would not keep track of the changes. So it’s a good idea to just do what Git tells us to do, let’s add them so that Git can track them:\nuser@localhost ➤ git add\nNothing specified, nothing added.\nhint: Maybe you wanted to say 'git add .'?\nhint: Turn this message off by running\nhint: \"git config advice.addEmptyPathspec false\"\nShoot, simply running git add does do us any good. We need to specify which files we want to add. We can name them one by one, for example git add file1.R file2.txt etc, but if we simply want to track all the files in the folder, we can simply use a . placeholder:\nuser@localhost ➤ git add .\nNo message this time… is that a good thing? Let’s run git status and see what’s going on:\nuser@localhost ➤ git status\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached <file>...\" to unstage)\n        new file:   analysis.R\n        new file:   save_data.R\nNice! Our two files are being tracked now, we can commit the changes. Committing means that we are happy with our work, so we can snapshot it. These snapshots then get uploaded to Github by pushing them. This way, the changes will be available for our coworkers for them to pull. Don’t worry if this is confusing, it won’t be by the end of the chapter. So let’s commit them, but I need to tell you something else first: each commit must have a commit message, and we can write this message as an option to the git commit command:\nuser@localhost ➤ git commit -am \"Project start\"\nApparently the -am option stands for apply mailbox, which we’re sure makes sense to some people, but we prefer to think of -am as standing for add message. All that remains is pushing this commit to Github. But let’s run git status again:\nuser@localhost ➤ git status\nOn branch master\nnothing to commit, working tree clean\nThis means that every change is accounted for in a commit. So if we were to push now, we could then set our computer on fire: every change would be safely backed up on Github.com.\nBefore pushing, let’s see what happens if we change one file. Open “analysis.R” in any editor and simply change the start of the script by adding one line. So go from:\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidyr)\nTo:\n# This script analyses housing data for Luxembourg\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidyr)\nand now run git status again:\nuser@localhost ➤ git status\nOn branch master\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   analysis.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nBecause the file is being tracked, Git can now tell us that something changed and that we did not commit this change. So if our computer would self-combust, these changes would get lost forever. Better commit them and push them to Github.com as soon as possible!\nSo first, we need to add these changes to a commit using git add .:\nuser@localhost ➤ git add .\n(You can run git status at this point to check if the file was correctly added to be committed.)\nThen, we need to commit the changes and add a nice commit message:\nuser@localhost ➤ git commit -am \"Added a comment to analysis.R\"\nTry to keep commit message as short and as explicit as possible. This is not always easy, but it really pays off to strive for short, clear messages. Also, ideally, you would want to keep commits as small as possible. For example, if you’re adding and amending comments in scripts, once you’re done with that make this a commit. Then, maybe clean up some code. That’s another, separate commit. This makes rolling back changes much easier. It is generally not a good idea to code all day and then only push one single big fat commit at the end of the day.\nBy the way, even if our changes are still not on Github.com, we can still now roll back to previous commits. For example, suppose that I delete the file accidentally by running rm analysis.R:\nuser@localhost ➤ rm analysis.R\nOn branch master\nChanges not staged for commit:\n  (use \"git add/rm <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        deleted:    analysis.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nYep, analysis.R is gone. And deleting on the console usually means that the file is gone forever. But thankfully, we were using Git! Because we did not commit the deletion of the file, we can simple tell Git to ignore our changes. A simple way to achieve this is to stash the changes, and then drop (or delete) the stash:\nuser@localhost ➤ git stash\nSaved working directory and index state WIP on master: ab43b4b Added a comment to analysis.R\nSo the deletion was stashed away, (so in case we want it back we could get it back with git stash pop) and our project was rolled back to the previous commit. Simply take a look at the files:\nuser@localhost ➤ ls\nanalysis.R save_data.R\nThere it is! You can get rid of the stash with git stash drop. But what if we had deleted the file and committed the change? In this scenario we could not use git stash, but we would need to revert back to a commit. Let’s try, first let me remove the file:\nuser@localhost ➤ rm analysis.R\nand check the status with git status:\nOn branch master\nChanges not staged for commit:\n  (use \"git add/rm <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        deleted:    analysis.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nLet’s add these changes and commit them:\nuser@localhost ➤ git add .\nuser@localhost ➤ git commit -am \"Removed analysis.R\"\n[master 8e51867] Removed analysis.R\n 1 file changed, 131 deletions(-)\n delete mode 100644 analysis.R\nWhat’s the status now?\nuser@localhost ➤ git status\nOn branch master\nnothing to commit, working tree clean\nNow, we’ve done it! git stash won’t be of any help now. So how to recover our file? For this, we need to know to which commit we want to roll back. Each commit not only has a message, but also an unique identifier that you can access with git log:\nuser@localhost ➤ git log\ncommit 8e51867dc5ae89e5f2ab2798be8920e703f73455 (HEAD -> master)\nAuthor: User <user@mailbox.com>\nDate:   Sun Feb 5 17:54:30 2023 +0100\n\n    Removed analysis.R\n\ncommit ab43b4b1069cd987685253632827f19d7a402b27\nAuthor: User <user@mailbox.com>\nDate:   Sun Feb 5 17:41:52 2023 +0100\n\n    Added a comment to analysis.R\n\ncommit df2beecba0101304f1b56e300a3cd713ce7366e5\nAuthor: User <user@mailbox.com>\nDate:   Sun Feb 5 17:32:26 2023 +0100\n\n    Project start\n\nThe first one from the top is the last commit we’ve made. We would like to go back to the one with the message “Added a comment to analysis.R”. See the very long string of characters after “commit”? That’s the commit unique identifier, called hash. You need to copy it (or only like the first 10 or so characters, that’s enough as well). By the way, depending on your terminal and operating system, git log may open less to view the log. less is a program that makes it easy to view long documents. Quit it by simply pressing q on your keyboard. We are now ready to revert back to the right commit with the following command:\nuser@localhost ➤ git revert ab43b4b1069cd98768..HEAD\nand we’re done! Check that all is right by running ls to see that the file magically returned, and git log to read the log of what happened:\nuser@localhost ➤ git log\ncommit b7f82ee119df52550e9ca1a8da2d81281e6aac58 (HEAD -> master)\nAuthor: User <user@mailbox.com>\nDate:   Sun Feb 5 18:03:37 2023 +0100\n\n    Revert \"Removed analysis.R\"\n    \n    This reverts commit 8e51867dc5ae89e5f2ab2798be8920e703f73455.\n\ncommit 8e51867dc5ae89e5f2ab2798be8920e703f73455 (HEAD -> master)\nAuthor: User <user@mailbox.com>\nDate:   Sun Feb 5 17:54:30 2023 +0100\n\n    Removed analysis.R\n\ncommit ab43b4b1069cd987685253632827f19d7a402b27\nAuthor: User <user@mailbox.com>\nDate:   Sun Feb 5 17:41:52 2023 +0100\n\n    Added a comment to analysis.R\n\ncommit df2beecba0101304f1b56e300a3cd713ce7366e5\nAuthor: User <user@mailbox.com>\nDate:   Sun Feb 5 17:32:26 2023 +0100\n\n    Project start\nThis small example illustrates how useful Git is, even without using Github, and even if working alone on a project. At the very least it offers you a way to simply walk back changes and gives you a nice timeline of your project. Maybe this does not impress you much, because we live in a world where cloud services like Dropbox made things like this very accessible. But where Git (with the help of a service like Github) really shines is when collaboration is needed. Git and code housting services like Github make it possible to collaborate at very large scale: thousands of developers contribute to the Linux kernel, arguably the most successful open source project ever, powering most of today’s smartphones, servers, super computers and embedded computers.[^].\n1"
  },
  {
    "objectID": "git.html#git-and-github",
    "href": "git.html#git-and-github",
    "title": "3  Version control",
    "section": "3.3 Git and Github",
    "text": "3.3 Git and Github\nSo we got some work done on our machine and made some commits. We are now ready to push these commits to Github. “Pushing” means essentially uploading these changes to Github. This makes them available to your coworkers if you’re pushing to a private repository, or makes them available to the world if you’re pushing to a public repository.\nBefore pushing anything to Github though, we need to create a new repository. This repository will contain the code to our project, as well as all the changes that Git has been tracking on our machine. So if, for example, a new team member joins, he or she will be able to clone the repository to his or her computer and every change, every commit message, every single bit of history of the project will be accessible. If it’s a public repository, anyone will be able to clone the repository and contribute code to it. We are going to walk you true some examples of how to collaborate with Git using Github in the remained of this chapter.\nSo, let’s first go back to https://github.com/ and create a new repository:\n\n\n\nCreating a new repository from your dashboard\n\n\nYou will then land on this page:\n\n\n\nName your repository and choose whether it’s a public or private repository\n\n\nName your repository, and choose whether it should be open to the word or if it should be private and only accessible to your coworkers. We are going to make it a public repository, but you could make it private and follow along, this would change nothing to what we’re going to learn.\nWe then land on this page:\n\n\n\nSome instructions to get you started\n\n\nWe get some instructions on how to actually get started with our project. The first thing you need to do though is click on “SSH”:\n\n\n\nSome instructions to get you started\n\n\nThis will changes the links in the instructions from https to ssh. We will explain why this is important in a couple paragraphs. For now, let’s read the instructions. Since we have already started working, we need to follow the instructions title “…or push an existing repository from the command line”. Let’s review these commands. This is what Github suggests we run:\ngit remote add origin git@github.com:rap4all/housing.git\ngit branch -M main\ngit push -u origin main\nWhat’s really important is the first command and last command. The first command adds a remote that we name origin. The link you see is the link to our repository. This links our folder in our machine to the Github repository online. So now, every time we push, our changes will get uploaded to Github. The second line renames the branch from “master” to “main”. You are of course free to do so. We will not do it ourselves, since the default branch Git creates is called “master” so we are going to keep using it to avoid confusion. The last command pushes our changes to the “main” branch (but we need to change “main” to “master”).\nLet’s do just that:\nuser@localhost ➤ git remote add origin git@github.com:rap4all/housing.git\nThis produces no output. We’re now ready to push:\nuser@localhost ➤ git push -u origin master\nand it fails:\nERROR: Permission to rap4all/housing.git denied to b-rodrigues.\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.\nThe reason is quite simple: Github has absolutely no idea who we are! Remember, if the repository is public, anyone can clone it. But that doesn’t mean that anyone can simply push code to the repo! This means that we need a way to tell Github that we are the owner of the repository. For this, we need a way to login securely, and we will do so using a public/private rsa key pair. The idea is quite simple; we are going to generate two files on our computer. These two files form a public/private key pair. We are going to upload the public key to Github; and every time we want to interact with Github, Github will check the public key to the private key that we keep on our machine (never, ever, send the private key to anyone). If they match, Github knows that we are who we claim to be and will let us push to the repository. This is why we switched from https to ssh before. https would allow us to login by typing a password each time we push (but actually, not anymore password login was turned off some years ago). It is much easier to not have to login manually and let our key pair do the job for us.\nLet’s generate a public/private rsa key pair. Open a terminal on Linux or macOS, or Git Bash on Windows and run the following command:\nuser@localhost ➤ ssh-keygen\nThe following lines will appear in your terminal:\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa): \nSimply leave this empty and press enter. This next message now appears:\nEnter passphrase (empty for no passphrase): \nLeave it empty as well. Entering a passphrase is not really needed, since the ssh key pair itself will deal with the login. In some situations a passphrase might be useful, if you’re worried that someone might get physical access to your machine and push code by impersonating you. But if you work with such sensitive data and code that this is a real worry, maybe don’t use Github?\nSo once you pressed enter, you get asked to confirm the passphrase:\nEnter same passphrase again: \nHere again, simply leave it empty and press enter on your keyboard. Once this is done, you should see this:\nYour identification has been saved in /home/user/.ssh/id_rsa\nYour public key has been saved in /home/user/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:tPZnR7qdN06mV53Mc36F3mASIyD55ktQJFBAVqJXNQw user@localhost\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .*=E*=.       |\n|   o o.oo.. .    |\n|  . .  o.  o o   |\n|   .  ..o.  . o  |\n|       +S    o.+.|\n|       .o.   o.o*|\n|       . o. + +=*|\n|        .  o ++*=|\n|            ..=oo|\n+----[SHA256]-----+\nIf now you go to the specified path on the first line (so in our case /home/user/.ssh/ you should see two files, id_rsa and id_rsa.pub, the private and public keys respectively. We’re almost done: what you need to do now is copy the contents of the id_rsa.pub file to Github. Go to your profile settings:\n\n\n\nClick on your user profile’s image in the top-right corner\n\n\nAnd then click on “SSH and GPG keys”:\n\n\n\nGo to your user settings and choose “SSH and GPG keys”\n\n\nand then click on “New SSH key”. Name this key (it’s a good idea to write something that makes recognizing the machine the key was generated easy) and paste the contents of id_rsa.pub in the text box and click on “add SSH key”:\n\n\n\nCopy the contents of the public key in here\n\n\nWe can now go back to our terminal and try to push again:\nuser@localhost ➤ git push -u origin master\nThe following message gets printed:\nThe authenticity of host 'github.com (140.82.121.3)' can't be established.\nED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nType yes and then you should see the following:\nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (9/9), done.\nWriting objects: 100% (10/10), 2.77 KiB | 2.77 MiB/s, done.\nTotal 10 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), done.\nTo github.com:rap4all/housing.git\n * [new branch]      master -> master\nBranch 'master' set up to track remote branch 'master' from 'origin'.\nAnd we’re done! Our commits are now safely backed up on Github. If we go to our repository’s main page, we should see the following:\n\n\n\nFinally!"
  },
  {
    "objectID": "git.html#getting-to-know-github",
    "href": "git.html#getting-to-know-github",
    "title": "3  Version control",
    "section": "3.4 Getting to know Github",
    "text": "3.4 Getting to know Github\nWe have succeeded in installing Git and making it work with our Github account. If you use another machine for development, you will need to generate another rsa key pair on that machine and add the public key to Github. If you use another code hosting platform, you can use the same rsa key pair, but will need to add the public key to this other code hosting platform. You can even use the same key pair as a passwordless authentication method for ssh (for example if you work on a server, but this is outside the scope of the present book). Before continuing with Git itself, and learn about collaborating using Git, we are going to take a little tour of Github itself.\n\n\n\nYou repository’s landing page\n\n\nOnce you’re on your repository’s landing page you see the same files and folders as in the root directory of the project on your computer. In our case here, we see our two files. Github suggests that we add a README file; we are going to ignore this for now. Take a closer look at the menu at the top, below your repository’s name:\n\n\n\nSeveral options to choose from\n\n\nMost important for our needs is the “Issues”, “Pull requests”, “Actions” and “Settings” tab. Let’s start with “Settings”.\n\n\n\nChoose the “Settings” tab\n\n\nThere are many options that you can choose from, but what’s important for our purposes is the “Collaborators” option. This is where you can invite people to contribute to the repository. People that are invited in this way can directly push to the repository. Let’s invite the author of this book:\n\n\n\nChoose the “Settings” tab\n\n\nStart by typing the person’s Github username. You can also invite collaborators by providing their email address.\n\n\n\nChoose the “Settings” tab\n\n\nClick then on the user’s profile and he or she should get an invitation per email.\nThis is what it looks like from the perspective of Bruno’s account now:\n\n\n\nBruno can now push as if he owned the repository\n\n\nIt’s important to understand the distinction between inviting someone to contribute to the repository and having someone contribute, even though that person has not been specifically invited to do so. We are going to explore these two scenarios in the next section, but before that, let’s see what the “Issues” tab is about.\n\n\n\nAnyone can open an issue in a public repository\n\n\nIf the repository is public, anyone can open an issue to either submit a bug, or suggest some ideas, and if the repository is private, only invited collaborators can do this.\nLet’s open an issue to illustrate how this works:\n\n\n\nWrite what the issue’s about here\n\n\nGive a nice title to the issue (1), add a thorough description (2), (optionally) assign it to someone (3) and (optionally) add a label to it (4), finally click on “Submit new issue” (5) to submit the issue:\n\n\n\nTry to provide as many details as possible\n\n\nSometimes issues don’t need to be very long, and act more as reminders than anything else. For example here, the owner of the repository didn’t have the time to add a Readme, but didn’t want to forget to add one later on. The author assigned the issue to Bruno: so it’ll be Bruno’s job to add the Readme. Issue-driven project management is a very valid strategy when working asynchronously and in a decentralized fashion.\nIf you encountered a bug and want to open an issue, it is very important that you provide a minimal, reproducible example (MRE). MREs are snippets of code that can be run very easily by someone other than yourself and which produce the bug reliably. Interestingly, if you understand what makes an MRE minimal and reproducible, you understand what will make our pipelines reproducible as well. So what’s important for an MRE?\nFirst, the code needs to be self-contained. For example, if some data is required you need to provide the data. If the data is sensitive, you need to think about the bug in greater detail: is the bug due to the structure of the data, or does the bug manifest itself on any kind of data? If that’s the case, use some of the built-in datasets to R (iris, mtcars, etc) for your MRE.\nDoes your MRE require extra packages to run? Then make this as clear as possible, and not only provide the package names, but also their versions (it is a good idea to copy and paste the output of sessionInfo() at the end of the issue.\nFinally, does your example depend on some object defined in the global state? If yes, you also need to provide the code to create this object.\nThe bar you need to set for an MRE is as follows: bar needed package dependencies that may need to be installed beforehand, people that try to help you should be able to run your script by simply copy and pasting it into an R console. Any other manipulation that you require from them is unacceptable: remember that in open source development, developers very often work in their free time, and don’t owe you tech support! And even if they did, it is always a good idea to make it as easy as possible for them to help you, because it simply increases the likelihood that they will actually help.\nAlso, writing an MRE can usually make you actually debug the code yourself. Just like in rubber duck debugging, the fact of simply trying to explain the problem can lead to finding what’s wrong. But by writing an MRE, you’re also reducing the problem into its most basic parts, and removing everything unnecessary. By doing so, you might realize that what you thought was a bug of the library was maybe rather a problem between the keyboard and the chair.\nSo don’t underestimate the usefulness of creating high quality MREs for your issues! One package that can assist you with this is {reprex} (read about it here)."
  },
  {
    "objectID": "git.html#collaborating-with-github",
    "href": "git.html#collaborating-with-github",
    "title": "3  Version control",
    "section": "3.5 Collaborating with Github",
    "text": "3.5 Collaborating with Github\nAs already mentioned several times, there are two ways of collaborating with Git (and Github): either as a team, or either as an external dev (external, as in, not part of the development team) who wishes to provide some code to a project (this only works for repositories that are public).\n\n3.5.1 Collaborating as a team\nRemember the"
  },
  {
    "objectID": "fprog.html#introduction",
    "href": "fprog.html#introduction",
    "title": "4  Functional programming",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nYou are very likely already familiar with some aspects of functional program. Let’s start by discussing the two central elements of functional programming: functions and lists.\nThere are several ways that you can structure a program, called programming paradigms. Functional programming is a paradigm that relies exclusively on the evaluation of functions to achieve the desired end result. If you have already written your own functions in the past, what follows will not be very new. But in order to write a good functional program, the functions that you write and evaluate have to have certain properties. Before discussing these properties, let’s start by with state.\n\n4.1.1 The state of your program\nLet’s suppose that you start a fresh R session, and immediately run this next line:\n\nls()\n\nIf you did not modify any of R’s configuration files that get automatically loaded on startup, you should see the following:\n\ncharacter(0)\n\nLet’s suppose that now you load some data:\n\ndata(mtcars)\n\nand define a variable a:\n\na <- 1\n\nRunning ls() now shows the following:\n\n[1] \"a\"      \"mtcars\"\n\nYou have just altered the state of your program. You can think of the state as a box that holds everything that gets defined by the user and is accessible at any time. Let’s now define a simple function that prints a sentence:\n\nf <- function(name){\n  print(paste0(name, \" likes lasagna\"))\n}\n\nf(\"Bruno\")\n\nand here’s the output:\n\n[1] \"Bruno likes lasagna\"\n\nLet’s run ls() again:\n\n[1] \"a\"      \"f\"      \"mtcars\"\n\nFunction f() is now listed there as well. This function has two nice properties:\n\nFor a given input, it always returns exactly the same output. So f(\"Bruno\") will always return “Bruno likes lasagna”.\nThis function does not change the state of your program, by adding new objects every time it’s run.\n\n\n\n4.1.2 Predictable functions\nLet’s now define another function called g(), that does not have the same properties as f(). First, let’s define a function that does not always return the same output given a particular input:\n\ng <- function(name){\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n  print(paste0(name, \" likes \", food))\n}\n\nFor the same input, “Bruno”, this function now produces (potentially) a different output:\n\ng(\"Bruno\")\n[1] \"Bruno likes lasagna\"\n\n\ng(\"Bruno\")\n[1] \"Bruno likes feijoada\"\n\nAnd now let’s consider function h() that modifies the state of the program:\n\nh <- function(name){\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  if(exists(\"food_list\")){\n    food_list <<- append(food_list, food)\n  } else {\n    food_list <<- append(list(), food)\n  }\n\n  print(paste0(name, \" likes \", food))\n}\n\nThis function uses the <<- operator. This operator saves definitions that are made inside the body of functions in the global environment. Before calling this function, run ls() again. You should see the same objects as before, plus the new functions we’ve defined:\n\n[1] \"a\"         \"f\"          \"g\"         \"h\"         \"mtcars\"   \n\nLet’s now run h() once:\n\nh(\"Bruno\")\n[1] \"Bruno likes feijoada\"\n\nAnd now ls() again:\n\n[1] \"a\"         \"f\"         \"food_list\" \"g\"         \"h\"         \"mtcars\" \n\nRunning h() did two things: it printed the message, but also created a variable called “food_list” in the global environment with the following contents:\n\nfood_list\n\n\n[[1]]\n[1] \"feijoada\"\n\nLet’s run h() again:\n\nh(\"Bruno\")\n[1] \"Bruno likes cassoulet\"\n\nand let’s check the contents of “food_list”:\n\nfood_list\n\n\n[[1]]\n[1] \"feijoada\"\n\n[[2]]\n[1] \"cassoulet\"\n\nIf you keep running h(), this list will continue growing. Let me just say that I hesitated showing you this; this is because if you didn’t know <<-, you might find the example above useful. But while useful, it is quite dangerous as well. Generally, we want to avoid using functions that change the state as much as possible because these function are unpredictable, especially if randomness is involved. It is much safer to define h() like this instead:\n\nh <- function(name, food_list = list()){\n\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  food_list <- append(food_list, food)\n\n  print(paste0(name, \" likes \", food))\n\n  food_list\n}\n\nThe difference now is that we made food_list the second argument of the function. Also, we defined it as being optional by writing:\n\nfood_list = list()\n\nThis means that if we omit this argument, the empty list will get used by default. This avoids the users having to manually specify it.\nWe can call it like this:\n\nfood_list <- h(\"Bruno\", food_list) # since food_list is already defined, we don't need to start with an empty list\n\n\n[1] \"Bruno likes feijoada\"\n\nWe save the output back to food_list. Let’s now check its contents:\n\nfood_list\n\n\n[[1]]\n[1] \"feijoada\"\n\n[[2]]\n[1] \"cassoulet\"\n\n[[3]]\n[1] \"feijoada\"\n\nThe only thing that we need now to deal with is the fact that the food gets chosen randomly. I’m going to show you the simple way of dealing with this, but later in this chapter we are going to use the {withr} package for situations like this. Let’s redefine h() one last time:\n\nh <- function(name, food_list = list(), seed = 123){\n\n  # We set the seed, making sure that we get the same selection of food for a given seed\n  set.seed(seed)\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  # We now need to unset the seed, because if we don't, guess what, the seed will stay set for the whole session!\n  set.seed(NULL)\n\n  food_list <- append(food_list, food)\n\n  print(paste0(name, \" likes \", food))\n\n  food_list\n}\n\nLet’s now call h() several times with its default arguments:\n\nh(\"Bruno\")\n\n\n[1] \"Bruno likes feijoada\"\n[[1]]\n[1] \"feijoada\"\n\n\nh(\"Bruno\")\n\n\n[1] \"Bruno likes feijoada\"\n[[1]]\n[1] \"feijoada\"\n\n\nh(\"Bruno\")\n\n\n[1] \"Bruno likes feijoada\"\n[[1]]\n[1] \"feijoada\"\n\nAs you can see, every time this function runs, it now produces the same result. Users can change the seed to have this function produce, consistently, another result.\n\n\n4.1.3 Referentially transparent and pure functions\nA referentially transparent function is a function that does not use any variable that is not also one of its inputs. For example, the following function:\n\nbad <- function(x){\n  x + y\n}\n\nis not referentially transparent, because y is not one of the functions inputs. What happens if you run bad() is that bad() needs to look for y. Because y is not one of its inputs, bad() then looks for it in the global environment. If y is defined there, it then gets used. Defining and using such functions must be avoided at all costs, because these functions are unpredictable. For example:\n\ny <- 10\nbad <- function(x){\n  x + y\n}\n\nbad(5)\n\nThis will return 15. But if y <- 45 then bad(5) would this time around return 50. It is much safer, and easier to make y an explicit input of the function instead of having to keep track of y’s value:\n\ngood <- function(x, y){\n  x + y\n}\n\ngood() is a referentially transparent function; it is much safer than bad(). good() is also a pure function, because it’s a function that does not interact in any way with the global environment. It does not write anything to the global environment, nor requires anything from the global environment. Function h() from the previous section was not pure, because it created an object and wrote it to the global environment (the food_list object). Turns out that pure functions are thus necessarily referentially transparent.\nSo the first lesson in your functional programming journey that you have to remember is to only use pure functions."
  },
  {
    "objectID": "fprog.html#writing-good-functions",
    "href": "fprog.html#writing-good-functions",
    "title": "4  Functional programming",
    "section": "4.2 Writing good functions",
    "text": "4.2 Writing good functions\n\n4.2.1 Functions are first class objects\nIn a functional programming language, functions are first class objects. Contrary to what the name implies, this means that functions, especially the ones you define yourself, are nothing special. A function is an object like any other, and can thus be manipulated as such. Think of anything that you can do with any object in R, and you can do the same thing with a function. For example, let’s consider the +() function. It takes two numeric objects and returns their sum:\n\n1 + 5.3\n\n[1] 6.3\n\n# or alternatively: `+`(1, 5.3)\n\nYou can replace the numbers by functions that return numbers:\n\nsqrt(1) + log(5.3)\n\n[1] 2.667707\n\n\nIt’s also possible to define a function that explicitly takes another function as an input:\n\nh <- function(number, f){\n  f(number)\n}\n\nYou can call then use h() as a wrapper for f():\n\nh(4, sqrt)\n\n[1] 2\n\nh(10, log10)\n\n[1] 1\n\n\nBecause h() takes another function as an argument, h() is called a higher-order function.\nIf you don’t know how many arguments f(), the function you’re wrapping, has, you can use the ...:\n\nh <- function(number, f, ...){\n  f(number, ...)\n}\n\n... are simply a placeholder for any potential additional argument that f() might have:\n\nh(c(1, 2, NA, 3), mean, na.rm = TRUE)\n\n[1] 2\n\nh(c(1, 2, NA, 3), mean, na.rm = FALSE)\n\n[1] NA\n\n\nna.rm is an argument of mean(). As the developer of h(), I don’t necessarily know what f() might be, or maybe I know f() and know all its arguments, but don’t want to have to rewrite them all to make them arguments of h(), so I can use ... instead. The following is also possible:\n\nw <- function(...){\npaste0(\"First argument: \", ..1, \", second argument: \", ..2, \", last argument: \", ..3)\n}\n\nw(1, 2, 3)\n\n[1] \"First argument: 1, second argument: 2, last argument: 3\"\n\n\nIf you want to learn more about ..., type ?dots in an R console.\nBecause functions are nothing special, you can also write functions that return functions. As an illustration, we’ll be writing a function that converts warnings to errors. This can be quite useful if you want your functions to fail early, which often makes debugging easier. For example, try running this:\n\nsqrt(-5)\n\nWarning in sqrt(-5): NaNs produced\n\n\n[1] NaN\n\n\nThis only raises a warning and returns NaN (Not a Number). This can be quite dangerous, especially when working non-interactively, which is what we will be doing a lot later on. It is much better if a pipeline fails early due to an error, than dragging an NaN value. This also happens with log():\n\nsqrt(-10)\n\nWarning in sqrt(-10): NaNs produced\n\n\n[1] NaN\n\n\nSo it could be useful to redefine this functions to raise an error instead, for example like this:\n\nstrict_sqrt <- function(x){\n\n  if(x <= 0) stop(\"x is negative\")\n\n  sqrt(x)\n\n}\n\nThis function now throws an error for negative x:\n\nstrict_sqrt(-10)\n\nError in strict_sqrt(-10) : x is negative\nHowever, it can be quite tedious to redefine every function that we need in our pipeline. This is where a function factory is useful. We can define a function that takes a function as an argument, converts any warning thrown by that function into an error, and returns the new function. For example it could look like this:\n\nstrictly <- function(f){\n  function(...){\n    tryCatch({\n      f(...)\n    },\n    warning = function(warning)stop(\"Can't do that chief\"))\n  }\n}\n\nThis function makes use of tryCatch() which catches warnings raised by an expression (in this example the expression is f(...)) and then raises an error instead with the stop() function. It is now possible to define new functions like this:\n\ns_sqrt <- strictly(sqrt)\n\n\ns_sqrt(-4)\n\nError in value[[3L]](cond) : Can't do that chief\n\ns_log <- strictly(log)\n\n\ns_log(-4)\n\nError in value[[3L]](cond) : Can't do that chief\nFunctions that return functions are called functions factories and they’re incredibly useful. I use this so much that I’ve written a package, available on CRAN, called {chronicler}, that does this:\n\ns_sqrt <- chronicler::record(sqrt)\n\n\nresult <- s_sqrt(-4)\n\nresult\n\nNOK! Value computed unsuccessfully:\n---------------\nNothing\n\n---------------\nThis is an object of type `chronicle`.\nRetrieve the value of this object with pick(.c, \"value\").\nTo read the log of this object, call read_log(.c).\n\n\nBecause the expression above resulted in an error, Nothing is returned. Nothing is a special value defined in the {maybe} package (check it out, very interesting package!). We can then even read the log to see what went wrong:\n\nchronicler::read_log(result)\n\n[1] \"Complete log:\"                                                                                \n[2] \"NOK! sqrt() ran unsuccessfully with following exception: NaNs produced at 2023-02-10 17:34:21\"\n[3] \"Total running time: 0.00129222869873047 secs\"                                                 \n\n\nThe {purrr} package also comes with function factories that you might find useful ({possibly}, {safely} and {quietly}).\n\n\n4.2.2 Optional arguments\nIt is possible to make function arguments optional, by using NULL. For example:\n\ng <- function(x, y = NULL){\n  if(is.null(y)){\n    print(\"optional argument y is NULL\")\n    x\n  } else {\n    if(y == 5) print(\"y is present\"); x+y\n  }\n}\n\nCalling g(10) prints the message “Optional argument y is NULL”, and returns 10. Calling g(10, 5) however, prints “y is present” and returns 15. It is also possible to use missing():\n\ng <- function(x, y){\n  if(missing(y)){\n    print(\"optional argument y is missing\")\n    x\n  } else {\n    if(y == 5) print(\"y is present\"); x+y\n  }\n}\n\nI however prefer the first approach, because it is clearer which arguments are optional, which is not the case with the second approach, where you need to read the body of the function.\n\n\n4.2.3 Safe functions\nIt is important that your functions are safe and predictable. You should avoid writing functions that behave like nchar(), a base R function. Let’s see why this function is not safe:\n\nnchar(\"10000000\")\n\n[1] 8\n\n\nIt returns the expected result of 8. But what if I remove the quotes?\n\nnchar(10000000)\n\n[1] 5\n\n\nWhat is going on here? I’ll give you a hint: simply type 10000000 in the console:\n\n10000000\n\n[1] 1e+07\n\n\n10000000 gets represented as 1e+07 by R. This number in scientific notation gets then converted into the character “1e+07” by nchar(), and this conversion happens silently. nchar() then counts the number of characters, and correctly returns 5. The problem is that it doesn’t make sense to provide a number to a function that expects a character. This function should have returned an error message, or at the very least raised a warning that the number got converted into a character. Here is how you could rewrite nchar() to make it safer:\n\nnchar2 <- function(x, result = 0){\n\n  if(!isTRUE(is.character(x))){\n    stop(paste0(\"x should be of type 'character', but is of type '\",\n                typeof(x), \"' instead.\"))\n  } else if(x == \"\"){\n    result\n  } else {\n    result <- result + 1\n    split_x <- strsplit(x, split = \"\")[[1]]\n    nchar2(paste0(split_x[-1],\n                     collapse = \"\"), result)\n  }\n}\n\nThis function now returns an error message if the input is not a character:\n\nnchar2(10000000)\n\nError in nchar2(10000000) : x should be of type 'character', but is of type 'integer' instead. \n\n\n4.2.4 Recursive functions\nYou may have noticed that in the last lines of nchar2(), that nchar2() calls itself. A function that calls itself in its own body is called a recursive function. It is sometimes easier to write down a function in its recursive form than in an iterative form. The most common example is the factorial function. However, there is an issue with recursive functions (in the R programming language, other programming languages may not have the same problem, like Haskell): while it is sometimes easier to write down a function using a recursive algorithm than an iterative algorithm, like for the factorial function, recursive functions in R are quite slow. Let’s take a look at two definitions of the factorial function, one recursive, the other iterative:\n\nfact_iter <- function(n){\n  result = 1\n  for(i in 1:n){\n    result = result * i\n    i = i + 1\n  }\n  result\n}\n\nfact_recur <- function(n){\n  if(n == 0 || n == 1){\n  result = 1\n  } else {\n    n * fact_recur(n-1)\n  }\n}\n\nUsing the {microbenchmark} package we can benchmark the code:\n\nmicrobenchmark::microbenchmark(\n  fact_recur(50), \n  fact_iter(50)\n)\n\nUnit: microseconds\n           expr    min     lq     mean median      uq    max neval\n fact_recur(50) 21.501 21.701 23.82701 21.901 22.0515 68.902   100\n  fact_iter(50)  2.000  2.101  2.74599  2.201  2.3510 21.000   100\nWe see that the recursive factorial function is 10 times slower then the iterative version. In this particular example it doesn’t make much of a difference, because the functions only take microseconds to run. But if you’re working with more complex functions, this is a problem. If you want to keep using the recursive function and not switch to an iterative algorithm, there are workarounds. The first is called trampolining. I won’t go into details, but if you’re interested, there is an R package that allows you to use trampolining with R, aptly called {trampoline}. Another solution is using the {memoise} package.\n\n\n4.2.5 Anonymous functions\nIt is possible to define a function and not give it a name. For example:\n\nfunction(x)(x+1)(10)\n\nSince R version 4.1, there is even a shorthand notation for anonymous functions:\n\n(\\(x)(x+1))(10)\n\nBecause we don’t name them, we cannot reuse them. So why is this useful? Anonymous functions are useful when you need to apply a function somewhere inside a pipe once, and don’t want to define a function just for this. This will become clearer once we learn about lists, but before that, let’s philosophize a bit.\n\n\n4.2.6 The Unix philosophy applied to R\n\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.\n\nDoug McIlroy, in A Quarter Century of Unix1\nWe can take inspiration from the Unix philosophy and rewrite it like this for our purposes:\nWrite functions that do one thing and do it well. Write functions that work together. Write functions that handle lists, because that is a universal interface.\nStrive for writing simple functions that only perform one task. Don’t hesitate to split a big function into smaller ones. Small functions that only perform one task are easier to maintain, test, document and debug. These smaller functions can then be chained using the |> operator. In other words, it is preferable to have something like:\na |> f() |> g() |> h()\nwhere a is for example a path to a data set, and where f(), g() and h() successively read, clean, and plot the data, than having something like:\nbig_function(a)\nthat does all the steps above in one go.\nThis idea of splitting the problem into smaller chunks, each chunk in turn split into even smaller units that can be handled by functions and then the results of these function combined into a final output is called composition.\nThe advantage of splitting big_function() into f(), g() and h() is that you can tackle big problems one bite at a time, and also reusing these smaller functions in other projects is much easier. So what’s important is that you can make small functions work together by sharing a common interface. The list is usually a good candidate for this."
  },
  {
    "objectID": "fprog.html#lists-a-powerful-data-structure",
    "href": "fprog.html#lists-a-powerful-data-structure",
    "title": "4  Functional programming",
    "section": "4.3 Lists: a powerful data-structure",
    "text": "4.3 Lists: a powerful data-structure\nLists are the second important ingredient of functional programming. In the R philosophy inspired from UNIX, I stated that lists are an universal interface in R, so our functions should handle lists. This of course depends on what it is your doing. If you need functions to handle numbers, then there’s little value in placing these numbers inside lists. But in practice, you will very likely manipulate objects that are more complex than numbers, and this is where lists come into play.\n\n4.3.1 Lists all the way down\nLists are extremely flexible, and most very complex objects classes that you manipulate are actually lists, but just fancier. For example, a data frame is a list:\n\ndata(mtcars)\n\ntypeof(mtcars)\n\n[1] \"list\"\n\n\nA fitted model is a list:\n\nmy_model <- lm(hp ~ mpg, data = mtcars)\n\ntypeof(my_model)\n\n[1] \"list\"\n\n\nA ggplot is a list:\n\nlibrary(ggplot2)\n\nmy_plot <- ggplot(data = mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\ntypeof(my_plot)\n\n[1] \"list\"\n\n\nIt’s lists all the way down, and it’s not a coincidence. It’s because, as stated, lists are very powerful. So it’s important to know what you can do with lists.\n\n\n4.3.2 Lists can hold many things\nIf you write a function that needs to return many objects, the only solution is to place them inside a list. For example, consider this function:\n\nsqrt_newton <- function(a, init = 1, eps = 0.01, steps = 1){\n    stopifnot(a >= 0)\n    while(abs(init**2 - a) > eps){\n        init <- 1/2 *(init + a/init)\n        steps <- steps + 1\n    }\n    list(\n      \"result\" = init,\n      \"steps\" = steps\n    )\n}\n\nThis function returns the square root of a number using Newton’s algorithm, as well as the number of steps, or iterations, it took to reach the solution:\n\nresult_list <- sqrt_newton(1600)\n\nresult_list\n\n$result\n[1] 40\n\n$steps\n[1] 10\n\n\nIt is quite common to instead print the number of steps to the console instead of returning them. But the issue with a function that prints something to the console instead of returning it, is that such a function is not pure, as it changes something outside of its scope. It is preferable to instead make the function pure by returning everything inside a neat list. It is then possible to separately save these objects if needed:\n\nresult <- result_list$result\n\nresult_steps <- result_list$steps\n\nOr you could define functions that know how to deal with the list:\n\nf <- function(result_list){\n  list(\n    \"result\" = result_list$result * 10,\n    \"steps\" = result_list$steps + 1\n    )\n}\n\nf(result_list)\n\n$result\n[1] 400\n\n$steps\n[1] 11\n\n\nIt all depends on what you want to do. But it is usually better to keep everything neatly inside a list.\nLists can also hold objects of different types:\n\nlist(\n  \"a\" = head(mtcars),\n  \"b\" = ~lm(y ~ x)\n  )\n\n$a\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n$b\n~lm(y ~ x)\n\n\nThe list above has two elements, the first is the head of the mtcars data frame, the second is a formula object. Lists can even hold other lists:\n\nlist(\n  \"a\" = head(mtcars),\n  \"b\" = list(\n    \"c\" = sqrt,\n    \"d\" = my_plot # Remember this ggplot object from before?\n    )\n  )\n\n$a\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n$b\n$b$c\nfunction (x)  .Primitive(\"sqrt\")\n\n$b$d\n\n\n\n\n\nUse this to your advantage.\n\n\n4.3.3 Lists as the cure to loops\nLoops are incredibly useful, and you are likely familiar with them. The problem with loops is that they are a concept from iterative programming, not functional programming, and this is a problem because loops rely on changing the state of your program to function. For example, let’s suppose that you wish to use a for-loop to compute the sum of the first 100 integers:\n\nresult <- 0\nfor (i in 1:100){\n  result <- result + i\n}\n\nprint(result)\n\n[1] 5050\n\n\nIf you run ls() now, you should see that there’s a variable i in your global environment. This could cause issues further down in your pipeline if you need to re-use i. Also, writing loops is, in my opinion, quite error prone. But how can we avoid using loops? For looping in a functional programming language, we need to use higher-order functions and lists. A reminder: a higher-order function is a function that takes another function as an argument. Looping is a task like any other, so we can write a function that does the looping for us. We will call it looping(), which will take a function as an argument, as well as list. The list will serve as the container to hold our numbers:\n\nlooping <- function(a_list, a_func, init = NULL, ...){\n  \n  # If the user does not provide an `init` value, set the head of the list as the initial value\n  if(is.null(init)){\n    init <- a_list[[1]]\n    a_list <- tail(a_list, -1)\n  }\n \n  # Separate the head from the tail of the list and apply the function to the initial value and the head of the list\n  head_list = a_list[[1]]\n  tail_list = tail(a_list, -1)\n  init = a_func(init, head_list, ...)\n\n  # Check if we're done: if there is still some tail, rerun the whole thing until there's no tail left\n  if(length(tail_list) != 0){\n    looping(tail_list, a_func, init, ...)\n  }\n  else {\n    init\n  }\n}\n\nNow, this might seem much more complicated than a for loop. However, now that we have abstracted the loop away inside a function, we can keep reusing this function:\n\nlooping(as.list(seq(1:100)), `+`)\n\n[1] 5050\n\n\nOf course, because this is so useful, looping() actually ships with R, and is called Reduce():\n\nReduce(`+`, seq(1:100)) # the order of the arguments is `function` then `list` for `Reduce()`\n\n[1] 5050\n\n\nBut this is not the only way that we can loop. We can also write a loop that applies a function to each element of a list, instead of operating on the whole list:\n\nresult <- as.list(seq(1:5))\nfor (i in seq_along(result)){\n  result[[i]] <- sqrt(result[[i]])\n}\n\nprint(result)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 2.236068\n\n\nHere again, we have to pollute the global environment by first creating a vessel for our results, and then apply the function at each index. We can abstract this process away in a function:\n\napplying <- function(a_list, a_func, ...){\n\n  head_list = a_list[[1]]\n  tail_list = tail(a_list, -1)\n  result = a_func(head_list, ...)\n\n  # Check if we're done: if there is still some tail, rerun the whole thing until there's no tail left\n  if(length(tail_list) != 0){\n    append(result, applying(tail_list, a_func, ...))\n  }\n  else {\n    result\n  }\n}\n\nOnce again this might seem complicated, and I would agree. Abstraction is complex. But once we have it, we can focus on the task at hand, instead of having to always tell the computer what we want:\n\napplying(as.list(seq(1:5)), sqrt)\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nOf course, R ships with its own, much more efficient, implementation of this function:\n\nlapply(list(seq(1:5)), sqrt)\n\n[[1]]\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nIn other programming languages, lapply() is often called map(). The {purrr} packages ships with other such useful higher-order functions that abstract loops away. For example, there’s the function called map2(), that maps a function of two arguments to each element of two atomic vectors or lists, two at a time:\n\nlibrary(purrr)\n\nmap2(\n  .x = seq(1:5),\n  .y = seq(1:5),\n  .f = `+`\n  )\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 10\n\n\nIf you have more than two lists, you can use pmap() instead.\n\n\n4.3.4 Data frames\nAs mentioned in the introduction of this section, data frames are a special type of list of atomic vectors. This means that just as I can use lapply() to compute the square root of the elements of an atomic vector, as in the previous example, I can also operate on all the columns of a data frame. For example, it is possible to determine the class of every variable like this:\n\nlapply(iris, class)\n\n$Sepal.Length\n[1] \"numeric\"\n\n$Sepal.Width\n[1] \"numeric\"\n\n$Petal.Length\n[1] \"numeric\"\n\n$Petal.Width\n[1] \"numeric\"\n\n$Species\n[1] \"factor\"\n\n\nUnlike a list however, the elements of a data frame must be of the same length. Data frames remain very flexible however, and using what we have learned until now, it is possible to use the data frame as a structure for all our computations. For example, suppose that we have a data frame that contains data on unemployment for the different subnational divisions of the Grand-Duchy of Luxembourg, the country the author of this book hails from. Let’s suppose that I want to generate several plots, per subnational division and per year. Typically, we would use a loop for this, but we can use what we’ve learned here, as well as some functions from the {dplyr}, {purrr}, {ggplot2} and {tidyr} packages. I will be downloading data that I made available inside a package, but instead of installing the package, we will download the .rda file (which is the file format of packaged data) and then load that data into our R session:\n\n# Create a temporary file\nunemp_path <- tempfile(fileext = \".rda\")\n\n# Download the data and save it to the path of the temporary file\ndownload.file(\"https://github.com/b-rodrigues/myPackage/raw/main/data/unemp.rda\", destfile = unemp_path)\n\n# Load the data. The data is now available as 'unemp'\nload(unemp_path)\n\nLet’s load the required packages and take a look at the data:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\nglimpse(unemp)\n\nRows: 472\nColumns: 9\n$ year                         <dbl> 2013, 2013, 2013, 2013, 2013, 2013, 2013,…\n$ place_name                   <chr> \"Luxembourg\", \"Capellen\", \"Dippach\", \"Gar…\n$ level                        <chr> \"Country\", \"Canton\", \"Commune\", \"Commune\"…\n$ total_employed_population    <dbl> 223407, 17802, 1703, 844, 1431, 4094, 214…\n$ of_which_wage_earners        <dbl> 203535, 15993, 1535, 750, 1315, 3800, 187…\n$ of_which_non_wage_earners    <dbl> 19872, 1809, 168, 94, 116, 294, 272, 113,…\n$ unemployed                   <dbl> 19287, 1071, 114, 25, 74, 261, 98, 45, 66…\n$ active_population            <dbl> 242694, 18873, 1817, 869, 1505, 4355, 224…\n$ unemployment_rate_in_percent <dbl> 7.947044, 5.674773, 6.274078, 2.876870, 4…\n\n\nColumn names are self-descriptive, but the level column needs some explanations. The country of Luxembourg is divided into Cantons, and these Cantons themselves into Communes.\nYou should know that the word Luxembourg can refer to the country, the canton or the commune of Luxembourg. Now let’s suppose that I want a separate plot for the three communes of Luxembourg, Esch-sur-Alzette and Wiltz. Instead of creating three separate data frames and feeding them to the same ggplot code, I can instead take advantage of the fact that data frames are lists, and are thus quite flexible. Let’s start with filtering:\n\nfiltered_unemp <- unemp %>%\n  filter(\n    level == \"Commune\",\n    place_name %in% c(\"Luxembourg\", \"Esch-sur-Alzette\", \"Wiltz\")\n   )\n\nglimpse(filtered_unemp)\n\nRows: 12\nColumns: 9\n$ year                         <dbl> 2013, 2013, 2013, 2014, 2014, 2014, 2015,…\n$ place_name                   <chr> \"Esch-sur-Alzette\", \"Luxembourg\", \"Wiltz\"…\n$ level                        <chr> \"Commune\", \"Commune\", \"Commune\", \"Commune…\n$ total_employed_population    <dbl> 12725, 39513, 2344, 13155, 40768, 2377, 1…\n$ of_which_wage_earners        <dbl> 12031, 35531, 2149, 12452, 36661, 2192, 1…\n$ of_which_non_wage_earners    <dbl> 694, 3982, 195, 703, 4107, 185, 710, 4140…\n$ unemployed                   <dbl> 2054, 3855, 318, 1997, 3836, 315, 2031, 3…\n$ active_population            <dbl> 14779, 43368, 2662, 15152, 44604, 2692, 1…\n$ unemployment_rate_in_percent <dbl> 13.898099, 8.889043, 11.945905, 13.179778…\n\n\nWe are now going to use the fact that data frames are lists, and that lists can hold any type of object. For example, remember this list from before where one of the elements is a data frame, and the second one a formula:\n\nlist(\n  \"a\" = head(mtcars),\n  \"b\" = ~lm(y ~ x)\n  )\n\n$a\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n$b\n~lm(y ~ x)\n\n\n{dplyr} comes with a function called group_nest() which groups the data frame by a variable (such that the next computations will be performed group-wise) and then nests the other columns into a smaller data frame. Let’s try it and see what happens:\n\nnested_unemp <- filtered_unemp %>%\n  group_nest(place_name) \n\nLet’s see how this looks like:\n\nnested_unemp\n\n# A tibble: 3 × 2\n  place_name                     data\n  <chr>            <list<tibble[,8]>>\n1 Esch-sur-Alzette            [4 × 8]\n2 Luxembourg                  [4 × 8]\n3 Wiltz                       [4 × 8]\n\n\nnested_unemp is a new data frame of 3 rows, one per commune (“Esch-sur-Alzette”, “Luxembourg”, “Wiltz”), and of two columns, one for the names of the communes, and the other contains every other variable inside a smaller data frame. So this is a data frame that has one column where each element of that column is itself a data frame. Such a column is called a list-column. This is essentially a list of lists.\nLet’s now think about this for a moment. If the column titled data is a list of data frames, it should be possible to use a function like map() or lapply() to apply a function on each of these data frames. Remember that map() or lapply() require a list of elements of whatever type and a function that accepts objects of this type as input. So this means that we could apply a function that plots the data to each element of the column titled data. Since each element of this column is a data frame, this functions needs a data frame as an input. As a first, simple, example to illustrate this, let’s suppose that we want to determine the number of rows of each data frame. This is how we would do it:\n\nnested_unemp %>%\n  mutate(nrows = map(data, nrow))\n\n# A tibble: 3 × 3\n  place_name                     data nrows    \n  <chr>            <list<tibble[,8]>> <list>   \n1 Esch-sur-Alzette            [4 × 8] <int [1]>\n2 Luxembourg                  [4 × 8] <int [1]>\n3 Wiltz                       [4 × 8] <int [1]>\n\n\nThe new column, titled nrows is a list of integers. We can simplify it by converting it directly to an atomic vector of integers by using map_int() instead of map():\n\nnested_unemp %>%\n  mutate(nrows = map_int(data, nrow))\n\n# A tibble: 3 × 3\n  place_name                     data nrows\n  <chr>            <list<tibble[,8]>> <int>\n1 Esch-sur-Alzette            [4 × 8]     4\n2 Luxembourg                  [4 × 8]     4\n3 Wiltz                       [4 × 8]     4\n\n\nLet’s try for a more complex example now. What if we want to filter rows? (The simplest way would of course to filter the rows we need before nesting the data frame). We need to apply the function filter() where its first argument is a data frame and the second argument is a predicate:\n\nnested_unemp %>%\n  mutate(nrows = map(data, \\(x)filter(x, year == 2015)))\n\n# A tibble: 3 × 3\n  place_name                     data nrows           \n  <chr>            <list<tibble[,8]>> <list>          \n1 Esch-sur-Alzette            [4 × 8] <tibble [1 × 8]>\n2 Luxembourg                  [4 × 8] <tibble [1 × 8]>\n3 Wiltz                       [4 × 8] <tibble [1 × 8]>\n\n\nIn this case here, we need to use an anonymous function. This is because filter() has two arguments, and we need to make clear what it is we are mapping over and what argument stays fixed; we are mapping over, or iterating if you will, data frames, but the predicate stays fixed.\nWe are now ready to plot our data. The best way to continue is to first get the function right by creating one plot for one single commune. Let’s select the dataset for the commune of Luxembourg:\n\nlux_data <- nested_unemp %>%\n  filter(place_name == \"Luxembourg\") %>%\n  unnest(data)\n\nTo plot this data, we can now write the required ggplot2() code:\n\nggplot(data = lux_data) +\n  theme_minimal() +\n  geom_line(\n    aes(year, unemployment_rate_in_percent, group = 1)\n   ) +\n  labs(title = \"Unemployment in Luxembourg\")\n\n\n\n\nTo turn the lines of code above into a function, you need to think about how many arguments that function would have. There is an obvious one, the data itself (in the snippet above, the data is the lux_data object). Another one that is less obvious is in the title:\n\n  labs(title = \"Unemployment in Luxembourg\")\n\n$title\n[1] \"Unemployment in Luxembourg\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nIdeally, we would want that title to change, depending on the data set. So we could write the function like so:\n\nmake_plot <- function(x, y){\n  ggplot(data = x) +\n    theme_minimal() +\n    geom_line(\n      aes(year, unemployment_rate_in_percent, group = 1)\n      ) +\n    labs(title = paste(\"Unemployment in\", y))\n}\n\nLet’s try it on our data:\n\nmake_plot(lux_data, \"Luxembourg\")\n\n\n\n\nOk, so now, we simply need to apply this function to our nested data frame:\n\nnested_unemp <- nested_unemp %>%\n  mutate(plots = map2(\n    .x = data,\n    .y = place_name,\n    .f = make_plot\n  ))\n\nnested_unemp\n\n# A tibble: 3 × 3\n  place_name                     data plots \n  <chr>            <list<tibble[,8]>> <list>\n1 Esch-sur-Alzette            [4 × 8] <gg>  \n2 Luxembourg                  [4 × 8] <gg>  \n3 Wiltz                       [4 × 8] <gg>  \n\n\nIf you look at the plots column, you see that it is a list of gg objects: these are our plots. Let’s take a look at them:\n\nnested_unemp$plots\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\nWe could also have used an anonymous function:\n\nnested_unemp %>%\n  mutate(plots2 = map2(\n    .x = data,\n    .y = place_name,\n    .f = \\(.x,.y)(\n                ggplot(data = .x) +\n                  theme_minimal() +\n                  geom_line(\n                    aes(year, unemployment_rate_in_percent, group = 1)\n                   ) +\n                  labs(title = paste(\"Unemployment in\", .y))\n                  )\n           )\n         ) %>%\n  pull(plots2)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\nThis column-list based workflow is extremely powerful and I highly advise you to take the required time to master it."
  },
  {
    "objectID": "fprog.html#functional-programming-in-r",
    "href": "fprog.html#functional-programming-in-r",
    "title": "4  Functional programming",
    "section": "4.4 Functional programming in R",
    "text": "4.4 Functional programming in R\nUp until now we focused more on concepts than on specifics of the R programming language when it comes to functional programming. In the section, we will be focusing entirely on R-specific capabilities and packages for functional programming.\n\n4.4.1 Base capabilities\nR is a functional programming language, as already stated, and as such it comes with many functions out of the box to write functional code. We have already discussed lapply() and Reduce(). You should know that depending on what you want to achieve, there are other functions that are similar to lapply(): apply(), sapply(), vapply(), mapply() and tapply(). There’s also Map() which is a wrapper around mapply(). Each function performs the same basic task of applying a function over all the elements of a list or list-like structure, but it can be hard to keep them apart. This is why {purrr}, which we will discuss in the next section, is quite an interesting alternative to base R’s offering.\nAnother one of the quintessential functional programming functions (alongside Reduce() and Map()) that ships with R is Filter(). If you know dplyr::filter() you should be familiar with the concept of filtering rows of a data frame where the elements of one particular column satisfy a predicate. Filter() works the same way, but focusing on lists instead of data frame:\n\nFilter(is.character,\n       list(\n         seq(1:5),\n         \"Hey\")\n       )\n\n[[1]]\n[1] \"Hey\"\n\n\nThe call above only returns the elements where is.character() evaluates to TRUE.\nAnother useful function is Negate() which is a function factory that takes a boolean function as an input and returns the opposite boolean function. As an illustration, suppose that in the example above we wanted to get everything but the character:\n\nFilter(Negate(is.character),\n       list(\n         seq(1:5),\n         \"Hey\")\n       )\n\n[[1]]\n[1] 1 2 3 4 5\n\n\nThere are some other functions like this that you might want to check out: type ?Negate in console to read more about them.\nBefore continuing with R packages that extend R’s functional programming capabilities it’s also important to stress that just as R is a functional programming language, it is also an object oriented language. In fact, R is what John Chambers called a functional OOP language (Chambers (2014)). We won’t delve too much into what this means (read Wickham (2019) for this), but as a short discussion, consider the print() function. Depending on what type of object the user gives it, it seems as if somehow print() knows what to do with it:\n\nprint(5)\n\n[1] 5\n\nprint(head(mtcars))\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nprint(str(mtcars))\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\nNULL\n\n\nThe way this works, is essentially a mixture of functional and object oriented programming, so functional OOP. Let’s take a closer look at the source code of print() by simply typing print without brackets, into a console:\n\nprint\n\nfunction (x, ...) \nUseMethod(\"print\")\n<bytecode: 0x563b134f5db8>\n<environment: namespace:base>\n\n\nQuite unexpectedly, the source code of print() is one line long and is just UseMethod(\"print\"). So all print() does is use a generic method called “print”. If your text editor has autocompletion enabled, you might see that there are actually quite a lot of print() functions. For example, type print.data.frame into a console:\n\nprint.data.frame\n\nfunction (x, ..., digits = NULL, quote = FALSE, right = TRUE, \n    row.names = TRUE, max = NULL) \n{\n    n <- length(row.names(x))\n    if (length(x) == 0L) {\n        cat(sprintf(ngettext(n, \"data frame with 0 columns and %d row\", \n            \"data frame with 0 columns and %d rows\"), n), \"\\n\", \n            sep = \"\")\n    }\n    else if (n == 0L) {\n        print.default(names(x), quote = FALSE)\n        cat(gettext(\"<0 rows> (or 0-length row.names)\\n\"))\n    }\n    else {\n        if (is.null(max)) \n            max <- getOption(\"max.print\", 99999L)\n        if (!is.finite(max)) \n            stop(\"invalid 'max' / getOption(\\\"max.print\\\"): \", \n                max)\n        omit <- (n0 <- max%/%length(x)) < n\n        m <- as.matrix(format.data.frame(if (omit) \n            x[seq_len(n0), , drop = FALSE]\n        else x, digits = digits, na.encode = FALSE))\n        if (!isTRUE(row.names)) \n            dimnames(m)[[1L]] <- if (isFALSE(row.names)) \n                rep.int(\"\", if (omit) \n                  n0\n                else n)\n            else row.names\n        print(m, ..., quote = quote, right = right, max = max)\n        if (omit) \n            cat(\" [ reached 'max' / getOption(\\\"max.print\\\") -- omitted\", \n                n - n0, \"rows ]\\n\")\n    }\n    invisible(x)\n}\n<bytecode: 0x563b146100d0>\n<environment: namespace:base>\n\n\nThis is the print function for data.frame objects. So what print() does is look at the class of its argument x, and then look for the right print function. In more traditional OOP languages, users would type something like:\n\nmtcars.print()\n\nIn these languages, objects encapsulate method (the equivalent of our functions), so if mtcars is a data frame, it encapsulates a print() method that then does the printing. R is different, because classes and methods are kept separate. If a package developer creates a new class of object, then the developer also must implement the required methods. For example in the {chronicler} package, the chronicler class is defined and a print.chronicler() function is defined to print these objects.\nAll of this to say that if you want to extend R by writing packages, learning some OOP essentials is also important. But for data analysis, functional programming does the job perfectly. To learn more about R’s different OOP systems (yes, R can do OOP in different ways and the one I sketched here is the simplest, but probably the most used as well, one), take a look at Wickham (2019).\n\n\n4.4.2 purrr\nThe {purrr} package, developed by Posit, contains many functions to make functional programming with R more smooth. In the previous section, we discussed the apply() family of function; they all do a very similar thing, which is looping over a list and applying a function to the elements of the list, but it is not quite easy to remember which one does what. Also, for some of these functions like apply(), the list comes first, and then the function, but in the case of mapply(), the function comes first. Another issue with these functions is that it is not always easy to know what type the output is going to be. List? Atomic vector? Something else?\n{purrr} solves this issue by offering the map() family of functions, which behave in a very consistent way. The basic function is called map() and we’ve already used it:\n\nmap(seq(1:5), sqrt)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.414214\n\n[[3]]\n[1] 1.732051\n\n[[4]]\n[1] 2\n\n[[5]]\n[1] 2.236068\n\n\nBut there are many interesting variants:\n\nmap_dbl(seq(1:5), sqrt)\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\nmap_dbl() coerces the output to an atomic vector of doubles instead of a list of doubles. Then there’s:\n\nmap_chr(letters, toupper)\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\nfor when the output needs to be an atomic vector of characters.\nThere are many others, so take a look at the document with ?map. There’s also walk() which is used if you’re only interested in the side-effect of the function (for example if the function takes paths as input and saves something to disk).\n{purrr} also has functions to replace Reduce(), simply called reduce() and accumulate(), and there are many, many other useful functions. Read through the documentation of the package and take the time to learn about all it has to offer.\n\n\n4.4.3 withr\n{withr} is a powerful package that makes it easy to “purify” functions that behave in a way that can cause problems. Remember the function from the introduction that randomly gave out a recipe Bruno liked? Here it is again:\n\nh <- function(name, food_list = list()){\n\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  food_list <- append(food_list, food)\n\n  print(paste0(name, \" likes \", food))\n\n  food_list\n}\n\nBecause this function returns results that are not consistent for a fixed input, this function is not referentially transparent. So we improved the function like this:\n\nh2 <- function(name, food_list = list(), seed = 123){\n\n  # We set the seed, making sure that we get the same selection of food for a given seed\n  set.seed(seed)\n  food <- sample(c(\"lasagna\", \"cassoulet\", \"feijoada\"), 1)\n\n  # We now need to unset the seed, because if we don't, guess what, the seed will stay set for the whole session!\n  set.seed(NULL)\n\n  food_list <- append(food_list, food)\n\n  print(paste0(name, \" likes \", food))\n\n  food_list\n}\n\nThe problem with this approach is that we need to modify our function. We can instead use withr::with_seed() to achieve the same effect:\n\nwithr::with_seed(seed = 123,\n                 h(\"Bruno\"))\n\n[1] \"Bruno likes feijoada\"\n\n\n[[1]]\n[1] \"feijoada\"\n\n\nIt is also easier to create a wrapper if needed:\n\nh3 <- function(..., seed){\n  withr::with_seed(seed = seed,\n                   h(...))\n}\n\n\nh3(\"Bruno\", seed = 123)\n\n[1] \"Bruno likes feijoada\"\n\n\n[[1]]\n[1] \"feijoada\"\n\n\nBefore we downloaded a dataset and loaded it into memory; we did so by first created a temporary file, then downloading it and then loading it. Suppose that instead of loading this data into our session, we simply wanted to test whether the link was still working. We wouldn’t want to keep the loaded data in our session, so to avoid having to delete it again manually, we could use with_tempfile():\n\nwithr::with_tempfile(\"unemp\", {\n  download.file(\"https://github.com/b-rodrigues/myPackage/raw/main/data/unemp.rda\", destfile = unemp)\n  load(unemp)\n  nrow(unemp)\n  }\n)\n\nWarning in unlink(mget(new, envir = env), recursive = TRUE): expanded path length 35189 would be too long for\nlist(year = c(2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, \n2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,  [... truncated]\n\n\n[1] 472\n\n\nThe data got downloaded, and then loaded, and then we computed the number of rows of the data, without touching the global environment, or state, of our current session.\nJust like for {purrr}, {withr} has many useful functions which I encourage you to familiarize yourself with."
  },
  {
    "objectID": "fprog.html#conclusion",
    "href": "fprog.html#conclusion",
    "title": "4  Functional programming",
    "section": "4.5 Conclusion",
    "text": "4.5 Conclusion\nIf there is only one thing that you should remember from this chapter, it would be pure functions. This is in my opinion not very difficult to do and comes with many benefits. But, avoiding loops and replacing them with higher-order functions (lapply(), Reduce(), purrr::map() – and its variants –) also pays off. While this chapter stresses the advantages of functional programming, you should not forget that R is not a pure, and solely, functional programming language and that other paradigms, like object oriented programming, are also available to you, and if your goal is to master the language (instead of “just” using it to solve data analysis problems), then you also need to know about R’s OOP capabilities.\n\n\n\n\nChambers, John M. 2014. “Object-Oriented Programming, Functional Programming and R.” Statistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452.\n\n\nWickham, Hadley. 2019. Advanced r. CRC press."
  },
  {
    "objectID": "testing.html#assertive-programming",
    "href": "testing.html#assertive-programming",
    "title": "5  Testing your code",
    "section": "5.1 Assertive programming",
    "text": "5.1 Assertive programming\nThe analysis is still in Quarto, so how could the readers of this book test their code? Copying here what Miles wrote on the subject:\n‘Assertive programming’ is a topic that might be missing from the book. I think of it as a kind of dual of unit testing. Unit testing is for more generally applicable packaged code. But when you have functions in your analysis pipeline that operate on a very specific kind of input data, unit testing becomes kind of nonsensical because you’re left to dream up endless variations of your input dataset that may never occur. It’s a bit easier to flip the effort to validating the assumptions you have about your input and output data, which you can do in the pipeline functions themselves rather than separate unit testing ones. This is nice because it ensures the validation is performed in the pipeline run, and so is backed by the same reproducibility guarantees.\nI think at the end of the chapter we should hint at unit testing, but leave it as a subsection of the next chapter that deals with packaging code."
  },
  {
    "objectID": "keep_dry.html#literate-programming",
    "href": "keep_dry.html#literate-programming",
    "title": "6  Keeping it DRY with literate programming",
    "section": "6.1 Literate programming",
    "text": "6.1 Literate programming\nIn literate programming, we mix code and prose together, in a way that makes it easy for both non-technical users and programmers to understand what is going on. Scripts written this way are also very easy to compile, or render, into a variety of document formats like html and docx. R supports several ways of doing literate programming: Sweave, knitr and Quarto.\nSweave was the first tool available to R (and S) users, and allowed the mixing of R and LaTeX code to create a document. Friedrich Leisch developed Sweave in 2002 and described it in his 2002 paper (Leisch (2002)). As Leisch argues, the traditional way of writing a report as part of a statistical data analysis project uses two separate steps: running the analysis using some software, and then copy and pasting the results into a word processing tool. The problem with this approach is that much time is wasted copy and pasting things, so experimenting with different layouts or data analysis techniques is very time consuming. Copy and paste mistakes will also happen (it’s not a question of if, but when) and updating reports (for example, when new data comes in) means that someone will have, again, to copy and paste the updated results into a new document.\nSweave provided (and still provides, as it is still well functioning!) a way to embed the analysis in the document itself, in this case a LaTeX source file, and R code was executed whenever the document was compiled. This gave researchers considerable time savings when it was time to update a report or drafting a research paper.\nThe snippet below shows the example from Leisch’s paper:\n\\documentclass[a4paper]{article}\n\n\\begin{document}\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a LaTeX document:\n\n<<>>=\ndata (airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n@\n\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n\\begin{center}\n<<fig=TRUE,echo=FALSE>>=\nboxplot(Ozone ~ Month, data = airquality)\n@\n\\end{center}\n\n\\end{document}\nEven if you’ve never seen a LaTeX source file, you should be able to figure out what’s going on. The first line states what type of document we’re writing. Then comes \\begin{document} which tells the compiler where the document starts. Then comes the content. You can see that it’s a mixture of plain English with R code defined inside chunks starting with <<>>= and ending with @. Finally, the documents ends with \\end{document}. Getting a human readable PDF from this source is a two-step process: first this source gets converted into a .tex file and then this .tex file into a PDF. Sweave is included with every R installation since version 1.5.0, and still works to this day. For example, we can test that our Sweave installation works just fine by compiling the example above. This is what the final output looks like:\n\n\n\nMore than 20 years later, the output is still the same\n\n\nLet us just state that the fact that it is still possible to compile this example more than 20 years later is an incredible testament to how mature and stable this software is (both R, Sweave, and LaTeX). But as impressive as this is, LaTeX has a steep learning curve, and Leisch even advocated the use of the Emacs text editor to edit Sweave files, which also has a very steep learning curve (but this is entirely optional; for example we edited and compiled the example on the Rstudio IDE).\nThe next generation of literate programming tools was provided by a package called {knitr} in 2012. From the perspective of the user, the biggest change from Sweave is that {knitr} is able to use many different formats as source files. The one that became very likely the most widely used format is a flavour of the Markdown markup language, R Markdown (Rmd). But {knitr} can also run code chunks for other languages, such as Python, Perl, Awk, Haskell, bash and more (Xie (2014)). Since version 1.18, {knitr} uses the {reticulate} package to provide a Python engine for the Rmd format. To illustrate the Rmd format, let’s rewrite the example from Leisch’s Sweave paper into it:\n---\noutput: pdf_document\n---\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a LaTeX document:\n\n```{r}\ndata (airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n```\n\n\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n```{r, echo = FALSE}\nboxplot(Ozone ~ Month, data = airquality)\n```\nThis is what the output looks like:\n\n\n\nIt’s very close to the Sweave output\n\n\nJust like in a Sweave document, an Rmd source file also has a header in which authors can define a number of options. Here we only specified that we wanted a pdf document as an output file. We then copy and pasted the contents from the Sweave source, but changed the chunk delimiters from <<>>= and @ to ```{r} to start an R chunk and ``` to end it. Remember; we need to specify the engine in the chunk because {knitr} supports many engines. For example, it is possible to run a bash command by adding this chunk to the source:\n---\noutput: pdf_document\n---\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a LaTeX document:\n\n```{r}\ndata (airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n```\n\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n```{r, echo = FALSE}\nboxplot(Ozone ~ Month, data = airquality)\n```\n\n```{bash}\npwd\n```\n(bash’s pwd command shows the current working directory). You may have noticed that we also keep two LaTeX commands in the source Rmd, \\texttt{} and LaTeX. This is because Rmd files get first converted into LaTeX files and then into a PDF. If you’re using RStudio, this document can be compiled by clicking a button or using a keyboard shortcut, but you can also use the rmarkdown::render() function. This function does two things transparently: it first converts the Rmd file into a source LaTeX file, and then converts it into a PDF. It is of course possible to convert the document to a Word document as well however, LaTeX commands will be ignored. Html is another widely used output format.\nIf you’re a researcher and prefer working with LaTeX directly instead of having to switch to Markdown, you can either use Sweave, or use {knitr} but instead of writing your documents using the R Markdown format, you can use the Rnw format which is basically the same as Sweave, but uses {knitr} for compilation. Take a look at this example from the {knitr} github repository for example.\nYou should know that {knitr} makes it possible to author many, many different types of documents. It is possible to write books, blogs, package documentation (and even entire packages, as we shall see later in this book), Powerpoint slides… It is extremely powerful because we can use the same general R Markdown knowledge to build many different outputs:\n\n\n\nOne format to rule them all\n\n\nFinally, the latest in literate programming for R is a new tool developed by Posit, called Quarto. If you’re an R user and already know {knitr} and the Rmd format, you should be able to immediately use Quarto. So what’s the difference? In practice and for R users not much. There are many things that Quarto is able to do out of the box that you can’t, without extensions, do with {knitr}. Quarto has some nice defaults; in fact this book is written in Quarto instead of {knitr} because the default Quarto output looks nicer than the default {knitr} output. However, there may even be things that Quarto can’t do at all (at least for now) when compared to {knitr}. So why bother switching? Well, Quarto provides sane defaults and some nice features out of the box, and the cost of switching from the Rmd format to Quarto’s Qmd format is basically 0. Also, and this is probably the biggest reason to use Quarto, is that Quarto is not tied to R. Quarto is actually a standalone tool that needs to be installed alongside your R installation, and works completely independently. In fact, you can use Quarto without having R installed at all, as Quarto, just like {knitr} supports many engines. This means that if you’re primarily using Python, you can author documents with Quarto. Quarto also supports the Julia programming language and Observable JS, making it possible to include interactive visualisations into an Html document. Let’s take a look at how the example from Leisch’s paper looks as a Qmd file:\n---\noutput: pdf\n---\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a LaTeX document:\n\n```{r}\ndata (airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n```\n\n\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n```{r, echo = FALSE}\nboxplot(Ozone ~ Month, data = airquality)\n```\n(I’ve omitted the bash chunk from before, not because Quarto does not support it, but to keep close to the original example from the paper.)\nAs you can see, it’s exactly the same as the Rmd file from before. The only difference is in the header. In the Rmd file we specified the output format as:\n\n---\noutput: pdf_document\n---\n\nwhereas in the Qmd file we changed it to:\n\n---\noutput: pdf\n---\n\nWhile Quarto is the latest option in literate programming, it is quite recent, and as such, we feel it might be better to stick with {knitr} and the Rmd format for now, so that’s what we’re going to use going forward. Also, the {knitr} and the Rmd format are here to stay, so there’s little risk in keeping using it, and anyways, as already stated, if switching to Quarto becomes a necessity, the cost of switch would be very, very low. In what follows, we won’t be focused on anything really {knitr} or Rmd specific, so should you want to use Quarto instead, you should be able to follow along without any problems at all, since the Rmd and Qmd formats have so much overlap (but whenever there would be major differences between the two formats, we will highlight them).\nIn the next two sections, we will discuss how to set up and use {knitr} as well as give you a quick overview of the R Markdown syntax. However, we will very quickly focus on the templating capabilities of {knitr}: expanding text, using child documents, and parameterised reports. These are advanced topics and not easy to tackle if you’re not comfortable with R already. Just as functions and higher-order functions like lapply() avoid having to repeat yourself, so does templating, but for literate programming. The goal is to write functions that returns literal R Markdown code, so that you can loop over these functions to build entire sections of your documents. However, difficult these features are to master, they are also extremely powerful, as they help you focus on what really matters."
  },
  {
    "objectID": "keep_dry.html#knitr-basics",
    "href": "keep_dry.html#knitr-basics",
    "title": "6  Keeping it DRY with literate programming",
    "section": "6.2 {knitr} basics",
    "text": "6.2 {knitr} basics\nThis section will be a very small intro to {knitr}. We are going to teach you just enough to get started, and we are going to focus on the Rmd format. There are many resources out there that you can use if you want to dig deeper, for instance the R Markdown website from Posit, or the R Markdown: The Definitive Guide and R Markdown Cookbook eBooks. We will also not assume that you are using the RStudio IDE and give you instead the lower level commands to render documents. If you use RStudio and want to know how to use it effectively to author Rmd documents, you should take a look at Quick Tour page. In fact, this section will basically focus on the same topics, but without focusing on how to use RStudio.\n\n6.2.1 Set up\nThe first step is to install the {knitr} and the {rmarkdown} packages. Simple type:\n\ninstall.packages(\"rmarkdown\")\n\nin an R console. Since {knitr} is required to install {rmarkdown}, it gets installed automatically. If you want to compile PDF documents, you should also have a working LaTeX distribution. You can skip this next part if you’re only interested in generating PDF and Word files. For what follows, we will only be rendering Html documents, so no need to install LaTeX (by the way, you do not need a working Word installation to compile documents to the docx format). However, if you already have a working LaTeX installation, you shouldn’t have to do anything else to generate PDF documents. If you don’t have a working LaTeX distribution, then Yihui Xie, the creator of {knitr} created an R package called {tinytex} that makes it extremely easy to install a LaTeX distribution. In fact, this is the way we recommend installing LaTeX even if you’re not an R user (it is possible to use the tinytex distribution without R; it’s just that the {tinytex} R package provides many functions that makes installing and maintaining it very easy). Simply run these commands in an R console to get started:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nand that’s it! If you need to install specific LaTeX packages, then refer to the Maintenance section on tinytex’s website. For example, to compile the example from Leisch’s article on Sweave discussed previously, we had to install the grfext LaTeX package (as explained by the error output in the console when we tried compiling). So, we simply needed to run the following command to get it:\n\ntlmgr_install(\"grfext\")\n\nAfter you’ve installed {knitr}, {rmarkdown} and, optionally, {tinytex}, simply try to compile the following document:\n---\noutput: html_document\n---\n\n# Document title\n\n## Section title\n\n### Subsection title\n\nThis is **bold** text. This is *text in italics*.\n\nMy favourite programming language for statistics is ~~SAS~~ R.\nsave this document into a file called rmd_intro.rmd using you’re favourite text editor. Then render it into an Html file by running the following command in the R console:\n\nrmarkdown::render(\"path/to/rmd_test.rmd\")\n\nThis should create a file called rmd_test.html; open it with your web browser and you should see the following:\n\n\n\nIt’s very close to the Sweave output\n\n\nCongratulations, you just knitted your first Rmd document!\n\n\n6.2.2 Markdown ultrabasics\nR Markdown is a flavour of Markdown, which means that you should know some Markdown to really take full advantage of R Markdown. The document from before should have already shown you some basics: titles, sections and subsections all start with a # and the depth level is determined by the number of #s. For bold text, simply put the words in between ** and for italics use only one *. If you want bold and italics, use ***. The original designer of Markdown did not think that underlining text was important, so there is no easy way of doing it unfortunately. For this, you need to use a somewhat hidden feature; without going into too much technical details, the program that converts Rmd files to the final output format is called Pandoc, and it’s possible to use some of Pandoc’s features to format text. For example, for underlining:\n[This is some underlined text in a R Markdown document]{.underline}\nThis will underline the text between square brackets.1\nThe next step is actually to mix code and prose. As you’ve seen from Leisch’s canonical example, this is quite easily achieved by using R code chunks. The R Markdown example below shows various code chunks alongside some options. For example, a code chunk that uses the echo = FALSE option will not appear (but the output of the computation will):\n---\ntitle: \"Document title\"\noutput: html_document\ndate: \"2023-01-28\"\n---\n\n# R code chunks\n\nThis below is an R code chunk:\n\n```{r}\ndata(mtcars)\n\nplot(mtcars)\n```\n\n\nThe code chunk above will appear in the final output. The code chunk below will be hidden:\n\n```{r, echo = FALSE}\ndata(iris)\n\nplot(iris)\n```\n\n\nThis next code chunk will not be evaluated:\n\n```{r, eval = FALSE}\ndata(titanic)\n\nstr(titanic)\n```\n\n\nThe last one runs, but code and output from the code is not shown in the final document.\nThis is useful for loading libraries and hiding startup messages:\n\n```{r, include = FALSE}\nlibrary(dplyr)\n```\nIf you use RStudio and create a new R Markdown file from the menu, a new R Markdown file is generated for you to fill out. The first R chunk is this one:\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\nThis is an R chunk named setup and with the option include = FALSE. Naming chunks is optional, but we are going to make use of this later on. What this chunk runs is one line of code that defines a global option to show all chunks by default (which is the default behaviour). You can change TRUE to FALSE if you want to hide every code chunk instead (if you’re using Quarto, global options are set differently).\nSomething else you might have noticed in the previous example, is that we’ve added some more content in the header:\n---\ntitle: \"Document title\"\noutput: html_document\ndate: \"2023-01-28\"\n---\nThere are several other options available that you can define in the header. Later on, when we’ll be building our project together, we will provide some more options (like having a table of contents).\nThe finish this part on code chunks, you should know about inline code chunks. Take a look at the following example:\n---\ntitle: \"Document title\"\noutput: html_document\ndate: \"2023-01-28\"\n---\n\n# R code chunks\n\n```{r, echo = FALSE}\ndata(iris)\n```\n\n\nThe iris dataset has `r nrow(iris)` rows.\nThe last sentence from this example has an inline code chunk. This quite useful, as it allows to parameterise sentences and paragraphs, and thus avoids needing to copy and paste (and we will go quite far into how to avoid copy and pasting, thanks to more advanced features we will shortly discuss).\nTo finish this crash course, you should know that to use footnotes you need to write the following:\nThis sentence has a footnote.[^1]\n\n[^1]: This is the footnote.\nand that you can write LaTeX formulas as well. For example, add the following into the the example from before and render either a PDF or a html document (don’t put the LaTeX formula below inside a chunk, simply paste it as if it were normal text. This doesn’t work for Word output because Word does not support LaTeX equations):\n\\begin{align*}\nS(\\omega) \n&= \\frac{\\alpha g^2}{\\omega^5} e^{[ -0.74\\bigl\\{\\frac{\\omega U_\\omega 19.5}{g}\\bigr\\}^{\\!-4}\\,]} \\\\\n&= \\frac{\\alpha g^2}{\\omega^5} \\exp\\Bigl[ -0.74\\Bigl\\{\\frac{\\omega U_\\omega 19.5}{g}\\Bigr\\}^{\\!-4}\\,\\Bigr] \n\\end{align*}"
  },
  {
    "objectID": "keep_dry.html#keeping-it-dry",
    "href": "keep_dry.html#keeping-it-dry",
    "title": "6  Keeping it DRY with literate programming",
    "section": "6.3 Keeping it DRY",
    "text": "6.3 Keeping it DRY\nWe’re now coming to why this chapter is titled as it is.\nWhatever you’re doing, you should keep it DRY - DRY stands for don’t repeat yourself. Repeating yourself by, say, copy and pasting, leads to errors and makes reading and understanding your code more difficult. This can be avoided by using functions, as we discussed in the previous chapter, but what if you need to write a document that has the following structure:\n\nA title\nA section\nA table inside this section\nAnother section\nAnother table inside this section\nYet another section\nYet another table inside this section\n\nIs there a way to automate the creation of such a document by taking advantage of the repeating structure? Of course there is. The question is not, is it possible to do X?, but how to do X?.\n\n6.3.1 Generating R Markdown code from code\nThe example below is a fully working minimal example of this. Copy it inside a document titled something like rmd_templating.Rmd and render it. You will see that the output contains more sections than defined in the source. This is because we use templating at the end. Take some time to read the document, as the text inside explains what is going on:\n---\ntitle: \"Templating\"\noutput: html_document\ndate: \"2023-01-27\"\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n\n## A function that creates tables\n\n```{r}\ncreate_table <- function(dataset, var){\n  table(dataset[var]) |>\n    knitr::kable()\n}\n```\n\n\nThe function above uses the `table()` function to create frequency tables, \nand then this gets passed to the `knitr::kable()` function that produces a \ngood looking table for our rendered document:\n\n```{r}\ncreate_table(mtcars, \"am\")\n```\n\n\nLet’ suppose that we want to generate a document that would look like this:\n\n- first a section title, with the name of the variable of interest\n- then the table\n\nSo it would look like this:\n\n## Frequency table for variable: \"am\"\n\n```{r}\ncreate_table(mtcars, \"am\")\n```\n\n\nWe don’t want to create these sections for every variable by hand.\n\nInstead, we can define a function that returns the R markdown code required\nto create this. This is this function:\n\n```{r}\nreturn_section <- function(dataset, var){\n  a <- knitr::knit_expand(text = c(\"## Frequency table for variable: {{variable}}\",   \n                                   create_table(dataset, var)),\n                          variable = var)\n  cat(a, sep = \"\\n\")\n}\n```\n\n\nThis new function, `return_section()` uses `knitr::knit_expand()` to generate \nR Markdown code. Words between `{{}}` get replaced by the provided `var` argument\nto the function. So when we call `return_section(\"am\")`, `{{variable}}` is replaced\nby `\"am\"`. `\"am\"` then gets passed down to `create_table()` and the frequency\ntable gets generated. We can now generate all the section by simply applying\nour function to a list of column names:\n\n```{r, results = \"asis\"}\ninvisible(lapply(colnames(mtcars), return_section, dataset = mtcars))\n```\nThe last function, named return_section() uses knitr::knit_expdand(), which is the one that does the heavy lifting. This function returns literal R Markdown code. It returns ## Frequency table for variable: {{variable}} which creates a level 2 section title with the text Frequency table for variable: xxx where the xxx will get replaced by the variable passed to return_section(). So calling return_section(mtcars, \"am\") will print the following in your console:\n## Frequency table for variable: am\n|am | Freq|\n|:--|----:|\n|0  |   19|\n|1  |   13|\nWe now simply need to find a clever way to apply this function to each variable in the mtcars dataset. For this, we are going to use lapply() which implements a for loop (you could use purrr::map() just as well for this):\n\ninvisible(lapply(colnames(mtcars),\n                 return_section,\n                 dataset = mtcars))\n\nThis will create, for each variable in mtcars, the same R Markdown code as above. Notice that the R Markdown chunk where the call to lapply() is has the option results = \"asis\". This is because the function returns literal Markdown code, and we don’t want the parser to have to parse it again. We tell the parser “don’t worry about this bit of code, it’s already good”. As you see, the call to lapply() is wrapped inside invisible(). This is because return_section() does not return anything, it just prints something to the console. No object is returned. So if you don’t wrap the call to lapply() inside invisible(), then a bunch of NULLs will also get printed. To avoid this, use invisible() (and use purrr::walk() rather than purrr::map()).\nClick here to see the output.\nThis is not an easy topic, so take the time to play around with the example above. Try to print a plot instead of a table, try to generate more complex Markdown code, remove the call to invisible() and knit the document and see what happens with the output, replace the call to lapply() with purrr::walk() or purrr::map(). Really take the time to understand what is going on.\nWhile extremely powerful, this approach using knitr::knit_expand() only works if your template only contains text. If you need to print something more complicated in the document, you need to use child documents instead. For example, suppose that instead of a table we wanted to show a plot made using {ggplot2}. This would not work, because a ggplot object is not made of text, but is a list with many elements. The print() method for ggplot object then does the fancy printing into a graph. But if you want to show plots using knitr::knit_expand(), then the contents of the list will be shown, not the plot itself. This is where child documents come in. Child documents are exactly what you think they are: they’re smaller documents that get knitted and then embedded into the parent document. You can define anything within these child documents, and as such you can even use them to print more complex objects, like a ggplot object. Let’s go back to the example from before and make use of a child document (for ease of presentation, we will not use a separate Rmd file, but will inline the child document into the main document). Read the Rmd example below carefully, as all the steps are explained:\n---\ntitle: \"Templating with child documents\"\noutput: html_document\ndate: \"2023-01-27\"\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(ggplot2)\n```\n\n## A function that creates ggplots\n\n```{r}\ncreate_plot <- function(dataset, aesthetic){\n\n  ggplot(dataset) +\n    geom_point(aesthetic)\n\n}\n```\n\nThe function above takes a dataset and an aesthetic made using `ggplot2::aes()` to\ncreate a plot:\n\n```{r}\ncreate_plot(mtcars, aes(y = mpg, x = hp))\n```\n\nLet’s suppose that we want to generate a document that would look like this:\n\n- first a section title, with the dataset used;\n- then a plot\n\nSo it would look like this:\n\n## Dataset used: \"mtcars\"\n\n```{r}\ncreate_plot(mtcars, aes(y = mpg, x = hp))\n```\n\nWe don’t want to create these sections for every aesthetic by hand.\n\nInstead, we can make use of a child document that gets knitted separately\nand then embedded in the parent document. The chunk below makes use of this trick:\n\n```{r, results = \"asis\"}\n\nx <- list(aes(y = mpg, x = hp),\n          aes(y = mpg, x = hp, size = am))\n\nres <- lapply(x,\n              function(dataset, x){\n\n  knitr::knit_child(text = c(\n\n    '\\n',\n    '## Dataset used: `r deparse(substitute(dataset))`',\n    '\\n',\n    '```{r, echo = F}',\n    'print(create_plot(dataset, x))',\n    '```'\n\n     ),\n     envir = environment(),\n     quiet = TRUE)\n\n}, dataset = mtcars)\n\n\ncat(unlist(res), sep = \"\\n\")\n```\n\nThe child document is the `text` argument to the `knit_child()` function. `text` is literal R\nMarkdown code: we define a level 2 header, and then an R chunk. This child document gets knitted,\nso we need to specify the environment in which it should get knitted. This means that the child\ndocument will get knitted in the same environment as the parent document (our current global\nenvironment). This way, every package that get loaded and every function or variable that got\ndefined in the parent document will also be available to the child document.\n\nTo get the dataset name as a string, we use the `deparse(substitute(dataset))` trick; this\nsubstitutes \"dataset\" by its bound value, so `mtcars`. But `mtcars` is an expression and we don’t\nwant it to get evaluated, or the contents of the entire dataset would be used in the title of the\nsection. So we use `deparse()` which turns unevaluated expressions into strings.\n\nWe then use `lapply()` to loop over two aesthetics with an anonymous function that encapsulates the\nchild document. So we get two child documents that get knitted, one per aesthetic. This gets saved\ninto variable `res`. This is thus a list of knitted Markdown.\n\nFinally, we need unlist `res` to actually merge the Markdown code from the child documents into the\nparent document.\nClick here to take a look at the output.\nHere again, take some time to play with the above example. Change the child document, try to print other types of output, really take your time to understand this. To know more about child documents, take a look at this section of the R Markdown Cookbook (Xie, Dervieux, and Riederer (2020)).\n\n\n6.3.2 Tables in R Markdown documents\nGetting tables right in Rmd documents is not always an easy task. There are several packages specifically made just for this task.\nIn this short section, we want to point you towards two packages that check the following boxes:\n\nWork the same way regardless of output format we want to knit our document into:\nWork for any type of table: summary tables, regression tables, two-way tables, etc.\n\nLet’s start with the simplest type of table, which would be showing the head of a dataset for example. {knitr} comes with the kable() function, but this function generates a very plain looking output. For something publication-worthy, we recommend the {flextable} package, developed by Gohel and Skintzos (2023):\n\nlibrary(flextable)\n\nmy_table <- head(mtcars)\n\nflextable(my_table) |>\n  set_caption(caption = \"Head of the mtcars dataset\") |>\n  theme_booktabs()\n\n\nmpgcyldisphpdratwtqsecvsamgearcarb21.061601103.902.62016.46014421.061601103.902.87517.02014422.84108933.852.32018.61114121.462581103.083.21519.44103118.783601753.153.44017.02003218.162251052.763.46020.221031\n\n\nWe won’t go into much detail on how {flextable} works, but it is very powerful, and the fact that it works for PDF, Html, Word and Powerpoint outputs is really a massive plus. If you want to learn more about {flextable}, there’s a whole, free, ebook on it. {flextable} can create very complicated tables, so really take the time to dig in!\nThe next package is {modelsummary}, by Arel-Bundock (2022), and this one focuses on regression and summary tables. It is extremely powerful as well, and just like {flextable}, works for any type of output. It is very simple to get started:\n\nlibrary(modelsummary)\n\nmodel_1 <- lm(mpg ~ hp + am, data = mtcars)\nmodel_2 <- lm(mpg ~ hp, data = mtcars)\n\nmodels <- list(\"Model 1\" = model_1,\n               \"Model 2\" = model_2)\n\nmodelsummary(models)\n\n\n\n \n  \n      \n    Model 1 \n     Model 2 \n  \n \n\n  \n    (Intercept) \n    26.585 \n    30.099 \n  \n  \n     \n    (1.425) \n    (1.634) \n  \n  \n    hp \n    −0.059 \n    −0.068 \n  \n  \n     \n    (0.008) \n    (0.010) \n  \n  \n    am \n    5.277 \n     \n  \n  \n     \n    (1.080) \n     \n  \n  \n    Num.Obs. \n    32 \n    32 \n  \n  \n    R2 \n    0.782 \n    0.602 \n  \n  \n    R2 Adj. \n    0.767 \n    0.589 \n  \n  \n    AIC \n    164.0 \n    181.2 \n  \n  \n    BIC \n    169.9 \n    185.6 \n  \n  \n    Log.Lik. \n    −78.003 \n    −87.619 \n  \n  \n    RMSE \n    2.77 \n    3.74 \n  \n\n\n\n\n\nHere again, we won’t got into much detail, but recommend instead that you read the package’s website which has very detailed documentation.\nThese packages can help you keeping it DRY, so take some time to learn them.\n\n\n6.3.3 Parametrized reports\nTemplating and child documents are very powerful, but sometimes you don’t want to have one section dedicated to each unit of analysis within the same report, but rather, you want a complete separate report by unit of analysis. This is also possible thanks to parameterised reports.\nLet’s modify the example from before, which consisted in creating one section per column of the mtcars dataset and a frequency table, and make it now one separate report for each column. The R Markdown file will look like this:\n---\ntitle: \"Report for column `r params$var` of dataset `r params$dataset`\"\noutput: html_document\ndate: \"2023-01-27\"\nparams:\n  dataset: mtcars\n  var: \"am\"\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n\n## Frequency table for `r params$var`\n\n```{r, echo = F}\ncreate_table <- function(dataset, var){\n\n  dataset <- get(dataset)\n\n  table(dataset[var]) |>\n    knitr::kable()\n}\n```\n\n\nThe table below is for variable `r params$var` of dataset `r params$dataset`.\n\n```{r}\ncreate_table(params$dataset, params$var)\n```\n\n```{r, eval = FALSE, echo = FALSE}\n# Run these lines to compile the document\n# Set eval and echo to FALSE, so that this does not appear\n# in the output, and does not get evaluated when knitting\nrmarkdown::render(\n             input = \"param_report_example.Rmd\",\n             params = list(\n               dataset = \"mtcars\",\n               var = \"cyl\"\n             )\n           )\n\n```\nSave the code above into an Rmd file title something like param_report_example.Rmd (preferably inside its own folder). At the end of the document, we wrote the lines to render this document inside a chunk that does not get shown to the reader, nor gets evaluated:\n```{r, eval = F, echo = FALSE}\nrmarkdown::render(\n             input = \"param_report_example.Rmd\",\n             params = list(\n               dataset = \"mtcars\",\n               var = \"cyl\"\n             )\n           )\n```\nYou need to run these lines yourself to knit the document.\nThis will pass the list params with elements “mtcars” and “cyl” down to the report. Every params$dataset and params$var in the report gets replaced by “mtcars” and “cyl” respectively. Also, notice that in the header of the document, we defined default values for the params. Something else you need to be aware of, is that the function create_table() inside the report is slightly different than before. It now starts with the following line:\n\ndataset <- get(dataset)\n\nLet’s break this down. params$dataset contains the string “mtcars”. I made the decision to pass the dataset as a string, so that I could use it in the title of the document. But then, inside the create_table() function, I have the following code:\n\ndataset[var]\n\ndataset can’t be a string here, but needs to be a variable name, so mtcars and not “mtcars”. This means that I need to convert that string into a name. get() searches an object by name, and then makes it possible to save it to a new variable called dataset. The rest of the function is then the same as before. This little difficulty can be avoided by hard-coding the dataset inside the R Markdown file, or by passing the dataset as the params$dataset and not the string, in the render function. However, if you pass down the name of the dataset as a variable instead of the dataset name as a string, then you need to covert it to a string if you want to use it in the text (so mtcars to “mtcars”, using deparse(substitute(dataset)) as in child documents example).\nIf you instead want to create one report per variable, you could compile all the documents at once with:\n```{r, eval = F, echo = F}\ncolumns <- colnames(mtcars)\n\nlapply(columns,\n  (\\(x)rmarkdown::render(\n                    input = \"param_report_example.Rmd\",\n                    output_file = paste0(\"param_report_example_\", x, \".html\"),\n                    params = list(\n                      dataset = \"mtcars\",\n                      var = x\n                    )\n                  )\n  )\n)\n```\nBy now, this should not intimidate you anymore; we use lapply() to loop over a list of column names (that we get using colnames()). Because we don’t want to overwrite the report we need to change the name of the output file. We do so by using paste0() which creates a new string that contains the variable name, so each report gets its own name. x inside the paste0() function is each element, one after the other, of the columns variable we defined first. Think of it as the i in a for loop. We then must also pass this to the params list, hence the var = x. The complete call to rmarkdown::render() is wrapped inside an anonymous function, because we need to use the argument x (which is each column defined in the columns list) in different places."
  },
  {
    "objectID": "keep_dry.html#conclusion",
    "href": "keep_dry.html#conclusion",
    "title": "6  Keeping it DRY with literate programming",
    "section": "6.4 Conclusion",
    "text": "6.4 Conclusion\nBefore continuing, we highly recommend that you try running this yourself, and also that you try to build your own little parameterised reports. Maybe start by replacing “mtcars” by “iris” in the code to compile the reports and see what happens, and then when you’re comfortable with parameterised reports, try templating inside a parameterised report!\nIt is important to not fall to the temptation of copy and pasting sections of your report, or parts of your script, instead of using these more advanced features provided by the language. It is tempting, especially under time pressure, to just copy and paste bits of code and get things done instead of writing what seems to be unnecessary code to finally achieve the same thing. The problem however, is that in practice copy and pasting code to simply get things done will come bite you sooner rather than later. Especially when you’re still in the exploration/drafting phase of the project. It make take more time to set up, but once you’re done, it is much easier to experiment with different parameters, test the code or even re-use the code for other projects. Not only that, but forcing you to actually think about how to set up your code in a way that avoids repeating yourself also helps with truly understanding the problem at hand. What part of the problem is constant and does not change? What does change? How often, and why? Can you also fix these parts or not? What if instead of five sections that I need to copy and paste, I had 50 sections? How could I scale that up?\nAsking yourself these questions, and solving them, will ultimately make you better programmer.\nRemember: keep it DRY.\n\n\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\nGohel, David, and Panagiotis Skintzos. 2023. Flextable: Functions for Tabular Reporting.\n\n\nLeisch, Friedrich. 2002. “Sweave: Dynamic Generation of Statistical Reports Using Literate Data Analysis.” In Compstat, edited by Wolfgang Härdle and Bernd Rönz, 575–80. Physica-Verlag HD.\n\n\nXie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible Research in R.” In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC. http://www.crcpress.com/product/isbn/9781466561595.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Chapman; Hall/CRC."
  },
  {
    "objectID": "part1_conclusion.html",
    "href": "part1_conclusion.html",
    "title": "7  Conclusion of part 1",
    "section": "",
    "text": "Before continuing, let’s quickly summarise what we’ve learned so far.\nWe started our journey with two scripts that download and analyse date about housing in Luxembourg. We then learned about tools and programming paradigms that we will now use in part 2 to make our scripts more robust:\n\nVersion control;\nFunctional programming;\nTesting;\nLiterate programming.\n\nWe will start part 2 by rewriting our scripts using what we’ve learned, and then, we will think about approaching the core problem differently, and structuring our project not as a series of scripts (or R Markdown files in the case of literate programming) but instead as a pipeline. We will learn about tools that will capture the computational environment that was used to set up this pipeline and how to use them effectively to make sure that our project is reproducible."
  },
  {
    "objectID": "project_rewrite.html",
    "href": "project_rewrite.html",
    "title": "8  Rewriting our project",
    "section": "",
    "text": "In this chapter, we will use what we’ve learned until now to rewrite our project."
  },
  {
    "objectID": "packages.html#benefits-of-packages",
    "href": "packages.html#benefits-of-packages",
    "title": "9  Packaging your code",
    "section": "9.1 Benefits of packages",
    "text": "9.1 Benefits of packages"
  },
  {
    "objectID": "packages.html#intro-to-packge-dev",
    "href": "packages.html#intro-to-packge-dev",
    "title": "9  Packaging your code",
    "section": "9.2 Intro to packge dev",
    "text": "9.2 Intro to packge dev\nThis is where fusen comes into play I guess; so we start from the Qmd file that was written before, containing the functions an the analysis, and see how we can now create a package from it, and use that file as a vignette? Copying here what Sébastien said on the matter"
  },
  {
    "objectID": "packages.html#document-your-package",
    "href": "packages.html#document-your-package",
    "title": "9  Packaging your code",
    "section": "9.3 Document your package (?)",
    "text": "9.3 Document your package (?)\nI guess fusen makes this process easy and leverages roxygen?"
  },
  {
    "objectID": "packages.html#managing-package-dependencies",
    "href": "packages.html#managing-package-dependencies",
    "title": "9  Packaging your code",
    "section": "9.4 Managing package dependencies (?)",
    "text": "9.4 Managing package dependencies (?)\nDiscuss NAMESPACE and DESCRIPTION and all that. I think it’s important to also discuss here how to define dependencies from remotes, not just CRAN."
  },
  {
    "objectID": "packages.html#unit-testing",
    "href": "packages.html#unit-testing",
    "title": "9  Packaging your code",
    "section": "9.5 Unit testing",
    "text": "9.5 Unit testing\nThis is where I think we should discuss unit testing"
  },
  {
    "objectID": "packages.html#pkgdown",
    "href": "packages.html#pkgdown",
    "title": "9  Packaging your code",
    "section": "9.6 pkgdown",
    "text": "9.6 pkgdown"
  },
  {
    "objectID": "targets.html",
    "href": "targets.html",
    "title": "10  Build automation",
    "section": "",
    "text": "Why build automation: removes cognitive load, is a form of documentation in and of itself, as Miles said\nIt is possible to communicate a great deal of domain knowledge in code, such that it is illuminating beyond the mere mechanical number crunching. To do this well the author needs to make use of certain styles and structures that produce code that has layers of domain specific abstraction a reader can traverse up and down as they build their understanding of the project. Functional programming style, coupled with a dependency graph as per {targets} are useful tools in this regard."
  },
  {
    "objectID": "repro_intro.html",
    "href": "repro_intro.html",
    "title": "11  Introduction to reproducibility",
    "section": "",
    "text": "Since we said in the intro to the book that reproducibility is on a continuum, I think that this chapter should focus on the bare minimum, which would culminate with renv\nThen at the end, explain why renv is not enough (does nothing for R itself, nor the environment the code is running on)"
  },
  {
    "objectID": "repro_cont.html#first-steps-with-docker",
    "href": "repro_cont.html#first-steps-with-docker",
    "title": "12  Advanced topics in reproducibility",
    "section": "12.1 First steps with Docker",
    "text": "12.1 First steps with Docker\nTo write your own Dockerfile, you need some familiarity with the Linux cli, so here’s…"
  },
  {
    "objectID": "repro_cont.html#a-primer-on-the-linux-command-line",
    "href": "repro_cont.html#a-primer-on-the-linux-command-line",
    "title": "12  Advanced topics in reproducibility",
    "section": "12.2 A primer on the Linux command line",
    "text": "12.2 A primer on the Linux command line"
  },
  {
    "objectID": "repro_cont.html#dockrizing-your-project",
    "href": "repro_cont.html#dockrizing-your-project",
    "title": "12  Advanced topics in reproducibility",
    "section": "12.3 Dockrizing your project",
    "text": "12.3 Dockrizing your project"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in\nR.” Journal of Statistical Software 103\n(1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\nChambers, John M. 2014. “Object-Oriented\nProgramming, Functional Programming and R.”\nStatistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452.\n\n\nGohel, David, and Panagiotis Skintzos. 2023. Flextable: Functions\nfor Tabular Reporting.\n\n\nLeisch, Friedrich. 2002. “Sweave: Dynamic Generation of\nStatistical Reports Using Literate Data Analysis.” In\nCompstat, edited by Wolfgang Härdle and Bernd Rönz, 575–80.\nPhysica-Verlag HD.\n\n\nWickham, Hadley. 2019. Advanced r. CRC press.\n\n\nXie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible\nResearch in R.” In Implementing Reproducible\nComputational Research, edited by Victoria Stodden, Friedrich\nLeisch, and Roger D. Peng. Chapman; Hall/CRC. http://www.crcpress.com/product/isbn/9781466561595.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R\nMarkdown Cookbook. Chapman; Hall/CRC."
  }
]